<?xml version="1.0" encoding="utf-8"?>
<posts>
  <row Id="1" PostTypeId="1" AcceptedAnswerId="3" CreationDate="2016-08-02T15:39:14.947" Score="11" ViewCount="801" Body="&lt;p&gt;What does &quot;backprop&quot; mean? Is the &quot;backprop&quot; term basically the same as &quot;backpropagation&quot; or does it have a different meaning?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-11-16T17:56:22.093" LastActivityDate="2021-07-08T10:45:23.250" Title="What is &quot;backprop&quot;?" Tags="&lt;neural-networks&gt;&lt;backpropagation&gt;&lt;terminology&gt;&lt;definitions&gt;" AnswerCount="5" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="2" PostTypeId="1" AcceptedAnswerId="9" CreationDate="2016-08-02T15:40:20.623" Score="14" ViewCount="1153" Body="&lt;p&gt;Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-02-23T22:36:19.090" LastActivityDate="2019-02-23T22:36:37.133" Title="How does noise affect generalization?" Tags="&lt;neural-networks&gt;&lt;machine-learning&gt;&lt;statistical-ai&gt;&lt;generalization&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="3" PostTypeId="2" ParentId="1" CreationDate="2016-08-02T15:40:24.820" Score="15" Body="&lt;p&gt;&quot;Backprop&quot; is the same as &quot;backpropagation&quot;: it's just a shorter way to say it. It is sometimes abbreviated as &quot;BP&quot;.&lt;/p&gt;&#xA;" OwnerUserId="4" LastActivityDate="2016-08-02T15:40:24.820" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="4" PostTypeId="1" AcceptedAnswerId="12" CreationDate="2016-08-02T15:41:22.020" Score="33" ViewCount="1399" Body="&lt;p&gt;When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-19T23:54:07.813" LastActivityDate="2021-01-19T23:54:07.813" Title="How to find the optimal number of neurons per layer?" Tags="&lt;neural-networks&gt;&lt;hyperparameter-optimization&gt;&lt;artificial-neuron&gt;&lt;hyper-parameters&gt;&lt;layers&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="6" PostTypeId="1" AcceptedAnswerId="20" CreationDate="2016-08-02T15:43:35.460" Score="7" ViewCount="299" Body="&lt;p&gt;Given the following definition of an intelligent agent (taken from a &lt;a href=&quot;http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wikipedia article&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;If an agent acts so as to maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and given that we, humans, all make mistakes, which means that we are not maximizing the expected value of a performance measure, then does this imply that humans are not intelligent? &lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="2444" LastEditDate="2019-06-15T18:25:58.513" LastActivityDate="2019-06-15T18:29:55.520" Title="Are humans intelligent according to the definition of an intelligent agent?" Tags="&lt;philosophy&gt;&lt;definitions&gt;&lt;intelligent-agent&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="7" PostTypeId="1" CreationDate="2016-08-02T15:45:09.070" Score="10" ViewCount="737" Body="&lt;p&gt;This &lt;a href=&quot;https://www.independent.co.uk/life-style/gadgets-and-tech/news/stephen-hawking-artificial-intelligence-could-wipe-out-humanity-when-it-gets-too-clever-humans-could-become-ants-being-stepped-a6686496.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;quote by Stephen Hawking&lt;/a&gt; has been in headlines for quite some time:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Why does he say this? To put it simply: what are the possible threats from AI (that Stephen Hawking is worried about)? If we know that AI is so dangerous, why are we still promoting it? Why is it not banned?&lt;/p&gt;&#xA;&lt;p&gt;What are the adverse consequences of the so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Technological_singularity&quot; rel=&quot;nofollow noreferrer&quot;&gt;Technological Singularity&lt;/a&gt;?&lt;/p&gt;&#xA;" OwnerUserId="26" LastEditorUserId="2444" LastEditDate="2021-01-20T00:00:31.027" LastActivityDate="2021-01-20T00:00:31.027" Title="Why does Stephen Hawking say &quot;Artificial Intelligence will kill us all&quot;?" Tags="&lt;agi&gt;&lt;superintelligence&gt;&lt;singularity&gt;&lt;ai-safety&gt;&lt;ai-takeover&gt;" AnswerCount="6" CommentCount="1" ClosedDate="2016-08-04T01:36:40.283" ContentLicense="CC BY-SA 4.0" />
  <row Id="9" PostTypeId="2" ParentId="2" CreationDate="2016-08-02T15:47:02.993" Score="9" Body="&lt;p&gt;Noise in the data, to a reasonable amount, may help the network to generalize better. Sometimes, it has the opposite effect. It partly depends on the kind of noise (&quot;true&quot; vs. artificial).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;ftp://ftp.sas.com/pub/neural/FAQ3.html#A_noise&quot; rel=&quot;nofollow noreferrer&quot;&gt;AI FAQ on ANN&lt;/a&gt; gives a good overview. Excerpt:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Noise in the actual data is never a good thing, since it limits the accuracy of generalization that can be achieved no matter how extensive the training set is. On the other hand, injecting artificial noise (jitter) into the inputs during training is one of several ways to improve generalization for smooth functions when you have a small training set.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In some field, such as computer vision, it's common to increase the size of the training set by copying some samples and adding some noises or other transformation.&lt;/p&gt;&#xA;" OwnerUserId="4" LastEditorUserId="2444" LastEditDate="2019-02-23T22:36:37.133" LastActivityDate="2019-02-23T22:36:37.133" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="10" PostTypeId="1" AcceptedAnswerId="32" CreationDate="2016-08-02T15:47:56.593" Score="51" ViewCount="2712" Body="&lt;p&gt;I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="10135" LastEditDate="2018-10-18T10:44:33.687" LastActivityDate="2022-02-24T01:25:23.263" Title="What is fuzzy logic?" Tags="&lt;deep-neural-networks&gt;&lt;terminology&gt;&lt;fuzzy-logic&gt;" AnswerCount="6" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="11" PostTypeId="2" ParentId="2" CreationDate="2016-08-02T15:48:56.970" Score="9" Body="&lt;p&gt;We typically think of machine learning models as modeling two different parts of the training data--the underlying generalizable truth (the signal), and the randomness specific to that dataset (the noise).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fitting both of those parts increases training set accuracy, but fitting the signal also increases test set accuracy (and real-world performance) while fitting the noise decreases both. So we use things like regularization and dropout and similar techniques in order to make it harder to fit the noise, and so more likely to fit the signal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just increasing the amount of noise in the training data is one such approach, but seems unlikely to be as useful. Compare random jitter to adversarial boosting, for example; the first will slowly and indirectly improve robustness whereas the latter will dramatically and directly improve it.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T15:48:56.970" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="12" PostTypeId="2" ParentId="4" CreationDate="2016-08-02T15:50:27.867" Score="19" Body="&lt;p&gt;There is no direct way to find the optimal number of them: people empirically try and see (e.g., using cross-validation). The most common search techniques are random, manual, and grid searches. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There exist more advanced techniques such as Gaussian processes, e.g. &lt;em&gt;&lt;a href=&quot;http://arxiv.org/abs/1609.08703&quot; rel=&quot;noreferrer&quot;&gt;Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification&lt;/a&gt;, IEEE SLT 2016&lt;/em&gt;.&lt;/p&gt;&#xA;" OwnerUserId="4" LastEditorUserId="4" LastEditDate="2016-09-29T00:24:06.177" LastActivityDate="2016-09-29T00:24:06.177" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="13" PostTypeId="1" AcceptedAnswerId="163" CreationDate="2016-08-02T15:52:19.413" Score="8" ViewCount="168" Body="&lt;p&gt;In particular, an embedded computer (with limited resources) analyzes live video stream from a traffic camera, trying to pick good frames that contain license plate numbers of passing cars. Once a plate is located, the frame is handed over to an OCR library to extract the registration and use it further.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my country two types of license plates are in common use - rectangular (the typical) and square - actually, somewhat rectangular but &quot;higher than wider&quot;, with the registration split over two rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(there are some more types, but let us disregard them; they are a small percent and usually belong to vehicles that lie outside our interest.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Due to the limited resources and need for rapid, real-time processing, the maximum size of the network (number of cells and connections) the system can handle is fixed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Would it be better to split this into two smaller networks, each recognizing one type of registration plates, or will the larger single network handle the two types better?&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="14723" LastEditDate="2018-04-12T02:30:42.823" LastActivityDate="2018-04-12T02:30:42.823" Title="Can a single neural network handle recognizing two types of objects, or should it be split into two smaller networks?" Tags="&lt;neural-networks&gt;&lt;image-recognition&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="15" PostTypeId="1" CreationDate="2016-08-02T15:52:50.827" Score="40" ViewCount="4240" Body="&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Turing_test&quot;&gt;Turing Test&lt;/a&gt; was the first test of artificial intelligence and is now a bit outdated. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test&quot;&gt;Total Turing Test&lt;/a&gt; aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_general_intelligence&quot;&gt;artificial general intelligence&lt;/a&gt; (strong AI)?&lt;/p&gt;&#xA;" OwnerUserId="9" LastEditorUserId="95" LastEditDate="2016-08-04T14:10:10.990" LastActivityDate="2018-04-11T20:16:35.997" Title="Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?" Tags="&lt;turing-test&gt;&lt;agi&gt;&lt;intelligent-agent&gt;&lt;narrow-ai&gt;" AnswerCount="6" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="16" PostTypeId="1" AcceptedAnswerId="142" CreationDate="2016-08-02T15:53:00.447" Score="9" ViewCount="773" Body="&lt;p&gt;What is &lt;a href=&quot;https://en.wikipedia.org/wiki/Early_stopping&quot; rel=&quot;nofollow noreferrer&quot;&gt;early stopping&lt;/a&gt; in machine learning and, in general, artificial intelligence? What are the advantages of using this method? How does it help exactly?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd be interested in perspectives and links to recent research.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-10-11T22:28:23.457" LastActivityDate="2022-05-29T04:37:24.917" Title="What is &quot;early stopping&quot; in machine learning?" Tags="&lt;deep-learning&gt;&lt;definitions&gt;&lt;overfitting&gt;&lt;regularization&gt;&lt;early-stopping&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="17" PostTypeId="1" AcceptedAnswerId="45" CreationDate="2016-08-02T15:53:38.273" Score="40" ViewCount="1773" Body="&lt;p&gt;I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence? Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2019-09-16T16:23:39.853" LastActivityDate="2020-11-17T13:21:19.530" Title="What is the concept of the technological singularity?" Tags="&lt;philosophy&gt;&lt;definitions&gt;&lt;agi&gt;&lt;superintelligence&gt;&lt;singularity&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="18" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:54:26.937" Score="3" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;To put it simply in layman terms, what are the possible threats from AI? &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Currently, there are no threat. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The threat comes if humans create a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity).  However, this could cause the machine to invent machines that can destruct humans, and we can't stop them because they are so much smarter than we are.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is all hypothetical, no one has even a clue of what an ultraintelligent machine looks like. &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;If we know that AI is so dangerous why are we still promoting it? Why is it not banned?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;As I said before, the existence of a ultraintelligent machine is hypothetical. Artificial Intelligence has lots of useful applications (more than this answer can contain), and if we develop it, we get even more useful applications. We just have to be careful that the machines won't overtake us. &lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2016-08-02T15:54:26.937" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="19" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:54:29.263" Score="4" Body="&lt;p&gt;Because he did not yet know how far away current AI is... Working in an media AI lab, I get this question a lot. But really... we are still a long way from this. The robots still do everything we detailledly describe them to do. Instead of seeing the robot as intelligent, I would look to the human programmer for where the creativity really happens.&lt;/p&gt;&#xA;" OwnerUserId="52" LastActivityDate="2016-08-02T15:54:29.263" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="20" PostTypeId="2" ParentId="6" CreationDate="2016-08-02T15:54:45.237" Score="2" Body="&lt;p&gt;It rather depends on how one defines several of the terms used. For example:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Whether the term &quot;expected&quot; is interpreted in a formal (i.e.&#xA;statistical) sense.  &lt;/li&gt;&#xA;&lt;li&gt;Whether it's assumed that humans have any kind of utilitarian&#xA;&quot;performance measure&quot;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The motivation for this description of &quot;agent&quot; arose from a desire to have a quantitative model - it's not clear that such a model is a good fit for human cognition.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, there are alternative definitions of agents, for example the &lt;a href=&quot;https://en.wikipedia.org/wiki/Belief%E2%80%93desire%E2%80%93intention_software_model&quot; rel=&quot;nofollow noreferrer&quot;&gt;BDI model&lt;/a&gt;, which are rather more open-ended and hence more obviously applicable to humans.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="2444" LastEditDate="2019-06-15T18:29:55.520" LastActivityDate="2019-06-15T18:29:55.520" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="21" PostTypeId="1" CreationDate="2016-08-02T15:55:15.957" Score="5" ViewCount="104" Body="&lt;p&gt;I'm worrying that my neural network has become too complex. I don't want to end up with half of the neural network doing nothing but just take up space and resources.&lt;/p&gt;&#xA;&lt;p&gt;So, what are the techniques for detecting and preventing overfitting, to avoid such problems?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-10T01:01:09.583" LastActivityDate="2021-01-10T01:03:39.270" Title="What are the techniques for detecting and preventing overfitting?" Tags="&lt;reference-request&gt;&lt;optimization&gt;&lt;deep-neural-networks&gt;&lt;overfitting&gt;&lt;generalization&gt;" AnswerCount="1" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="22" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:56:10.167" Score="5" Body="&lt;p&gt;It's not just Hawking, you hear variations on this refrain from a lot of people.  And given that they're mostly very smart, well educated, well informed people (Elon Musk is another, for example), it probably shouldn't be dismissed out of hand.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyway, the basic idea seems to be this: If we create &quot;real&quot; artificial intelligence, at some point, it will be able to improve itself, which improves it's ability to improve itself, which means it can improve it's ability to improve itself even more, and so on... a runaway cascade leading to &quot;superhuman intelligence&quot;.  That is to say, leading to something that more intelligent than we area.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So what happens if there is an entity on this planet which is literally more intelligent than us (humans)? Would it be a threat to us?  Well, it certainly seems reasonable to speculate that it &lt;em&gt;could&lt;/em&gt; be so.   OTOH, we have no particular reason, right now, to think that it &lt;em&gt;will&lt;/em&gt; be so. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So it seems that Hawking, Musk, etc. are just coming down on the more cautious / fearful side of things.  Since we don't &lt;em&gt;know&lt;/em&gt; if a superhuman AI will be dangerous or not, and given that it could be unstoppable if it were to become malicious (remember, it's smarter than we are!), it's a reasonable thing to take under consideration.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Eliezer Yudkowsky has also written quite a bit on this subject, including come up with the famous &quot;AI Box&quot; experiment.  I think anybody interested in this topic should read some of his material.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.yudkowsky.net/singularity/aibox/&quot; rel=&quot;noreferrer&quot;&gt;http://www.yudkowsky.net/singularity/aibox/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="33" LastActivityDate="2016-08-02T15:56:10.167" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="23" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:57:19.303" Score="4" Body="&lt;p&gt;As Andrew Ng &lt;a href=&quot;http://www.theregister.co.uk/2015/03/19/andrew_ng_baidu_ai/&quot; rel=&quot;nofollow noreferrer&quot;&gt;said&lt;/a&gt;, worrying about such threat from AI is like worrying about of overpopulation on Mars. It is science fiction. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/m6jnl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/m6jnl.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That being said, given the rise of (much weaker) robots and other (semi-)autonomous agents, the fields of the law and ethics are increasingly incorporating them, e.g. see &lt;a href=&quot;https://en.wikipedia.org/wiki/Roboethics&quot; rel=&quot;nofollow noreferrer&quot;&gt;Roboethics&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="4" LastActivityDate="2016-08-02T15:57:19.303" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="24" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:57:48.363" Score="2" Body="&lt;p&gt;He says this because it can happen. If something becomes smarter than us, why would it continue to serve us? The worst case scenario is that it takes over all manufacturing processes and consumes all matter to convert it into material capable of computation, extending outward infinitely until all matter is consumed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We know that AI is dangerous but it doesn't matter because most people don't believe in it. It goes against every comfort religion has to offer. Man is the end-all-be-all of the universe and if that fact is disputed, people will feel out of place and purposeless.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The fact is most people just don't acknowledge it's possible, or that it will happen in our lifetimes, even though many reputable AI experts put the occurrence of the singularity within two decades. If people truly acknowledged that AI that was smarter than them was possible, wouldn't they be living differently? Wouldn't they be looking to do things that they enjoy, knowing that whatever it is they do that they dread will be automated? Wouldn't everyone be calling for a universal basic income?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other reason we don't ban it is because its promise is so great. One researcher could be augmented by 1,000 digital research assistants. All manual labor could be automated. For the first time, technology offers us real freedom to do whatever we please.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But even in this best case scenario where it doesn't overtake us, humans still have to adapt and alter their economic system to one where labor isn't necessary. Otherwise, those who aren't technically-trained will starve and revolt.&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2016-08-02T16:46:21.237" LastActivityDate="2016-08-02T16:46:21.237" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="25" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:58:13.970" Score="3" Body="&lt;p&gt;There are a number of long resources to answer this sort of question: consider Stuart Armstrong's book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/B00IB4N4KU&quot; rel=&quot;nofollow&quot;&gt;Smarter Than Us&lt;/a&gt;, Nick Bostrom's book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/B00LOOCGB2&quot; rel=&quot;nofollow&quot;&gt;Superintelligence&lt;/a&gt;, which grew out of this &lt;a href=&quot;http://www.nickbostrom.com/views/superintelligence.pdf&quot; rel=&quot;nofollow&quot;&gt;edge.org answer&lt;/a&gt;, &lt;a href=&quot;http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html&quot; rel=&quot;nofollow&quot;&gt;Tim Urban's explanation&lt;/a&gt;, or &lt;a href=&quot;https://aisafety.wordpress.com/&quot; rel=&quot;nofollow&quot;&gt;Michael Cohen's explanation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But here's my (somewhat shorter) answer: intelligence is all about decision-making, and we don't have any reason to believe that humans are anywhere near close to being the best possible at decision-making. Once we are able to build an AI AI researcher (that is, a computer that knows how to make computers better at thinking), the economic and military relevance of humans will rapidly disappear as any decision that could be made by a human could be made better by a computer. (Why have human generals instead of robot generals, human engineers instead of robot engineers, and so on.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This isn't necessarily a catastrophe. If the Vulcans showed up tomorrow and brought better decision-making to Earth, we could avoid a lot of misery. The hard part is making sure that what we get are Vulcans who want us around and happy, instead of something that doesn't share our values.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T15:58:13.970" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="26" PostTypeId="1" AcceptedAnswerId="189" CreationDate="2016-08-02T15:58:31.413" Score="24" ViewCount="1475" Body="&lt;p&gt;I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Are there examples where this is already happening to a degree today?  &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2020-03-08T03:17:30.877" LastActivityDate="2021-02-01T01:03:23.217" Title="How could emotional intelligence be implemented?" Tags="&lt;emotional-intelligence&gt;&lt;turing-test&gt;&lt;affective-computing&gt;" AnswerCount="4" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="27" PostTypeId="2" ParentId="15" CreationDate="2016-08-02T16:01:59.740" Score="10" Body="&lt;p&gt;The problem of the Turing Test is that it tests the machines ability to resemble humans. Not necessarily every form of AI has to resemble humans. This makes the Turing Test less reliable. However, it is still useful since it is an actual test. It is also noteworthy that there is a prize for passing or coming closest to passing the Turing Test, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Loebner_Prize&quot;&gt;Loebner Prize&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from &lt;a href=&quot;http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition&quot;&gt;Wikipedia&lt;/a&gt;). This definition is used more often and does not depend on the ability to resemble humans. However, it is harder to test this. &lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2016-08-02T16:01:59.740" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="28" PostTypeId="1" AcceptedAnswerId="143" CreationDate="2016-08-02T16:02:44.553" Score="14" ViewCount="7815" Body="&lt;p&gt;Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence? If not, how do they differ? Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2019-06-20T20:36:38.073" LastActivityDate="2019-06-20T21:03:43.207" Title="Is a genetic algorithm an example of artificial intelligence?" Tags="&lt;philosophy&gt;&lt;genetic-algorithms&gt;&lt;terminology&gt;" AnswerCount="5" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="29" PostTypeId="5" CreationDate="2016-08-02T16:03:16.133" Score="0" Body="" OwnerUserId="5" LastEditorUserId="5" LastEditDate="2016-08-04T14:45:26.583" LastActivityDate="2016-08-04T14:45:26.583" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="30" PostTypeId="4" CreationDate="2016-08-02T16:03:16.133" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-02T16:03:16.133" LastActivityDate="2016-08-02T16:03:16.133" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="31" PostTypeId="2" ParentId="10" CreationDate="2016-08-02T16:04:09.333" Score="10" Body="&lt;p&gt;It's analogous to analogue versus digital, or the many shades of gray in between black and white: when evaluating the truthiness of a result, in binary boolean it's either true or false (0 or 1), but when utilizing fuzzy logic, it's an estimated probability between 0 and 1 (such as 0.75 being mostly probably true). It's useful for making calculated decisions when all information needed isn't necessarily available.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fuzzy_logic&quot; rel=&quot;noreferrer&quot;&gt;Wikipedia has a fantastic page for this&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="62" LastActivityDate="2016-08-02T16:04:09.333" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="32" PostTypeId="2" ParentId="10" CreationDate="2016-08-02T16:04:39.867" Score="53" Body="&lt;p&gt;&lt;em&gt;As complexity rises, precise statements lose meaning and meaningful statements lose precision.&lt;/em&gt; ( Lofti Zadeh ).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fuzzy logic deals with reasoning that is approximate rather than fixed and exact. This may make the reasoning more meaningful for a human:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/xdHPJ.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/xdHPJ.png&quot; alt=&quot;Precision and significance - comic&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Fuzzy logic is an extension of Boolean logic by Lotfi Zadeh in 1965 based on the&#xA;mathematical theory of fuzzy sets, which is a generalization of the classical set theory.&#xA;By introducing the notion of &lt;em&gt;degree in the verification&lt;/em&gt; of a condition, thus enabling a&#xA;condition to be in a state other than true or false, fuzzy logic provides a very valuable&#xA;flexibility for reasoning, which makes it possible to take into account inaccuracies and&#xA;uncertainties.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One advantage of fuzzy logic in order to formalize human reasoning is that the rules&#xA;are set in natural language. For example, here are some rules of conduct that a driver&#xA;follows, assuming that he does not want to lose his driver’s licence:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/TM2UE.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/TM2UE.png&quot; alt=&quot;Fuzzy logic decision table&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Intuitively, it thus seems that the input variables like in this example are approximately&#xA;appreciated by the brain, such as the degree of verification of a condition in fuzzy&#xA;logic.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I've written a short &lt;a href=&quot;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=kz2aIc8AAAAJ&amp;amp;citation_for_view=kz2aIc8AAAAJ:eQOLeE2rZwMC&quot; rel=&quot;noreferrer&quot;&gt;introduction to fuzzy logic&lt;/a&gt; that goes into a bit more details but should be very accessible.&lt;/p&gt;&#xA;" OwnerUserId="4" LastEditorUserId="2255" LastEditDate="2020-05-26T17:15:36.130" LastActivityDate="2020-05-26T17:15:36.130" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="33" PostTypeId="2" ParentId="17" CreationDate="2016-08-02T16:04:57.997" Score="3" Body="&lt;p&gt;The concept of &quot;the singularity&quot; is when machines outsmart the humans. Although Stephen Hawking opinion is that this situation is inevitable, but I think it'll be very difficult to reach that point, because every A.I. algorithm needs to be programmed by humans, therefore it would be always more limited than its creator.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We would probably know when that point when humanity will lose control over Artificial Intelligence where super-smart AI would be in competition with humans and maybe creating more sophisticated intelligent beings occurred, but currently, it's more like science fiction (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/Skynet_(Terminator)&quot; rel=&quot;nofollow noreferrer&quot;&gt;Terminator's Skynet&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The risk could involve killing people (like self-flying war &lt;em&gt;drones&lt;/em&gt; making their own decision), destroying countries or even the whole planet (like A.I. connected to the nuclear weapons (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/WarGames&quot; rel=&quot;nofollow noreferrer&quot;&gt;WarGames&lt;/a&gt; movie), but it doesn't prove the point that the machines would be smarter than humans.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="14723" LastEditDate="2018-04-11T16:38:28.877" LastActivityDate="2018-04-11T16:38:28.877" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="35" PostTypeId="1" CreationDate="2016-08-02T16:05:26.390" Score="103" ViewCount="16478" Body="&lt;p&gt;These two terms seem to be related, especially in their application in computer science and software engineering.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is one a subset of another?&lt;/li&gt;&#xA;&lt;li&gt;Is one a tool used to build a system for the other?&lt;/li&gt;&#xA;&lt;li&gt;What are their differences and why are they significant?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="69" LastEditorUserId="48391" LastEditDate="2021-11-18T10:18:35.200" LastActivityDate="2021-11-18T10:18:35.200" Title="What is the difference between artificial intelligence and machine learning?" Tags="&lt;machine-learning&gt;&lt;comparison&gt;&lt;terminology&gt;&lt;ai-field&gt;" AnswerCount="9" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="36" PostTypeId="1" AcceptedAnswerId="114" CreationDate="2016-08-02T16:06:25.853" Score="48" ViewCount="2007" Body="&lt;p&gt;What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2016-08-02T19:13:49.690" LastActivityDate="2021-12-24T17:45:24.580" Title="To what extent can quantum computers help to develop Artificial Intelligence?" Tags="&lt;quantum-computing&gt;" AnswerCount="5" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="37" PostTypeId="1" AcceptedAnswerId="73" CreationDate="2016-08-02T16:07:29.317" Score="7" ViewCount="3945" Body="&lt;p&gt;I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event.  What are examples of the application of a Markov chain and can it be used to create artificial intelligence?  Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation?&lt;/p&gt;&#xA;" OwnerUserId="55" LastActivityDate="2016-08-03T06:37:01.983" Title="What is a Markov chain and how can it be used in creating artificial intelligence?" Tags="&lt;genetic-algorithms&gt;&lt;markov-chain&gt;&lt;probability&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="38" PostTypeId="2" ParentId="28" CreationDate="2016-08-02T16:08:06.920" Score="3" Body="&lt;p&gt;This is probably more a question of philosophy than anything. In terms of how things are commonly defined, I'll say &quot;yes, genetic algorithms are part of AI&quot;.  If you pick up a comprehensive book on artificial intelligence, there will probably be a chapter on genetic algorithms (or more broadly, evolutionary algorithms). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One area that has been extensively studied in the past is the idea of using genetic algorithms to train neural networks.  I don't know if people are still actively researching this topic or not, but it at least illustrates that GA's are part of the overall rubric of AI in one regard.&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="2444" LastEditDate="2019-06-20T21:03:43.207" LastActivityDate="2019-06-20T21:03:43.207" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="39" PostTypeId="2" ParentId="15" CreationDate="2016-08-02T16:08:09.103" Score="16" Body="&lt;p&gt;The rhetorical point of the Turing Test is that it places the 'test' for 'humanity' in &lt;em&gt;observable outcomes&lt;/em&gt;, instead of in &lt;em&gt;internal components&lt;/em&gt;. If you would behave the same in interacting with an AI as you would with a person, how could &lt;em&gt;you&lt;/em&gt; know the difference between them?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But that doesn't mean it's reliable, because intelligence has many different components and there are many sorts of intellectual tasks. The Turing Test, in some respects, is about the reaction of people to behavior, which is not at all reliable--remember that many people thought &lt;a href=&quot;https://en.wikipedia.org/wiki/ELIZA&quot;&gt;ELIZA&lt;/a&gt;, a very simple chatbot, was an excellent listener and got deeply emotionally involved very quickly. It calls to mind the &lt;a href=&quot;https://www.youtube.com/watch?v=dBqhIVyfsRg&quot;&gt;Ikea commercial about throwing out a lamp&lt;/a&gt;, where the emotional attachment comes &lt;em&gt;from the human viewer&lt;/em&gt; (and the music), rather than from the lamp.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Turing tests for specific economic activities are much more practically interesting--if one can write an AI that replaces an Uber driver, for example, what that will imply is much clearer than if someone can create a conversational chatbot.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T16:08:09.103" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="40" PostTypeId="1" AcceptedAnswerId="44" CreationDate="2016-08-02T16:08:23.377" Score="13" ViewCount="3789" Body="&lt;p&gt;What purpose does the &quot;dropout&quot; method serve and how does it improve the overall performance of the neural network?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="10" LastEditDate="2016-08-02T16:10:22.307" LastActivityDate="2020-10-01T20:16:56.117" Title="What is the &quot;dropout&quot; technique?" Tags="&lt;deep-neural-networks&gt;&lt;overfitting&gt;&lt;performance&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="41" PostTypeId="1" AcceptedAnswerId="65" CreationDate="2016-08-02T16:08:34.350" Score="14" ViewCount="1248" Body="&lt;p&gt;Can an AI program have an IQ? In other words, can the IQ of an AI program be measured? Like how humans can do an IQ test.&lt;/p&gt;&#xA;" OwnerUserId="72" LastEditorUserId="2444" LastEditDate="2021-12-14T21:49:50.667" LastActivityDate="2021-12-14T21:49:50.667" Title="Can the IQ of an AI program be measured?" Tags="&lt;intelligence-testing&gt;&lt;intelligence-quotient&gt;" AnswerCount="4" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="42" PostTypeId="1" AcceptedAnswerId="51" CreationDate="2016-08-02T16:09:25.427" Score="7" ViewCount="4256" Body="&lt;p&gt;Why would anybody want to use &quot;hidden layers&quot;? How do they enhance the learning ability of the network in comparison to the network which doesn't have them (linear models)?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-10T01:10:28.160" LastActivityDate="2021-01-10T22:56:08.420" Title="What is the purpose of the hidden layers?" Tags="&lt;neural-networks&gt;&lt;deep-learning&gt;&lt;deep-neural-networks&gt;&lt;hidden-layers&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="43" PostTypeId="2" ParentId="10" CreationDate="2016-08-02T16:10:01.630" Score="22" Body="&lt;p&gt;Fuzzy logic is based on regular boolean logic. Boolean logic means you are working with truth values of either true or false (or 1 or 0 if you prefer). Fuzzy logic is the same apart from you can have truth values that are in-between true and false, which is to say, you are working with any number between 0 (inclusive) and 1 (inclusive). The fact that you can have a 'partially true and partially false' truth value is where the word &amp;quot;fuzzy&amp;quot; comes from. Natural languages often use fuzzy logic like &amp;quot;that balloon is red&amp;quot; meaning that balloon could be any colour that is similar enough to red, or &amp;quot;the shower is warm&amp;quot;. Here is a rough diagram for how &amp;quot;the temperature of the shower is warm&amp;quot; could be represented in terms of fuzzy logic (the y axis being the truth value and the x-axis being the temperature):&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/G7szY.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/G7szY.png&quot; alt=&quot;y-axis=truth value of statement about temperature, x-axis=temperature&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fuzzy logic can be applied to boolean operations such as &lt;strong&gt;and&lt;/strong&gt;, &lt;strong&gt;or&lt;/strong&gt;, and &lt;strong&gt;not&lt;/strong&gt;. Note that you can define the fuzzy logic operations in different ways. One way is with the min and max functions which return the lessermost and greatermost values of the two values inputted respectively. This would work as such:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;A and B = min(A,B)&#xA;A or B  = max(A,B)&#xA;not A   = 1-A&#xA;(where A and B are real values from 0 (inclusive) to 1 (inclusive))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;When defined like this they are called the &lt;strong&gt;Zadeh operators&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Another way would be to define &lt;strong&gt;and&lt;/strong&gt; as the first argument times the second argument, which yields different outputs for the same inputs as the Zadeh &lt;strong&gt;and&lt;/strong&gt; operator (&lt;code&gt;min(0.5,0.5)=0.5, 0.5*0.5=0.25&lt;/code&gt;). Then other operators are derived based on the &lt;strong&gt;and&lt;/strong&gt; and &lt;strong&gt;not&lt;/strong&gt; operators. This would work as such:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;A and B = A*B&#xA;not A = 1-A&#xA;A or B = not ((not A) and (not B)) = 1-((1-A)*(1-B)) = 1-(1-A)*(1-B)&#xA;(where A and B are real values from 0 (inclusive) to 1 (inclusive))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;You can then use the three &amp;quot;basic fuzzy logic operations&amp;quot; to build all other &amp;quot;fuzzy logic operations&amp;quot;, just like you can use the three &amp;quot;basic boolean operations&amp;quot; to build all other &amp;quot;boolean logic operations&amp;quot;.&lt;/p&gt;&#xA;&lt;p&gt;Note that the latter definition of the three basic operations is more in line with probability theory, so could be considered the more natural choice.&lt;/p&gt;&#xA;&lt;p&gt;Sources:&#xA;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fuzzy_logic&quot; rel=&quot;nofollow noreferrer&quot;&gt;Fuzzy logic wikipedia&lt;/a&gt;,&#xA;&lt;a href=&quot;https://en.wikipedia.org/wiki/Boolean_algebra&quot; rel=&quot;nofollow noreferrer&quot;&gt;Boolean algebra wikipedia&lt;/a&gt;,&#xA;&lt;a href=&quot;https://www.youtube.com/watch?v=r804UF8Ia4c&quot; rel=&quot;nofollow noreferrer&quot;&gt;Explanation of fuzzy logic on Youtube&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Note: if anyone could suggest some more reliable sources in the comments, I will happily add them to the list (I understand that the current ones aren't too reliable).&lt;/p&gt;&#xA;" OwnerUserId="47" LastEditorUserId="47" LastEditDate="2022-02-24T01:25:23.263" LastActivityDate="2022-02-24T01:25:23.263" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="44" PostTypeId="2" ParentId="40" CreationDate="2016-08-02T16:12:08.767" Score="8" Body="&lt;p&gt;Dropout means that every individual data point is only used to fit a random subset of the neurons. This is done to make the neural network more like an ensemble model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That is, just as a random forest is averaging together the results of many individual decision trees, you can see a neural network trained using dropout as averaging together the results of many individual neural networks (with 'results' understood to mean activations at every layer, rather than just the output layer).&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="12133" LastEditDate="2018-01-17T00:30:27.303" LastActivityDate="2018-01-17T00:30:27.303" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="45" PostTypeId="2" ParentId="17" CreationDate="2016-08-02T16:12:49.850" Score="25" Body="&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Technological_singularity&quot; rel=&quot;nofollow noreferrer&quot;&gt;technological singularity&lt;/a&gt; is a theoretical point in time at which a &lt;em&gt;self-improving&lt;/em&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_general_intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;artificial general intelligence&lt;/a&gt; becomes able to understand and manipulate concepts outside of the human brain's range, that is, the moment when it can understand things humans, by biological design, can't.&lt;/p&gt;&#xA;&lt;p&gt;The fuzziness about the singularity comes from the fact that, from the singularity onwards, history is effectively &lt;em&gt;unpredictable&lt;/em&gt;. Humankind would be unable to predict any future events, or explain any present events, as science itself becomes incapable of describing machine-triggered events. Essentially, machines would think of us the same way we think of ants. Thus, we can make no predictions past the singularity. Furthermore, as a logical consequence, we'd be unable to define the point at which the singularity may occur at all, or even recognize it when it happens.&lt;/p&gt;&#xA;&lt;p&gt;However, in order for the singularity to take place, AGI needs to be developed, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_general_intelligence#Feasibility&quot; rel=&quot;nofollow noreferrer&quot;&gt;whether that is possible is quite a hot debate&lt;/a&gt; right now. Moreover, an algorithm that creates &lt;em&gt;superhuman intelligence&lt;/em&gt; (or &lt;a href=&quot;https://en.wikipedia.org/wiki/Superintelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;superintelligence&lt;/em&gt;&lt;/a&gt;) out of bits and bytes would have to be designed. By definition, a human programmer wouldn't be able to do such a thing, as his/her brain would need to be able to comprehend concepts beyond its range. There is also the argument that an &lt;a href=&quot;https://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;intelligence explosion&lt;/em&gt;&lt;/a&gt; (the mechanism by which a technological singularity would theoretically be formed) would be impossible due to the difficulty of the design challenge of making itself more intelligent, getting larger proportionally to its intelligence, and that the difficulty of the design itself may overtake the intelligence required to solve the said challenge.&lt;/p&gt;&#xA;&lt;p&gt;Also, there are related theories involving &lt;a href=&quot;https://en.wikipedia.org/wiki/AI_takeover&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;machines taking over humankind&lt;/em&gt;&lt;/a&gt; and all of that sci-fi narrative. However, that's unlikely to happen, if &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Laws_of_Robotics&quot; rel=&quot;nofollow noreferrer&quot;&gt;Asimov's laws&lt;/a&gt; are followed appropriately. &lt;a href=&quot;https://www.youtube.com/watch?app=desktop&amp;amp;v=7PKx3kS7f4A&quot; rel=&quot;nofollow noreferrer&quot;&gt;Even if Asimov's laws were not enough&lt;/a&gt;, a series of constraints would still be necessary in order to avoid the misuse of AGI by misintentioned individuals, and Asimov's laws are the nearest we have to that.&lt;/p&gt;&#xA;" OwnerUserId="71" LastEditorUserId="2444" LastEditDate="2020-11-17T13:21:19.530" LastActivityDate="2020-11-17T13:21:19.530" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="46" PostTypeId="1" AcceptedAnswerId="71" CreationDate="2016-08-02T16:14:26.350" Score="11" ViewCount="4004" Body="&lt;p&gt;When did research into Artificial Intelligence first begin?  Was it called Artificial Intelligence then or was there another name?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2021-01-31T16:41:08.950" LastActivityDate="2021-01-31T16:46:15.343" Title="When did Artificial Intelligence research first start?" Tags="&lt;research&gt;&lt;history&gt;&lt;ai-field&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="47" PostTypeId="2" ParentId="28" CreationDate="2016-08-02T16:14:29.337" Score="2" Body="&lt;p&gt;The notion of genetics used in Genetic Algorithms (GAs) is a &lt;em&gt;very&lt;/em&gt; stripped down version relative to genetics in nature, essentially consisting of a population of 'genes' (representing solutions to some predefined problem) subject to `survival of the fittest' during iterated application of recombination and mutation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nowadays, the term 'Computational Intelligence' (CI) tends to be used to describe computational techniques intended to produce `the appearance of intelligence by &lt;em&gt;any&lt;/em&gt; computational means', rather than specifically attempting to mimic the mechanisms that are believed to give rise to human (or animal) intelligence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, the distinction between CI and AI is not so hard and fast, and arguably arose during the `AI Winter' when the term AI was out of fashion.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-02T16:14:29.337" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="48" PostTypeId="2" ParentId="42" CreationDate="2016-08-02T16:15:49.970" Score="3" Body="&lt;p&gt;Hidden layers by themselves aren't useful. If you had hidden layers that were linear, the end result would still be a linear function of the inputs, and so you could collapse an arbitrary number of linear layers down to a single layer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is why we use nonlinear &lt;a href=&quot;https://en.wikipedia.org/wiki/Activation_function&quot; rel=&quot;nofollow&quot;&gt;activation functions&lt;/a&gt;, like RELU. This allows us to add a level of nonlinear complexity with each hidden layer, and with arbitrarily many hidden layers we can construct arbitrarily complicated nonlinear functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because we can (at least in theory) capture any degree of complexity, we think of neural networks as &quot;universal learners,&quot; in that a large enough network could mimic any function.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T16:15:49.970" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="49" PostTypeId="2" ParentId="35" CreationDate="2016-08-02T16:16:25.863" Score="25" Body="&lt;p&gt;Machine learning is a subset of artificial intelligence. Roughly speaking, it corresponds to its learning side. There is no &quot;official&quot; definitions, boundaries are a bit fuzzy.&lt;/p&gt;&#xA;" OwnerUserId="4" LastActivityDate="2016-08-02T16:16:25.863" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="50" PostTypeId="1" AcceptedAnswerId="138" CreationDate="2016-08-02T16:16:46.797" Score="5" ViewCount="8926" Body="&lt;p&gt;How would you estimate the generalization error? What are the methods of achieving this?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2020-11-07T17:26:05.080" LastActivityDate="2020-11-08T13:07:02.920" Title="How can the generalization error be estimated?" Tags="&lt;machine-learning&gt;&lt;deep-neural-networks&gt;&lt;overfitting&gt;&lt;computational-learning-theory&gt;&lt;generalization&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="51" PostTypeId="2" ParentId="42" CreationDate="2016-08-02T16:16:59.330" Score="5" Body="&lt;p&gt;&quot;Hidden&quot; layers really aren't all that special... a hidden layer is really no more than any layer that isn't input or output. So even a very simple 3 layer NN has 1 hidden layer. So I think the question isn't really &quot;How do hidden layers help?&quot; as much as &quot;Why are deeper networks better?&quot;.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;And the answer to that latter question is an area of active research. Even top experts like Geoffrey Hinton and Andrew Ng will freely admit that we don't really understand why deep neural networks work. That is, we don't understand them in complete detail anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, the theory, as I understand it goes something like this...  successive layers of the network learn successively more sophisticated features, which build on the features from preceding layers. So, for example, an NN used for facial recognition might work like this: the first layer detects edges and nothing else. The next layer up recognizes geometric shapes (boxes, circles, etc.). The next layer up recognizes primitive features of a face, like eyes, noses, jaw, etc. The next layer up then recognizes composites based on combinations of &quot;eye&quot; features, &quot;nose&quot; features, and so on.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, in theory, deeper networks (more hidden layers) are better in that they develop a more granular/detailed representation of a &quot;thing&quot; being recognized.  &lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="14723" LastEditDate="2018-04-12T02:30:32.837" LastActivityDate="2018-04-12T02:30:32.837" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="52" PostTypeId="1" AcceptedAnswerId="1437" CreationDate="2016-08-02T16:19:30.337" Score="7" ViewCount="732" Body="&lt;p&gt;I've implemented &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot; rel=&quot;nofollow noreferrer&quot;&gt;the reinforcement learning algorithm&lt;/a&gt; for an agent to play &lt;a href=&quot;https://github.com/admonkey/snappybird&quot; rel=&quot;nofollow noreferrer&quot;&gt;snappy bird&lt;/a&gt; (a shameless cheap ripoff of flappy bird) utilizing a q-table for storing the history for future lookups. It works and eventually achieves perfect convergence after enough training.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q-table? Obviously, storage is a concern with the q-table, but it doesn't seem to ever train with the neural net alone. Perhaps training the NN on an existing q-table would work, but I would like to not use a q-table at all if possible.&lt;/p&gt;&#xA;" OwnerUserId="62" LastEditorUserId="2444" LastEditDate="2019-05-10T14:42:23.103" LastActivityDate="2019-05-10T14:42:23.103" Title="Is it possible to implement reinforcement learning using a neural network?" Tags="&lt;neural-networks&gt;&lt;reinforcement-learning&gt;&lt;deep-rl&gt;&lt;function-approximation&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="53" PostTypeId="2" ParentId="35" CreationDate="2016-08-02T16:20:03.773" Score="65" Body="&lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; has been defined by many people in multiple (often similar) ways [&lt;a href=&quot;http://noiselab.ucsd.edu/ECE228/Murphy_Machine_Learning.pdf&quot; rel=&quot;noreferrer&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://www.cs.ubbcluj.ro/%7Egabis/ml/ML-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf&quot; rel=&quot;noreferrer&quot;&gt;2&lt;/a&gt;]. One definition says that machine learning (ML) is the field of study that gives computers the &lt;em&gt;ability to learn&lt;/em&gt; without being explicitly programmed.&lt;/p&gt;&#xA;&lt;p&gt;Given the above definition, we might say that machine learning is geared towards problems for which we have (lots of) data (experience), from which a program can learn and can get better at a task.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Artificial intelligence&lt;/strong&gt; has many more aspects, where machines may not get better at tasks by learning from data, but may exhibit &lt;em&gt;intelligence&lt;/em&gt; through rules (e.g. expert systems like &lt;a href=&quot;https://en.wikipedia.org/wiki/Mycin&quot; rel=&quot;noreferrer&quot;&gt;Mycin&lt;/a&gt;), &lt;a href=&quot;https://silp.iiita.ac.in/wp-content/uploads/PROLOG.pdf&quot; rel=&quot;noreferrer&quot;&gt;logic&lt;/a&gt; or algorithms, e.g. path-finding).&lt;/p&gt;&#xA;&lt;p&gt;The book &lt;a href=&quot;http://aima.cs.berkeley.edu/&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Artificial Intelligence: A Modern Approach&lt;/em&gt;&lt;/a&gt; shows more research fields of AI, like &lt;em&gt;Constraint Satisfaction Problems&lt;/em&gt;, &lt;em&gt;Probabilistic Reasoning&lt;/em&gt; or &lt;em&gt;Philosophical Foundations&lt;/em&gt;.&lt;/p&gt;&#xA;" OwnerUserId="28" LastEditorUserId="2444" LastEditDate="2021-01-17T20:24:34.557" LastActivityDate="2021-01-17T20:24:34.557" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="54" PostTypeId="1" AcceptedAnswerId="76" CreationDate="2016-08-02T16:20:40.520" Score="7" ViewCount="214" Body="&lt;p&gt;I read that in the spring of 2016 a computer &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_Go&quot; rel=&quot;nofollow noreferrer&quot;&gt;Go program&lt;/a&gt; was finally able to beat a professional human for the first time.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are some of the methods used to program the successful Go-playing program? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are those methods considered to be artificial intelligence?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2021-01-10T01:13:07.280" LastActivityDate="2021-01-10T01:13:07.280" Title="Does the recent advent of a Go playing computer represent Artificial Intelligence?" Tags="&lt;philosophy&gt;&lt;definitions&gt;&lt;agi&gt;&lt;alphago&gt;&lt;narrow-ai&gt;" AnswerCount="4" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="56" PostTypeId="2" ParentId="35" CreationDate="2016-08-02T16:20:52.200" Score="9" Body="&lt;p&gt;Many terms have 'mostly' the same meanings, and so the differences are just in emphasis, perspective, or historical descent. People disagree as to which label refers to the superset or the subset; there are people who will call AI a branch of ML and people who will call ML a branch of AI.&lt;/p&gt;&#xA;&lt;p&gt;I typically hear Machine Learning used as a form of 'applied statistics' where we specify a learning problem in enough detail that we can just feed training data into it and get a useful model out the other side.&lt;/p&gt;&#xA;&lt;p&gt;I typically hear Artificial Intelligence as a catch-all term to refer to any sort of intelligence embedded in the environment or in code. This is a very expansive definition, and others use narrower ones (such as focusing on artificial &lt;em&gt;general&lt;/em&gt; intelligence, which is not domain-specific). (Taken to an extreme, my version includes thermostats.)&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="2444" LastEditDate="2021-01-15T20:47:05.733" LastActivityDate="2021-01-15T20:47:05.733" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="57" PostTypeId="2" ParentId="17" CreationDate="2016-08-02T16:23:13.273" Score="1" Body="&lt;p&gt;The &quot;singularity,&quot; viewed narrowly, refers to a point at which economic growth is so fast that we can't make useful predictions about what the future past that point will look like.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's often used interchangeably with &quot;intelligence explosion,&quot; which is when we get so-called Strong AI, which is AI that is intelligent enough to understand and improve itself. It seems reasonable to expect that the intelligence explosion would immediately lead to an economic singularity, but the reverse is not necessarily true.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T16:23:13.273" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="58" PostTypeId="1" CreationDate="2016-08-02T16:25:20.223" Score="15" ViewCount="4646" Body="&lt;p&gt;Who first coined the term Artificial Intelligence? Is there a published research paper that first used that term?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2019-10-05T02:07:00.143" LastActivityDate="2020-04-21T11:12:25.793" Title="Who first coined the term Artificial Intelligence?" Tags="&lt;terminology&gt;&lt;history&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="60" PostTypeId="1" AcceptedAnswerId="1464" CreationDate="2016-08-02T16:27:49.533" Score="12" ViewCount="555" Body="&lt;p&gt;I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just &lt;em&gt;how&lt;/em&gt; complicated AI is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are other issues currently facing AI development?&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2018-04-12T02:34:45.870" Title="What are the main problems hindering current AI development?" Tags="&lt;machine-learning&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="61" PostTypeId="2" ParentId="36" CreationDate="2016-08-02T16:28:29.363" Score="4" Body="&lt;p&gt;Quantum computers can help further develop A.I. algorithms and solve the problems to the extent of our creativity and ability to define the problem. For example breaking cryptography can take seconds, where it can takes thousands of years for standard computers. The same with artificial intelligence, it can predict all the combinations for the given problem defined by algorithm. This is due to superposition of multiple states of quantum bits.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently, quantum computers are still in the early stages of development and can perform complex calculation. There are already technologies like &lt;a href=&quot;https://en.wikipedia.org/wiki/D-Wave_Systems&quot; rel=&quot;nofollow&quot;&gt;D-Wave&lt;/a&gt; systems which are used by Google and NASA for complex data analysis, using Multi-Qubit type quantum computers for &lt;a href=&quot;https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations&quot; rel=&quot;nofollow&quot;&gt;solving NSE fluid dynamics problems&lt;/a&gt; of interest or global surveillance for military purposes, and many more which we're not aware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently there are only a few quantum computers available to the public, like &lt;a href=&quot;http://www.research.ibm.com/quantum/&quot; rel=&quot;nofollow&quot;&gt;IBM Quantum Experience&lt;/a&gt; (the world’s first quantum computing platform delivered via the IBM Cloud), but it's programming on quantum logic gates levels, so we're many years behind creating artificial intelligence available to public. There are some &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_programming&quot; rel=&quot;nofollow&quot;&gt;quantum computing languages&lt;/a&gt; such as QCL, Q or Quipper, but I'm not aware any libraries which can provide artificial intelligence frameworks. It doesn't mean it's not there, and I'm sure huge companies and governments organisations are using it for their agenda to outcome the competition (like financial market analysis, etc.).&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="8" LastEditDate="2016-08-04T21:06:48.983" LastActivityDate="2016-08-04T21:06:48.983" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="62" PostTypeId="2" ParentId="28" CreationDate="2016-08-02T16:29:11.850" Score="2" Body="&lt;p&gt;Human intelligence is &lt;strong&gt;not&lt;/strong&gt; an example of natural genetic algorithms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Genetic algorithms have collections of solutions that are collided with each other to make new solutions, eventually returning the best solution. Human intelligence is a network of neurons doing information processing, and almost all of it doesn't behave the same way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But that something doesn't behave in the same way that human intelligence does doesn't mean that it's not an AI algorithm; I would include 'genetic algorithms' as a numerical optimization technique, and since optimization and intelligence are deeply linked any numerical optimization technique could be seen as an AI technique.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T16:29:11.850" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="63" PostTypeId="1" AcceptedAnswerId="208" CreationDate="2016-08-02T16:29:24.803" Score="11" ViewCount="272" Body="&lt;p&gt;I've read that the most of the problems can be solved with 1-2 hidden layers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do you know you need more than 2? For what kind of problems you would need them (give me an example)?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="14723" LastEditDate="2018-04-11T18:49:51.953" LastActivityDate="2018-04-11T18:49:51.953" Title="What kind of problems require more than 2 hidden layers?" Tags="&lt;deep-neural-networks&gt;&lt;hidden-layers&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="64" PostTypeId="1" CreationDate="2016-08-02T16:29:29.207" Score="4" ViewCount="81" Body="&lt;p&gt;What were the first areas of research into Artificial Intelligence and what were some early successes?  More recently we've had:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Beating a human at the game of chess&lt;/li&gt;&#xA;&lt;li&gt;Convincing a human that a person was conversing with them (passing the Turing test)&lt;/li&gt;&#xA;&lt;li&gt;Beating a human at Jeopardy game show&lt;/li&gt;&#xA;&lt;li&gt;Beating a human at the game of go.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Were there milestones that were considered major in the field before the 1990s?&lt;/p&gt;&#xA;" OwnerUserId="55" LastActivityDate="2017-03-15T05:38:48.220" Title="What were the first areas of research and what were some early successes?" Tags="&lt;history&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="65" PostTypeId="2" ParentId="41" CreationDate="2016-08-02T16:30:28.737" Score="12" Body="&lt;p&gt;Short answer: No.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Longer answer: It depends on what IQ exactly is, and when the question is asked compared to ongoing development. The topic you're referring to is actually more commonly described as AGI, or Artificial General Intelligence, as opposed to AI, which could be any narrow problem solving capability represented in software/hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Intelligence_quotient&quot;&gt;Intelligence quotient&lt;/a&gt; is a rough estimate of how well humans are able to generally answer questions they have not previously encountered, but as a predictor it is somewhat flawed, and has many criticisms and detractors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently (2016), no known programs have the ability to generalize, or apply learning from one domain to solving problems in an arbitrarily different domain through an abstract understanding. (However there are programs which can effectively analyze, or break down some information domains into simpler representations.) This seems likely to change as time goes on and both hardware and software techniques are developed toward this goal. Experts widely disagree as to the likely timing and approach of these developments, as well as to the most probable outcomes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's also worth noting that there seems to be a large deficit of understanding as to what exactly consciousness is, and disagreement over whether there is ever likely to be anything in the field of artificial intelligence that compares to it.&lt;/p&gt;&#xA;" OwnerUserId="46" LastEditorUserId="46" LastEditDate="2016-08-02T17:08:34.127" LastActivityDate="2016-08-02T17:08:34.127" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="66" PostTypeId="2" ParentId="58" CreationDate="2016-08-02T16:31:24.350" Score="14" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist))&quot; rel=&quot;noreferrer&quot;&gt;John McCarthy&lt;/a&gt; (1927 - 2011) was an American computer scientist. A pioneer in the foundations of artificial intelligence research, &lt;strong&gt;he coined the term &quot;artificial intelligence&quot;&lt;/strong&gt;. He was one of the creators of the (original) Lisp programming language, which was quite involved in early AI research in the 1960s and 1970s.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;He coined the term in 1955, and organized the first Artificial Intelligence conference in 1956, while working as a math teacher at Dartmouth. He founded the AI labs at MIT and Stanford.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;He's responsible for developing several other important concepts in today's mainstream computer science. Namely, he developed &lt;a href=&quot;https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)&quot; rel=&quot;noreferrer&quot;&gt;garbage collection&lt;/a&gt; (used by a Lisp interpreter) and designed the first &lt;a href=&quot;https://en.wikipedia.org/wiki/Time-sharing&quot; rel=&quot;noreferrer&quot;&gt;time-sharing systems&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On a side note, McCarthy predicted that creating a truly intelligent machine would require &lt;a href=&quot;https://www.nytimes.com/2011/10/26/science/26mccarthy.html&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;1.8 Einsteins and one-tenth the resources of the Manhattan Project&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="71" LastEditorUserId="2444" LastEditDate="2019-10-05T02:11:56.117" LastActivityDate="2019-10-05T02:11:56.117" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="67" PostTypeId="1" AcceptedAnswerId="72" CreationDate="2016-08-02T16:31:51.380" Score="8" ViewCount="7392" Body="&lt;p&gt;Why somebody would use SAT solvers (&lt;a href=&quot;https://en.wikipedia.org/wiki/Boolean_satisfiability_problem&quot; rel=&quot;nofollow&quot;&gt;Boolean satisfiability problem&lt;/a&gt;) to solve their real world problems?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any examples of the real uses of this model?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="10135" LastEditDate="2018-10-18T10:45:09.917" LastActivityDate="2020-08-23T12:05:32.803" Title="What are the real world uses for SAT solvers?" Tags="&lt;models&gt;&lt;problem-solving&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="68" PostTypeId="1" CreationDate="2016-08-02T16:33:52.707" Score="6" ViewCount="201" Body="&lt;p&gt;What evolutionary algorithms are there that model or incorporate some notion of &lt;a href=&quot;https://en.wikipedia.org/wiki/Epigenetics&quot; rel=&quot;nofollow noreferrer&quot;&gt;epigenetics&lt;/a&gt;? What are the pros/cons of those approaches? Are there vast insufficiencies or wide-open questions about their usefulness?&lt;/p&gt;&#xA;" OwnerUserId="46" LastEditorUserId="2444" LastEditDate="2021-01-10T22:24:44.517" LastActivityDate="2021-01-10T22:24:44.517" Title="What evolutionary algorithms are there that model epigenetics?" Tags="&lt;reference-request&gt;&lt;genetic-algorithms&gt;&lt;evolutionary-algorithms&gt;&lt;genetic-programming&gt;&lt;evolutionary-computation&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="69" PostTypeId="2" ParentId="54" CreationDate="2016-08-02T16:34:26.163" Score="6" Body="&lt;p&gt;It doesn't make much sense to have a single threshold with &quot;unintelligent&quot; below it and &quot;intelligent&quot; above it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think it makes more sense to have a gradation of intelligence by cognitive task. Inverting a matrix is a 'cognitive task,' and one where working memory pays off immensely; computers have been much better at that cognitive task than humans for a long time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What the AlphaGo victory represents has several components. One is that we have algorithms that are competitive with the best board-game playing humans at doing tactical and strategic thinking in the well-described world of Go. Another is that the deeper structure of the human visual system seems to have been duplicated, and so we have algorithms that can recognize patterns as well as humans--with &lt;em&gt;very&lt;/em&gt; limited resolution. (AlphaGo is seeing one pixel per stone, whereas we have very, very high-resolution eyes and the visual cortex to match.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Different people have different intuitions, but it seems to me that visual intelligence is a huge component of human intelligence in general. If we know most of the secrets of human visual intelligence, that means there might be many tasks that computers could now perform as well as humans (if provided the correct training data).&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T16:34:26.163" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="70" PostTypeId="1" CreationDate="2016-08-02T16:38:55.800" Score="27" ViewCount="453" Body="&lt;p&gt;Can a Convolutional Neural Network be used for pattern recognition in problem domains without image data? For example, by representing abstract data in an image-like format with spatial relations? Would that always be less efficient?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://youtu.be/py5byOOHZM8?t=815&quot; rel=&quot;noreferrer&quot; title=&quot;This Developer&quot;&gt;This developer&lt;/a&gt; says current development could go further but not if there's a limit outside image recognition.&lt;/p&gt;&#xA;" OwnerUserId="46" LastEditorUserId="1641" LastEditDate="2021-01-11T19:03:23.563" LastActivityDate="2021-01-11T19:03:23.563" Title="Is the pattern recognition capability of CNNs limited to image processing?" Tags="&lt;deep-neural-networks&gt;&lt;neural-networks&gt;&lt;image-recognition&gt;&lt;convolutional-neural-networks&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="71" PostTypeId="2" ParentId="46" CreationDate="2016-08-02T16:39:00.200" Score="11" Body="&lt;p&gt;The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 30s, 40s and early 50s (e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Logic&quot; rel=&quot;nofollow noreferrer&quot;&gt;formal logic&lt;/a&gt;, automata, &lt;a href=&quot;https://en.wikipedia.org/wiki/Robot&quot; rel=&quot;nofollow noreferrer&quot;&gt;robots&lt;/a&gt;). Although the &lt;a href=&quot;https://academic.oup.com/mind/article/LIX/236/433/986238&quot; rel=&quot;nofollow noreferrer&quot;&gt;Turing test&lt;/a&gt; was proposed in the 1950s by &lt;a href=&quot;https://en.wikipedia.org/wiki/Alan_Turing&quot; rel=&quot;nofollow noreferrer&quot;&gt;Alan Turing&lt;/a&gt;, the work culminated back in the 1940s in the invention of the programmable digital computers, an abstract essence of mathematical reasoning. These ideas were inspired by a handful of scientists from a variety of fields who began seriously considering the possibility of building an electronic brain. &lt;a href=&quot;https://ojs.aaai.org//index.php/aimagazine/article/view/1904&quot; rel=&quot;nofollow noreferrer&quot;&gt;The field of artificial intelligence research was officially founded as an academic discipline in 1956 during the Dartmouth workshop&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;However, the concept of artificial beings is not new and it's as old as the Greek myths of Hephaestus and Pygmalion, which incorporated the idea of intelligent robots (such as &lt;em&gt;Talos&lt;/em&gt;) and artificial beings (such as &lt;em&gt;Galatea&lt;/em&gt; and &lt;em&gt;Pandora&lt;/em&gt;).&lt;/p&gt;&#xA;&lt;p&gt;See the following articles at Wikipedia for further details:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_intelligence#History&quot; rel=&quot;nofollow noreferrer&quot;&gt;Artificial intelligence (AI)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/History_of_artificial_intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;History of artificial intelligence&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;Timeline of artificial intelligence&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-31T16:46:15.343" LastActivityDate="2021-01-31T16:46:15.343" CommentCount="0" CommunityOwnedDate="2021-01-31T16:43:08.193" ContentLicense="CC BY-SA 4.0" />
  <row Id="72" PostTypeId="2" ParentId="67" CreationDate="2016-08-02T16:39:12.377" Score="5" Body="&lt;p&gt;Instead of talking about just SAT solvers, let me talk about optimization in general. Many economic problems can be cast as optimization problems: for example, FedEx may have a list of packages and the destinations for those packages, and must decide which packages to put on which trucks, and what order to deliver those packages in.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you write out a mathematical description of this problem, there are a truly stunning number of possible solutions, and a well-defined way to evaluate which of two solutions is better. A solver is an algorithm that will evaluate a solution, come up with another solution, and then evaluate that one, and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In small cases and simple problems, the solver can also terminate with a proof that it is actually the best solution possible. But typically instead the solver just reports &quot;this is the best solution that I've seen,&quot; and that's used. An improvement in the solver means you can reliably get lower-cost solutions than you were seeing before.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the SAT problem specifically, the Wikipedia page on &lt;a href=&quot;https://en.wikipedia.org/wiki/Boolean_satisfiability_problem&quot; rel=&quot;nofollow&quot;&gt;SAT&lt;/a&gt; gives some examples:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Since the SAT problem is NP-complete, only algorithms with exponential worst-case complexity are known for it. In spite of this, efficient and scalable algorithms for SAT were developed over the last decade[when?] and have contributed to dramatic advances in our ability to automatically solve problem instances involving tens of thousands of variables and millions of constraints (i.e. clauses).[1] Examples of such problems in electronic design automation (EDA) include formal equivalence checking, model checking, formal verification of pipelined microprocessors,[12] automatic test pattern generation, routing of FPGAs,[14] planning, and scheduling problems, and so on. A SAT-solving engine is now considered to be an essential component in the EDA toolbox.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="10" LastEditorUserId="42" LastEditDate="2016-08-11T14:54:21.720" LastActivityDate="2016-08-11T14:54:21.720" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="73" PostTypeId="2" ParentId="37" CreationDate="2016-08-02T16:40:20.240" Score="10" Body="&lt;p&gt;A Markov model includes the probability of transitioning to each state considering the current state. &quot;Each state&quot; may be just one point - whether it rained on specific day, for instance - or it might look like multiple things - like a pair of words. You've probably seen automatically generated weird text that &lt;em&gt;almost&lt;/em&gt; makes sense, like &lt;a href=&quot;https://blog.codinghorror.com/markov-and-you/&quot;&gt;Garkov&lt;/a&gt; (the output of a Markov model based on the Garfield comic strips). That Coding Horror article also mentions the applications of Markov techniques to Google's PageRank.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Markov models are really only powerful when they have a lot of input to work with. If a machine looked through a lot of English text, it would get a pretty good idea of what words generally come after other words. Or after looking through someone's location history, it could figure out where that person is likely to go next from a certain place. Constantly updating the &quot;input corpus&quot; as more data is received would let the machine tune the probabilities of all the state transitions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Genetic algorithms are fairly different things. They create functions by shuffling around parts of functions and seeing how good each function is at a certain task. A child algorithm will depend on its parents, but Markov models are interested mostly in predicting what thing will come next in a sequence, not creating a new chunk of code. You might be able to use a Markov model to spit out a candidate function, though, depending on how simple the &quot;alphabet&quot; is. You could even then give more weight to the transitions in successful algorithms.&lt;/p&gt;&#xA;" OwnerUserId="75" LastActivityDate="2016-08-02T16:40:20.240" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="74" PostTypeId="1" AcceptedAnswerId="141" CreationDate="2016-08-02T16:42:35.817" Score="50" ViewCount="23690" Body="&lt;p&gt;I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2019-06-20T20:32:04.477" LastActivityDate="2019-06-20T20:32:04.477" Title="What is the difference between strong-AI and weak-AI?" Tags="&lt;terminology&gt;&lt;definitions&gt;&lt;agi&gt;&lt;comparison&gt;&lt;narrow-ai&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="75" PostTypeId="1" CreationDate="2016-08-02T16:42:53.110" Score="0" ViewCount="125" Body="&lt;p&gt;As AI gains capabilities, and becomes more prevalent in society, our legal system will encounter questions it has not encountered before.  For example, if a self-driving car is involved in an accident while being controlled by the AI, who is at fault?  The &quot;driver&quot; (who's really just a passenger), the programmer(s) who made the AI, or the AI itself?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, what's on the cutting edge in terms of these kinds of issues at the intersection of law and artificial intelligence?&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="33" LastEditDate="2016-08-02T18:00:37.377" LastActivityDate="2018-07-17T00:07:33.083" Title="What's the state of the art w.r.t research on the legal aspects of Artificial Intelligence?" Tags="&lt;legal&gt;" AnswerCount="1" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="76" PostTypeId="2" ParentId="54" CreationDate="2016-08-02T16:43:14.880" Score="9" Body="&lt;p&gt;There are at least two questions in your question: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;What are some of the methods used to program the successful go playing program?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Are those methods considered to be artificial intelligence?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The first question is deep and technical, the second broad and philosophical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The methods have been described in: &lt;a href=&quot;https://www.researchgate.net/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mastering the Game of Go with Deep Neural Networks and Tree Search&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem of Go or perfect information games in general is that:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;exhaustive search is infeasible.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So the methods will concentrate on shrinking the search space in an efficient way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Methods and structures described in the paper include:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;learning from expert human players in a supervised fashion&lt;/li&gt;&#xA;&lt;li&gt;learning by playing against itself (reinforcement learning)&lt;/li&gt;&#xA;&lt;li&gt;Monte-Carlo tree search (MCTS) combined with policy and value networks&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The second question has no definite answer, as you will have at least two angles on AI: &lt;a href=&quot;https://en.wikipedia.org/wiki/Chinese_room#Strong_AI&quot; rel=&quot;nofollow noreferrer&quot;&gt;strong&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Weak_AI&quot; rel=&quot;nofollow noreferrer&quot;&gt;weak&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;All real-world systems labeled &quot;artificial intelligence&quot; of any sort are &lt;strong&gt;weak AI at most&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So, yes, it is artificial intelligence, but it is non-sentient.&lt;/p&gt;&#xA;" OwnerUserId="28" LastEditorUserId="2444" LastEditDate="2020-03-09T21:29:34.550" LastActivityDate="2020-03-09T21:29:34.550" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="77" PostTypeId="1" AcceptedAnswerId="131" CreationDate="2016-08-02T16:45:01.487" Score="29" ViewCount="3155" Body="&lt;p&gt;I know that language of Lisp was used early on when working on artificial intelligence problems. Is it still being used today for significant work? If not, is there a new language that has taken its place as the most common one being used for work in AI today?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="14723" LastEditDate="2018-04-14T01:47:35.577" LastActivityDate="2021-01-10T23:03:14.497" Title="Is Lisp still being used to tackle AI problems?" Tags="&lt;history&gt;&lt;programming-languages&gt;&lt;lisp&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="79" PostTypeId="2" ParentId="75" CreationDate="2016-08-02T16:45:51.657" Score="3" Body="&lt;p&gt;One person working in this space is Dr. Woody Barfield. He just wrote a book titled &lt;em&gt;&lt;a href=&quot;http://www.springer.com/us/book/9783319250489&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cyberhumans: Our Future With Machines&lt;/a&gt;&lt;/em&gt; that focuses largely on the legal/policy issues around AI (and related topics). In addition to the book, he is continuing with other research in this area.&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="14723" LastEditDate="2018-04-14T02:00:29.083" LastActivityDate="2018-04-14T02:00:29.083" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="80" PostTypeId="1" AcceptedAnswerId="85" CreationDate="2016-08-02T16:46:07.253" Score="14" ViewCount="2131" Body="&lt;p&gt;What are the specific requirements of the Turing test? &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What requirements if any must the evaluator fulfill in order to be qualified to give the test?&lt;/li&gt;&#xA;&lt;li&gt;Must there always be two participants in the conversation (one human and one computer) or can there be more?&lt;/li&gt;&#xA;&lt;li&gt;Are placebo tests (where there is not actually a computer involved) allowed or encouraged?&lt;/li&gt;&#xA;&lt;li&gt;Can there be multiple evaluators? If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="96" LastEditorUserId="2444" LastEditDate="2020-05-18T23:55:50.073" LastActivityDate="2020-05-18T23:55:50.073" Title="What are the specific requirements of the Turing test?" Tags="&lt;natural-language-processing&gt;&lt;intelligence-testing&gt;&lt;turing-test&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="81" PostTypeId="1" AcceptedAnswerId="87" CreationDate="2016-08-02T16:49:40.830" Score="8" ViewCount="2521" Body="&lt;p&gt;I believe that statistical AI uses inductive thought processes. For example, deducing a trend from a pattern, after training.&lt;/p&gt;&#xA;&lt;p&gt;What are some examples of successfully applied Statistical AI to real-world problems?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2021-12-06T08:29:54.127" LastActivityDate="2021-12-06T08:29:54.127" Title="What are some examples of Statistical AI applications?" Tags="&lt;applications&gt;&lt;statistical-ai&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="82" PostTypeId="1" CreationDate="2016-08-02T16:50:15.330" Score="1" ViewCount="116" Body="&lt;p&gt;How do the basic components &lt;a href=&quot;https://en.wikipedia.org/wiki/Optimality_theory&quot; rel=&quot;nofollow&quot;&gt;optimality theory&lt;/a&gt; apply to artificial intelligence?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How is optimality theory related to neural network research?&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="2444" LastEditDate="2019-04-05T12:19:34.277" LastActivityDate="2019-04-05T12:19:34.277" Title="What is the relation between optimality theory and AI?" Tags="&lt;neural-networks&gt;&lt;applications&gt;&lt;comparison&gt;" AnswerCount="0" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="83" PostTypeId="2" ParentId="1" CreationDate="2016-08-02T16:54:40.380" Score="3" Body="&lt;p&gt;Yes, as Franck has rightly put, &quot;backprop&quot; means backpropogation, which is frequently used in the domain of neural networks for error optimization.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a detailed explanation, I would point out &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap2.html&quot; rel=&quot;nofollow&quot;&gt;this tutorial&lt;/a&gt; on the concept of backpropogation by a very good book of Michael Nielsen. &lt;/p&gt;&#xA;" OwnerUserId="101" LastActivityDate="2016-08-02T16:54:40.380" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="84" PostTypeId="1" CreationDate="2016-08-02T16:55:37.050" Score="11" ViewCount="1115" Body="&lt;p&gt;Some programs do exhaustive searches for a solution while others do heuristic searches for a similar answer. For example, in chess, the search for the best next move tends to be more exhaustive in nature whereas, in Go, the search for the best next move tends to be more heuristic in nature due to the much larger search space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI? If so, is the chess-playing computer beating a human professional seen as a meaningful milestone?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="14723" LastEditDate="2018-04-14T02:03:13.207" LastActivityDate="2018-12-11T09:53:09.487" Title="Are methods of exhaustive search considered to be AI?" Tags="&lt;gaming&gt;&lt;search&gt;&lt;chess&gt;&lt;heuristics&gt;" AnswerCount="5" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="85" PostTypeId="2" ParentId="80" CreationDate="2016-08-02T16:59:11.467" Score="10" Body="&lt;p&gt;The &quot;Turing Test&quot; is generally taken to mean an updated version of the Imitation Game Alan Turing proposed in his 1951 paper of the same name. An early version had a human (male or female) and a computer, and a judge had to decide which is which, and what gender they were if human. If they were correct less than 50% then the computer was considered &quot;intelligent.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The current generally accepted version requires only one contestant, and a judge to decide whether it is human or machine. So yes, sometimes this will be a placebo, effectively, if we consider a human to be a placebo.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your first and fourth questions are related - and there are no strict guidelines. If the computer can fool a greater number of judges then it will of course be considered a better AI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The University of Toronto has a validity section in &lt;a href=&quot;http://www.psych.utoronto.ca/users/reingold/courses/ai/turing.html&quot;&gt;this paper on Turing&lt;/a&gt;, which includes a link to &lt;a href=&quot;http://ciips.ee.uwa.edu.au/Papers/Technical_Reports/1997/05/Index.html&quot;&gt;Jason Hutchens' commentary&lt;/a&gt; on why the Turing test may not be relevant (humans may also fail it) and the &lt;a href=&quot;http://www.loebner.net/Prizef/loebner-prize.html&quot;&gt;Loebner Prize&lt;/a&gt;, a formal instantiation of a Turing Test .&lt;/p&gt;&#xA;" OwnerUserId="97" LastEditorUserId="97" LastEditDate="2016-08-02T17:07:15.277" LastActivityDate="2016-08-02T17:07:15.277" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="86" PostTypeId="1" AcceptedAnswerId="93" CreationDate="2016-08-02T16:59:30.683" Score="31" ViewCount="1249" Body="&lt;p&gt;How is a neural network having the &quot;deep&quot; adjective actually distinguished from other similar networks?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-02-23T22:05:20.793" LastActivityDate="2020-08-23T11:06:25.767" Title="How is a deep neural network different from other neural networks?" Tags="&lt;neural-networks&gt;&lt;machine-learning&gt;&lt;deep-neural-networks&gt;&lt;terminology&gt;&lt;comparison&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="87" PostTypeId="2" ParentId="81" CreationDate="2016-08-02T16:59:50.293" Score="10" Body="&lt;p&gt;There are several examples. For example, one instance of using Statistical AI from my workplace is:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Analyzing the behavior of the customer and their food-ordering trends, and then trying to upsell by recommending them the dishes which they might like to order/eat. This can be done through the apriori and FP-growth algorithms. We then, automated the algorithm, and then the algorithm improves itself through an &lt;code&gt;Ordered/Not-Ordered&lt;/code&gt; metric.&lt;/li&gt;&#xA;&lt;li&gt;Self-driving cars. They use reinforcement and supervised learning algorithms for learning the route and the gradient/texture of the road.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="101" LastEditorUserId="14723" LastEditDate="2018-04-12T14:09:57.913" LastActivityDate="2018-04-12T14:09:57.913" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="88" PostTypeId="1" AcceptedAnswerId="2573" CreationDate="2016-08-02T17:01:18.317" Score="4" ViewCount="601" Body="&lt;p&gt;What is the effectiveness of pre-training of unsupervised deep learning?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does unsupervised deep learning actually work?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-11-19T21:16:17.260" LastActivityDate="2020-04-21T11:14:24.307" Title="Why does unsupervised pre-training help in deep learning?" Tags="&lt;deep-learning&gt;&lt;unsupervised-learning&gt;&lt;transfer-learning&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="89" PostTypeId="2" ParentId="84" CreationDate="2016-08-02T17:02:05.890" Score="7" Body="&lt;p&gt;If a computer is just brute-forcing the solution, it's not learning anything or using any kind of intelligence at all, and therefore it shouldn't be called &quot;artificial intelligence.&quot; It has to make decisions based on what's happened before in similar instances. For something to be intelligent, it needs a way to keep track of what it's learned. A chess program might have a really awesome measurement algorithm to use on every possible board state, but if it's always trying each state and never storing what it learns about different approaches, it's not intelligent.&lt;/p&gt;&#xA;" OwnerUserId="75" LastEditorUserId="75" LastEditDate="2016-08-02T17:33:59.340" LastActivityDate="2016-08-02T17:33:59.340" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="91" PostTypeId="1" AcceptedAnswerId="97" CreationDate="2016-08-02T17:04:35.297" Score="17" ViewCount="468" Body="&lt;p&gt;Are search engines considered AI because of the way they analyze what you search for and remember it? Or how they send you ads of what you've searched for recently? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this considered AI or just smart?&lt;/p&gt;&#xA;" OwnerUserId="5" LastEditorUserId="2444" LastEditDate="2019-06-24T13:10:54.557" LastActivityDate="2019-06-24T13:10:54.557" Title="Are search engines considered AI?" Tags="&lt;machine-learning&gt;&lt;philosophy&gt;&lt;definitions&gt;" AnswerCount="1" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="92" PostTypeId="1" AcceptedAnswerId="250" CreationDate="2016-08-02T17:05:27.590" Score="85" ViewCount="7451" Body="&lt;p&gt;The following &lt;a href=&quot;http://www.evolvingai.org/fooling&quot; rel=&quot;noreferrer&quot;&gt;page&lt;/a&gt;/&lt;a href=&quot;http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf&quot; rel=&quot;noreferrer&quot;&gt;study&lt;/a&gt; demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7pgrH.jpg&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7pgrH.jpg&quot; alt=&quot;Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &amp;gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/pBm48.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/pBm48.png&quot; alt=&quot;Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How this is possible? Can you please explain ideally in plain English?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-02-18T19:48:25.217" LastActivityDate="2021-02-18T19:48:25.217" Title="How is it possible that deep neural networks are so easily fooled?" Tags="&lt;convolutional-neural-networks&gt;&lt;computer-vision&gt;&lt;image-recognition&gt;&lt;deep-neural-networks&gt;&lt;adversarial-ml&gt;" AnswerCount="9" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="93" PostTypeId="2" ParentId="86" CreationDate="2016-08-02T17:06:21.223" Score="32" Body="&lt;p&gt;The difference is mostly in the number of layers. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a long time, it was believed that &quot;1-2 hidden layers are enough for most tasks&quot; and it was impractical to use more than that, because training neural networks can be very computationally demanding.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nowadays, computers are capable of much more, so people have started to use networks with more layers and found that they work very well for some tasks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The word &quot;deep&quot; is there simply to distinguish these networks from the traditional, &quot;more shallow&quot; ones.&lt;/p&gt;&#xA;" OwnerUserId="30" LastEditorUserId="2444" LastEditDate="2019-02-23T22:08:58.833" LastActivityDate="2019-02-23T22:08:58.833" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="94" PostTypeId="1" AcceptedAnswerId="132" CreationDate="2016-08-02T17:06:46.317" Score="3" ViewCount="430" Body="&lt;p&gt;In a feedforward neural network, the inputs are fed directly to the outputs via a series of &lt;strong&gt;weights&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;What purpose do the weights serve, and how are they significant in this neural network?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-12-04T09:23:57.703" LastActivityDate="2021-12-04T09:23:57.703" Title="What is the significance of weights in a feedforward neural network?" Tags="&lt;neural-networks&gt;&lt;weights&gt;&lt;perceptron&gt;&lt;feedforward-neural-networks&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="95" PostTypeId="2" ParentId="86" CreationDate="2016-08-02T17:08:48.340" Score="10" Body="&lt;p&gt;A deep neural network is just a (feed-forward) neural network with many layers.&lt;/p&gt;&#xA;&lt;p&gt;However, deep belief networks, Deep Boltzmann networks, etc., are not considered (debatable) deep neural networks, as their topology is different (i.e. they have undirected networks in their topology).&lt;/p&gt;&#xA;&lt;p&gt;See also this: &lt;a href=&quot;https://stats.stackexchange.com/a/59854/84191&quot;&gt;https://stats.stackexchange.com/a/59854/84191&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="101" LastEditorUserId="2444" LastEditDate="2020-08-23T11:06:25.767" LastActivityDate="2020-08-23T11:06:25.767" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="96" PostTypeId="1" AcceptedAnswerId="98" CreationDate="2016-08-02T17:12:58.533" Score="11" ViewCount="348" Body="&lt;p&gt;What is the definition of a deep neural network? Why are they so popular or important?&lt;/p&gt;&#xA;" OwnerUserId="5" LastEditorUserId="2444" LastEditDate="2019-06-23T12:18:37.573" LastActivityDate="2020-04-21T11:25:25.153" Title="What is a deep neural network?" Tags="&lt;machine-learning&gt;&lt;deep-learning&gt;&lt;terminology&gt;&lt;deep-neural-networks&gt;&lt;definitions&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="0" ClosedDate="2020-08-23T11:03:13.587" ContentLicense="CC BY-SA 4.0" />
  <row Id="97" PostTypeId="2" ParentId="91" CreationDate="2016-08-02T17:15:22.887" Score="22" Body="&lt;p&gt;I believe it would be more accurate to say that (some) search engines &lt;em&gt;use&lt;/em&gt; AI.  Broadly saying &quot;search engines are AI&quot; is not really correct. At the core, most search engines are nothing more than an inverted text index using something like tf–idf scoring. That's a very mechanical/simple thing that nobody would really call AI. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But more sophisticated search engines may &lt;em&gt;use&lt;/em&gt; AI or AI techniques to do things like semantic analysis - so they can actually &quot;answer questions&quot; instead of just looking up words in an index.  &lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="14723" LastEditDate="2018-04-13T18:07:34.777" LastActivityDate="2018-04-13T18:07:34.777" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="98" PostTypeId="2" ParentId="96" CreationDate="2016-08-02T17:18:12.383" Score="12" Body="&lt;p&gt;A deep neural network (DNN) is nothing but a neural network which has multiple layers, where &lt;em&gt;multiple&lt;/em&gt; can be subjective.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;IMHO, any network which has 6 or 7 or more layers is considered deep. So, the above would form a very basic definition of a deep network. &lt;/p&gt;&#xA;" OwnerUserId="101" LastEditorUserId="2444" LastEditDate="2019-06-24T10:56:42.927" LastActivityDate="2019-06-24T10:56:42.927" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="100" PostTypeId="2" ParentId="96" CreationDate="2016-08-02T17:36:52.900" Score="5" Body="&lt;p&gt;Deep networks have two main differences with 'normal' networks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first is that computational power and training datasets have grown immensely, meaning that it's practical to run larger networks and statistically valid (that is, we have enough training examples that we won't just run into over-fitting problems with larger networks).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second is that back propagation is limited the more layers you have; each layer represents a gradient of the error, and so by the time one is about six layers deep there isn't much error left to modify the neuron weights. But one might reasonably expect earlier neurons to be more important than later neurons, since they represent 'concepts' that are closer to the raw inputs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;New training techniques sidestep this problem, typically by doing unsupervised learning on the raw inputs, creating higher-level 'concepts' that are then useful as inputs for supervised learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(For example, consider the problem of determining whether or not an image contains a cat from the pixels. The early layers of the network should be doing things like detecting edges, which one could expect to be shared among all images and mostly independent of what one is trying to do with the output layers, thus also hard to train through 'cat-not cat' signals many layers up.&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="5" LastEditDate="2016-08-03T15:13:11.603" LastActivityDate="2016-08-03T15:13:11.603" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="101" PostTypeId="2" ParentId="84" CreationDate="2016-08-02T17:38:38.237" Score="8" Body="&lt;p&gt;If one thinks of intelligence as a continuous measure of optimization power (that is, how much better are outcomes for any unit of cognitive effort expended), then exhaustive search has non-zero intelligence (in that it does actually give better outcomes as more effort is expended) but &lt;em&gt;very, very low&lt;/em&gt; intelligence (as the outcomes are better mostly by luck, and the amount of effort expended can be impossibly large).&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T17:38:38.237" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="102" PostTypeId="2" ParentId="80" CreationDate="2016-08-02T17:42:05.387" Score="3" Body="&lt;p&gt;There are really two questions here, that I can see. One is &quot;what were the specific requirements of the original Turing test, as stated by Turing himself?&quot; The other is &quot;What should the specific requirements of a modern Turing test be?&quot; Things have advanced a lot since Turing's day, and I think it's reasonable for us to consider extending/modifying his test to reflect our current understanding.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The answer to the first question is easy enough to look up, so I think the interesting one is the second one. What &lt;em&gt;should&lt;/em&gt; a test to determine intelligence look like? With that in mind, I think the answer to all four questions posed by the OP is &quot;it depends&quot;. I don't think there's universal consensus on how to structure a perfect Turing test, so a given experimenter is really free to set things up however he/she wants.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is all, of course, based on the assumption that the Turing test or a Turing Test-like test is actually of value. That's not necessarily a given.  Consider that, to some extent, what we're talking about is designing an AI with an exceptional ability for deceit! That is, assuming the questioner is allowed to simply ask &quot;are you human&quot;, then we have to assume that the AI is supposed to lie if it wants to pass the test. So one might rightly ask, is designing a system to be really good at telling lies, a valuable approach to AI?&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="14723" LastEditDate="2018-04-13T03:22:45.343" LastActivityDate="2018-04-13T03:22:45.343" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="103" PostTypeId="1" CreationDate="2016-08-02T17:47:41.750" Score="5" ViewCount="5591" Body="&lt;p&gt;I believe that Classical AI uses deductive thought processes. For example, given as a set of constraints, deduce a conclusion.&lt;/p&gt;&#xA;&lt;p&gt;What are some examples of successfully applying Classical AI to real-world problems?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2021-12-06T08:27:14.780" LastActivityDate="2021-12-06T08:27:14.780" Title="What are some examples of Classical AI applications?" Tags="&lt;applications&gt;&lt;symbolic-ai&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="104" PostTypeId="1" CreationDate="2016-08-02T17:56:02.743" Score="10" ViewCount="317" Body="&lt;p&gt;In &lt;a href=&quot;https://youtu.be/oSdPmxRCWws?t=30&quot;&gt;this video&lt;/a&gt; an expert says, &quot;One way of thinking about what intelligence is [specifically with regard to artificial intelligence], is as an optimization process.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can intelligence always be thought of as an optimization process, and can artificial intelligence always be modeled as an optimization problem? What about pattern recognition? Or is he mischaracterizing?&lt;/p&gt;&#xA;" OwnerUserId="46" LastEditorUserId="46" LastEditDate="2016-08-02T18:01:01.713" LastActivityDate="2018-04-13T18:00:00.527" Title="Can artificial intelligence be thought of as optimization?" Tags="&lt;optimization&gt;&lt;agi&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="105" PostTypeId="2" ParentId="103" CreationDate="2016-08-02T18:12:04.120" Score="8" Body="&lt;p&gt;The term &lt;em&gt;classical AI&lt;/em&gt; refers to the concept of intelligence that was broadly accepted after the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dartmouth_workshop&quot; rel=&quot;nofollow noreferrer&quot;&gt;Dartmouth Conference&lt;/a&gt; and basically refers to a kind of intelligence that is strongly symbolic and oriented to logic and language processing. One basic point is the duality &lt;strong&gt;body&lt;/strong&gt; vs. &lt;strong&gt;mind&lt;/strong&gt;. It's in this period that the mind starts to be compared with computer software.&lt;/p&gt;&#xA;&lt;h3&gt;Two classical historical examples of this conception of intelligence&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;strong&gt;Deep Blue&lt;/strong&gt;&lt;/a&gt;, whose aim in life was to be the master of chess, ruling over the (not-so) intelligent mankind&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/ELIZA&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;strong&gt;Eliza&lt;/strong&gt;&lt;/a&gt; a computer-based therapist that turned out to &lt;a href=&quot;https://en.wikipedia.org/wiki/Joseph_Weizenbaum&quot; rel=&quot;nofollow noreferrer&quot;&gt;trigger a critic&lt;/a&gt; to the classical AI&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2&gt;Two technical examples of classical AI&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Expert systems&lt;/strong&gt;, which are computer programs that strongly rely on the type of constraints and conclusions that you refer to, &lt;a href=&quot;https://www.britannica.com/technology/expert-system&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;in order to accomplish feats of apparent intelligence&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Fuzzy logic&lt;/strong&gt;, &lt;a href=&quot;https://de.mathworks.com/help/fuzzy/what-is-fuzzy-logic.html?requestedDomain=www.mathworks.com&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;which is an extension of multivalued logic&lt;/em&gt;&lt;/a&gt;, but with continuous values instead of discrete ones&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Note that in all cases the &lt;em&gt;hardware&lt;/em&gt; (once compared with the &lt;em&gt;body&lt;/em&gt;) does not play any role: Intelligence is abstract and independent from the material world.&lt;/p&gt;&#xA;" OwnerUserId="70" LastEditorUserId="2444" LastEditDate="2021-01-24T22:04:16.883" LastActivityDate="2021-01-24T22:04:16.883" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="106" PostTypeId="2" ParentId="54" CreationDate="2016-08-02T18:14:37.117" Score="3" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of ever more processing power being applied to the problem?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Neither, really. It is a milestone and a significant advance in computers beating humans in games, but the techniques used are only relevant to that game, not for other purposes in AI. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The solution lies in humans analyzing the game and implementing algorithms for finding a good move. This is the main reason that a computer can beat the humans, together with the fact that it can calculate much faster and that it doesn't make really bad moves by not seeing something.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Processing power helps, but the game-tree complexity for Go is very large, estimated to be larger than 10&lt;sup&gt;200&lt;/sup&gt;, whereas the game-tree complexity for chess is only 10&lt;sup&gt;120&lt;/sup&gt; (known as the Shannon number), so chess is less hard. This means that for neither chess nor go a database can be created with all possible positions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The fact that Deep Blue beat Kasparov in a six-game match in 1997 was quite a development since this was one of the first &quot;hard&quot; games where a computer beat a top human. But it still isn't really Artificial Intelligence, more analyzing the game. Implementing an opening and endgame book was a large part, the middle game was done using analysis, I don't know the details. &lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="14723" LastEditDate="2018-04-12T02:46:33.907" LastActivityDate="2018-04-12T02:46:33.907" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="107" PostTypeId="2" ParentId="81" CreationDate="2016-08-02T18:16:43.960" Score="4" Body="&lt;p&gt;There are many online services that use statistical neural networks for recommendations. For example, we have &lt;a href=&quot;http://imhonet.ru&quot; rel=&quot;nofollow noreferrer&quot;&gt;a well known service&lt;/a&gt; here in Russia that could give it's users recommendations for movies and shows to watch and books to read. Its recommendation core is based on many things known about a user: what movies/books he or she loves and what not, analyses his or her friends like and so on. While you have only a few items rayed it will give you very strange recommendations but then it becomes more accurate and really could give you some true gems.&lt;/p&gt;&#xA;" OwnerUserId="112" LastEditorUserId="1671" LastEditDate="2018-04-13T03:20:26.790" LastActivityDate="2018-04-13T03:20:26.790" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="108" PostTypeId="1" CreationDate="2016-08-02T18:17:44.297" Score="9" ViewCount="2234" Body="&lt;p&gt;What specific advantages of declarative languages make them more applicable to AI than imperative languages?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can declarative languages do easily that other languages styles find difficult for this kind of problem?&lt;/p&gt;&#xA;" OwnerUserId="69" LastEditorUserId="1581" LastEditDate="2018-11-13T17:24:41.510" LastActivityDate="2018-11-18T12:34:56.643" Title="What are the main advantages of using declarative programming languages for building AI?" Tags="&lt;declarative-programming&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="109" PostTypeId="1" CreationDate="2016-08-02T18:29:19.443" Score="4" ViewCount="87" Body="&lt;p&gt;In years past, GOFAI (Good Old Fashioned AI) was heavily based on &quot;rules&quot; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;symbolic computation&lt;/a&gt; based on rules.  Unfortunately, that approach ran into stumbling blocks, and the world moved heavily towards statistical/probabilistic approaches leading to the current wave of interest in &quot;machine learning&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems though, that the symbolic/rule-based approach probably still has application. So, could one &quot;learn&quot; rules using a probabilistic &lt;a href=&quot;https://en.wikipedia.org/wiki/Rule_induction&quot; rel=&quot;nofollow noreferrer&quot;&gt;rule induction&lt;/a&gt; method, and then layer symbolic computation on top? If so, how could the whole process be made truly two-way, so that something &quot;learned&quot; from processing rules, can be fed back into how the system learns rules?&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="14723" LastEditDate="2018-04-14T02:02:43.383" LastActivityDate="2018-08-15T22:45:20.830" Title="Can rule induction be considered a way to &quot;hybridize&quot; probabilistic / statistical approaches and symbolic approaches?" Tags="&lt;symbolic-ai&gt;&lt;symbolic-computing&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="111" PostTypeId="1" AcceptedAnswerId="1790" CreationDate="2016-08-02T18:57:57.550" Score="104" ViewCount="7133" Body="&lt;p&gt;Obviously, self-driving cars aren't perfect, so imagine that the Google car (as an example) got into a difficult situation.&lt;/p&gt;&#xA;&lt;p&gt;Here are a few examples of unfortunate situations caused by a set of events:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),&lt;/li&gt;&#xA;&lt;li&gt;Avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,&lt;/li&gt;&#xA;&lt;li&gt;Killing an animal on the street in favour of a human being,&lt;/li&gt;&#xA;&lt;li&gt;Purposely changing lanes to crash into another car to avoid killing a dog,&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;And here are a few dilemmas:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Does the algorithm recognize the difference between a human being and an animal?&lt;/li&gt;&#xA;&lt;li&gt;Does the size of the human being or animal matter?&lt;/li&gt;&#xA;&lt;li&gt;Does it count how many passengers it has vs. people in the front?&lt;/li&gt;&#xA;&lt;li&gt;Does it &amp;quot;know&amp;quot; when babies/children are on board?&lt;/li&gt;&#xA;&lt;li&gt;Does it take into the account the age (e.g. killing the older first)?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;How would an algorithm decide what it should do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)? &lt;/p&gt;&#xA;&lt;p&gt;Related articles:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/&quot; rel=&quot;noreferrer&quot;&gt;Why Self-Driving Cars Must Be Programmed to Kill&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/&quot; rel=&quot;noreferrer&quot;&gt;How to Help Self-Driving Cars Make Ethical Decisions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastEditorUserId="36530" LastEditDate="2020-06-26T22:37:36.083" LastActivityDate="2020-06-26T22:37:36.083" Title="How could self-driving cars make ethical decisions about who to kill?" Tags="&lt;philosophy&gt;&lt;ethics&gt;&lt;autonomous-vehicles&gt;&lt;decision-theory&gt;" AnswerCount="14" CommentCount="5" ContentLicense="CC BY-SA 4.0" />
  <row Id="112" PostTypeId="1" CreationDate="2016-08-02T18:59:44.230" Score="6" ViewCount="1652" Body="&lt;p&gt;Which deep neural network is used in &lt;a href=&quot;https://en.wikipedia.org/wiki/Google_self-driving_car&quot; rel=&quot;nofollow noreferrer&quot;&gt;Google's driverless cars&lt;/a&gt; to analyze the surroundings? Is this information open to the public?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="14723" LastEditDate="2018-04-13T18:53:56.720" LastActivityDate="2019-06-29T15:24:10.747" Title="Which machine learning algorithm is used in self-driving cars?" Tags="&lt;deep-neural-networks&gt;&lt;algorithm&gt;&lt;autonomous-vehicles&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="113" PostTypeId="1" AcceptedAnswerId="5572" CreationDate="2016-08-02T19:02:33.003" Score="10" ViewCount="3030" Body="&lt;p&gt;Two common activation functions used in deep learning are the hyperbolic tangent function and the sigmoid activation function. I understand that the hyperbolic tangent is just a rescaling and translation of the sigmoid function: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;span class=&quot;math-container&quot;&gt;$\tanh(z) = 2\sigma(z) - 1$&lt;/span&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a significant difference between these two activation functions, and in particular, &lt;strong&gt;when is one preferable to the other&lt;/strong&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I realize that in some cases (like when estimating probabilities) outputs in the range of &lt;span class=&quot;math-container&quot;&gt;$[0,1]$&lt;/span&gt; are more convenient than outputs that range from &lt;span class=&quot;math-container&quot;&gt;$[-1,1]$&lt;/span&gt;. I want to know if there are differences &lt;strong&gt;other than convenience&lt;/strong&gt; which distinguish the two activation functions.&lt;/p&gt;&#xA;" OwnerUserId="127" LastEditorUserId="2444" LastEditDate="2019-05-02T16:09:44.820" LastActivityDate="2019-05-02T16:09:44.820" Title="What's the difference between hyperbolic tangent and sigmoid neurons?" Tags="&lt;neural-networks&gt;&lt;machine-learning&gt;&lt;deep-neural-networks&gt;&lt;comparison&gt;&lt;hidden-layers&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="114" PostTypeId="2" ParentId="36" CreationDate="2016-08-02T19:02:42.300" Score="29" Body="&lt;p&gt;Quantum computers are super awesome at matrix multiplication, &lt;a href=&quot;http://twistedoakstudios.com/blog/Post8887_what-quantum-computers-do-faster-with-caveats&quot; rel=&quot;noreferrer&quot;&gt;with some limitations&lt;/a&gt;. Quantum superposition allows each bit to be in &lt;em&gt;a lot&lt;/em&gt; more states than just zero or one, and quantum gates can fiddle those bits in many different ways. Because of that, a quantum computer can process a lot of information at once for certain applications.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of those applications is the &lt;a href=&quot;http://algorithmicassertions.com/quantum/2014/03/07/Building-your-own-Quantum-Fourier-Transform.html&quot; rel=&quot;noreferrer&quot;&gt;Fourier transform&lt;/a&gt;, which is useful in a lot of problems, like &lt;a href=&quot;https://dsp.stackexchange.com/q/69&quot;&gt;signal analysis&lt;/a&gt; and array processing. There's also &lt;a href=&quot;http://twistedoakstudios.com/blog/Post2644_grovers-quantum-search-algorithm&quot; rel=&quot;noreferrer&quot;&gt;Grover's quantum search algorithm&lt;/a&gt;, which finds the single value for which a given function returns something different. If an AI problem can be expressed in a mathematical form &lt;a href=&quot;http://algorithmicassertions.com/quantum/2014/04/27/The-Not-Quantum-Laplace-Transform.html&quot; rel=&quot;noreferrer&quot;&gt;amenable to quantum computing&lt;/a&gt;, it can receive great speedups. Sufficient speedups could transform an AI idea from &quot;theoretically interesting but insanely slow&quot; to &quot;quite practical once we get a good handle on quantum computing.&quot;&lt;/p&gt;&#xA;" OwnerUserId="75" LastEditorUserId="-1" LastEditDate="2017-04-13T12:47:29.797" LastActivityDate="2016-08-02T19:02:42.300" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="116" PostTypeId="2" ParentId="108" CreationDate="2016-08-02T19:16:46.910" Score="5" Body="&lt;p&gt;The advantage of a declarative language like Prolog is that it can be used to express facts and inference rules separately from control flow.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This allows the developer to focus on the data and inference rules (the knowledge model), and allows the developer to extend the knowledge model more easily.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I should add that in practice, this dichotomy between facts/rules on the one hand, and control flow on the other, is not strict. A knowledge engineer who writes a code base in Prolog does sometimes have to consider control flow. The &quot;!&quot; operator is used so that the developer can influence the evaluation of the rules.&lt;/p&gt;&#xA;" OwnerUserId="66" LastActivityDate="2016-08-02T19:16:46.910" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="117" PostTypeId="2" ParentId="104" CreationDate="2016-08-02T19:18:06.670" Score="10" Body="&lt;p&gt;A good answer to this question depends on what you want to use the labels for.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I think about &quot;optimization,&quot; I think about a solution space and a cost function; that is, there are many possible answers that could be returned and we can know what the cost is of any particular answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this view, the answer is &quot;yes&quot;--pattern recognition is a case where each pattern is a possible answer, and the optimization method is trying to find the one where the cost is lowest (that is, where the answer matches what you want it to match).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But most interesting optimization problems are characterized by exponential solution spaces and clean cost functions, and so can be thought of more as 'search' problems, whereas most pattern recognition problems are characterized by simple solution spaces and complicated cost functions, and it might feel unnatural to put the two of them together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(In general, I do think that optimization and intelligence are deeply linked enough that optimization power is a good measure of intelligence, and certainly a better measure of the &lt;em&gt;practical&lt;/em&gt; use of intelligence than pattern recognition.)&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T19:18:06.670" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="118" PostTypeId="1" AcceptedAnswerId="122" CreationDate="2016-08-02T19:22:20.577" Score="7" ViewCount="488" Body="&lt;p&gt;&lt;a href=&quot;https://ai.stackexchange.com/questions/10/what-is-fuzzy-logic&quot;&gt;Fuzzy logic&lt;/a&gt; is the logic where every statement can have any real truth value between 0 and 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can fuzzy logic be used in creating AI? Is it useful for certain decision problems involving multiple inputs? Can you give an example of an AI that uses it?&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="2444" LastEditDate="2021-11-10T15:51:59.617" LastActivityDate="2021-11-10T15:51:59.617" Title="How can fuzzy logic be used in creating AI?" Tags="&lt;applications&gt;&lt;fuzzy-logic&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="119" PostTypeId="2" ParentId="113" CreationDate="2016-08-02T19:28:00.640" Score="3" Body="&lt;p&gt;I don't think it makes sense to decide activation functions based on desired properties of the output; you can easily insert a calibration step that maps the 'neural network score' to whatever units you actually want to use (dollars, probability, etc.).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I think preference between different activation functions mostly boils down to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions&quot; rel=&quot;nofollow&quot;&gt;different properties&lt;/a&gt; of those activation functions (like whether or not they're continuously differentiable). Because there's just a linear transformation between the two, I think that means there isn't a meaningful difference between them.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T19:28:00.640" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="120" PostTypeId="1" AcceptedAnswerId="125" CreationDate="2016-08-02T19:31:01.370" Score="5" ViewCount="318" Body="&lt;p&gt;In &lt;a href=&quot;https://philpapers.org/rec/LUCMMA-8&quot; rel=&quot;nofollow noreferrer&quot;&gt;Minds, Machines and Gödel&lt;/a&gt; (1959), J. R. Lucas shows that any human mathematician can not be represented by an algorithmic automaton (a Turing Machine, but any computer is equivalent to it by the Church-Turing thesis), using Gödel's incompleteness theorem.&lt;/p&gt;&#xA;&lt;p&gt;As I understand it, he states that since the computer is an algorithm and hence a formal system, Gödel's incompleteness theorem applies. But a human mathematician also has to work in a formal axiom system to prove a theorem, so wouldn't it apply there as well?&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="2444" LastEditDate="2021-02-02T01:07:51.347" LastActivityDate="2021-02-02T01:07:51.347" Title="How does Lucas's argument work?" Tags="&lt;philosophy&gt;&lt;agi&gt;&lt;artificial-consciousness&gt;&lt;incompleteness-theorems&gt;&lt;turing-machine&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="121" PostTypeId="2" ParentId="118" CreationDate="2016-08-02T19:34:16.217" Score="3" Body="&lt;p&gt;My impression is that fuzzy logic has mostly declined in relevance and &lt;a href=&quot;https://en.wikipedia.org/wiki/Probabilistic_logic&quot; rel=&quot;nofollow&quot;&gt;probabilistic logic&lt;/a&gt; has taken over its niche. (See the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fuzzy_logic#Comparison_to_probability&quot; rel=&quot;nofollow&quot;&gt;comparison on Wikipedia&lt;/a&gt;.) The two are somewhat deeply related, and so it's mostly a change in perspective and language.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That is, fuzzy logic mostly applies to &lt;em&gt;labels&lt;/em&gt; which have &lt;em&gt;uncertain ranges&lt;/em&gt;. An object that's cool but not too cool &lt;em&gt;could&lt;/em&gt; be described as either cold or warm, and fuzzy logic handles this by assigning some fractional truth value to the 'cold' and 'warm' labels and no truth to the 'hot' label.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Probabilistic logic focuses more on the probability of some fact given some observations, and is deeply focused on the uncertainty of observations. When we look at an email, we track our belief that the email is &quot;spam&quot; and shouldn't be shown to the user with some number, and adjust that number as we see evidence for and against it being spam.&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="10" LastEditDate="2016-08-02T20:10:13.770" LastActivityDate="2016-08-02T20:10:13.770" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="122" PostTypeId="2" ParentId="118" CreationDate="2016-08-02T19:34:26.923" Score="10" Body="&lt;p&gt;A classical example of fuzzy logic in an AI is the expert system Mycin.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fuzzy logic can be used to deal with probabilities and uncertainties. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If one looks at, for example, predicate logic, then every statement is either true or false. In reality, we don't have this mathematical certainty. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, let's say a physician (or expert system) sees a symptom that can be attributed to a few different diseases (say A, B and C). The physician will now attribute a higher likelihood to the possibility of the patient having any of these three diseases. There is no definite true or false statement, but there is a change of weights. This can be reflected in fuzzy logic, but not so easily in symbolic logic.&lt;/p&gt;&#xA;" OwnerUserId="66" LastActivityDate="2016-08-02T19:34:26.923" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="123" PostTypeId="1" CreationDate="2016-08-02T19:42:07.160" Score="13" ViewCount="1080" Body="&lt;p&gt;Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence, it was math.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules but does not understand what (s)he is communicating.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does the Chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?&lt;/p&gt;&#xA;" OwnerUserId="66" LastEditorUserId="2444" LastEditDate="2021-02-15T23:45:03.590" LastActivityDate="2021-02-15T23:45:25.733" Title="Does the Chinese Room argument hold against AI?" Tags="&lt;philosophy&gt;&lt;agi&gt;&lt;chinese-room-argument&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="124" PostTypeId="2" ParentId="120" CreationDate="2016-08-02T19:46:58.277" Score="2" Body="&lt;p&gt;After he lays out his argument, he deals with some counterarguments. The following looks like the weakest one to me:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;We can use the same analogy also against those who, finding a formula their first machine cannot produce as being true, concede that that machine is indeed inadequate, but thereupon seek to construct a second, more adequate, machine, in which the formula can be produced as being true. This they can indeed do: but then the second machine will have a Gödelian formula all of its own, constructed by applying Gödel's procedure to the formal system which represents its (the second machine's) own, enlarged, scheme of operations. And this formula the second machine will not be able to produce as being true, while a mind will be able to see that it is true. And if now a third machine is constructed, able to do what the second machine was unable to do, exactly the same will happen: there will be yet a third formula, the Gödelian formula for the formal system corresponding to the third machine's scheme of operations, which the third machine is unable to produce as being true, while a mind will still be able to see that it is true. And so it will go on.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In short, by making the system more complex, it can see the inadequacy of a less complex system, but a yet more complex system can see its inadequacy. But from whence comes the claim that a mind &lt;em&gt;could&lt;/em&gt; see the inadequacy in the &lt;em&gt;n&lt;/em&gt;th machine? If, say, the Gödelian formula had as many components to it as a human brain had neurons, it seems suspect to claim that the human &lt;em&gt;could&lt;/em&gt; evaluate that formula and identify that it is in fact a Gödelian formula, rather than a similar but not quite identical sentence.  &lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T19:46:58.277" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="125" PostTypeId="2" ParentId="120" CreationDate="2016-08-02T19:49:46.050" Score="4" Body="&lt;p&gt;Yes, it applies. If a statement cannot be derived in a finite number of steps, then it doesn't matter if the person trying to prove it is a human or a computer.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The mathematician has one advantage over a standard theorem proving algorithm: the mathematician can &quot;step out of the system&quot; (as Douglas Hofstadter called in &lt;em&gt;G&amp;ouml;del, Escher, Bach&lt;/em&gt;), and start thinking &lt;em&gt;about&lt;/em&gt; the system. From this point of view, the mathematician may find that the derivation is impossible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, an AI for proving theorems could be programmed to recognize patterns in the derivation, just like our hypothetical mathematician, and start reasoning &lt;em&gt;about&lt;/em&gt; the formal system to derive properties of the formal system itself.&#xA;Both the AI and the mathematician would still be bound by the laws of mathematics, and not be able to prove a theorem if it was mathematically improvable.&lt;/p&gt;&#xA;" OwnerUserId="66" LastActivityDate="2016-08-02T19:49:46.050" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="126" PostTypeId="2" ParentId="123" CreationDate="2016-08-02T19:51:31.490" Score="3" Body="&lt;p&gt;Depends on who you ask! John Searle, who proposed this argument, would say &quot;yes&quot;, but others would say it is irrelevant. The Turing Test does not stipulate that a machine must actually &quot;understand&quot; what it is doing, as long as it seems that way to a human. You could argue that our &quot;thinking&quot; is only a more sophisticated form of clever algorithmics.&lt;/p&gt;&#xA;" OwnerUserId="148" LastEditorUserId="29" LastEditDate="2016-08-11T10:38:18.247" LastActivityDate="2016-08-11T10:38:18.247" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="127" PostTypeId="2" ParentId="123" CreationDate="2016-08-02T19:52:48.540" Score="8" Body="&lt;p&gt;There are two broad types of responses to philosophical queries like this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first is to make analogies and refer to intuition; one could, for example, actually calculate the necessary size for such a Chinese room, and suggest that it exists outside the realm of intuition and thus any analogies using it are suspect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second is to try to define the terms more precisely. If by &quot;intelligence&quot; we mean not &quot;the magic thing that humans do&quot; but &quot;information processing,&quot; then we can say &quot;yes, obviously the Chinese Room involves successful information processing.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tend to prefer the second because it forces conversations towards &lt;em&gt;observable outcomes&lt;/em&gt;, and puts the difficulty of defining a term like &quot;intelligence&quot; on the person who wants to make claims about it. If &quot;understanding&quot; is allowed to have an amorphous definition, then &lt;em&gt;any&lt;/em&gt; system could be said to have or not have understanding. But if &quot;understand&quot; is itself understood in terms of observable behavior, then it becomes increasingly difficult to construct an example of a system that &quot;is not intelligent&quot; and yet shares all the observable consequences of intelligence.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T19:52:48.540" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="128" PostTypeId="2" ParentId="123" CreationDate="2016-08-02T19:53:45.877" Score="9" Body="&lt;p&gt;It depends on the definition of (artificial) intelligence.&lt;/p&gt;&#xA;&lt;p&gt;The position that Searle originally tried to refute with the Chinese room experiment was the so-called position of strong AI: An appropriately programmed computer would have a mind in the exact same sense as humans have minds.&lt;/p&gt;&#xA;&lt;p&gt;Alan Turing tried to give a definition of artificial intelligence with the Turing Test, stating that a machine is intelligent if it can pass the test. The Turing Test is introduced &lt;a href=&quot;https://en.wikipedia.org/wiki/Turing_test&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. I won't explain it in detail because it is not really relevant to the answer. If you define (artificial) intelligence as Turing did, then the Chinese room experiment is not valid.&lt;/p&gt;&#xA;&lt;p&gt;So the point of the Chinese room experiment is to show that an appropriately programmed computer is not the same as a human mind, and therefore that Turing's Test is not a good one.&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="2444" LastEditDate="2021-02-15T23:45:25.733" LastActivityDate="2021-02-15T23:45:25.733" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="130" PostTypeId="1" AcceptedAnswerId="135" CreationDate="2016-08-02T19:58:28.117" Score="3" ViewCount="279" Body="&lt;p&gt;What are the main differences between a &lt;a href=&quot;https://en.wikipedia.org/wiki/Boltzmann_machine&quot; rel=&quot;nofollow noreferrer&quot;&gt;deep Boltzmann machine&lt;/a&gt; (DBM) (a recurrent neural network) and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_belief_network&quot; rel=&quot;nofollow noreferrer&quot;&gt;deep belief network&lt;/a&gt; (which is based on RBMs)?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2020-05-18T12:01:28.283" LastActivityDate="2020-05-18T12:01:28.283" Title="What are the main differences between a deep Boltzmann machine and a deep belief network?" Tags="&lt;neural-networks&gt;&lt;comparison&gt;&lt;boltzmann-machine&gt;&lt;deep-belief-network&gt;&lt;deep-boltzmann-machine&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="131" PostTypeId="2" ParentId="77" CreationDate="2016-08-02T20:01:05.593" Score="10" Body="&lt;p&gt;Overall, the answer is no, but the current paradigms owe a lot to LISP. The language most commonly used today is python.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Relevant answers:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Stack Overflow thread explaining why LISP was thought of as the AI language: &lt;a href=&quot;https://stackoverflow.com/questions/130475/why-is-lisp-used-for-ai&quot;&gt;Why is Lisp used for AI&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Quora answer by Peter Norvig, who wrote a popular textbook on the subject and is currently Director of Research at Google: &lt;a href=&quot;https://www.quora.com/Is-it-true-that-Lisp-is-highly-used-programming-language-in-AI&quot; rel=&quot;nofollow noreferrer&quot;&gt;Is it true that Lisp is highly used programming language in AI?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;LISP pioneered many important concepts in what we now call functional programming, with a key attraction being how close the programs were to math. Many of these features have since been incorporated into modern languages (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Lisp_(programming_language)&quot; rel=&quot;nofollow noreferrer&quot;&gt;the Wikipedia page&lt;/a&gt;). LISP is very expressive: it has very little syntax (just lists and some elementary operations on them) but you can write short succinct programs that represent complex ideas. This amazes newcomers and has sold it as the language for AI. However, this is a property of programs in general. Short programs can represent complex concepts. And while you can write powerful code in LISP, any beginner will tell you that it is also very hard to read anyone else's LISP code or to debug your own LISP code. Initially, there were also performance considerations with functional programming and it fell out of favor to be replaced by low level imperative languages like C. (For example, functional programming requires that no object ever be changed (&quot;mutated&quot;), so every operation requires a new object to be created. Without good garbage collection, this can get unwieldy). Today, we've learned that a mix of functional and imperative programming is needed to write good code and modern languages like python, ruby and scala support both. At this point, and this is just my opinion, there is no reason to prefer LISP over python.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The paradigm for AI that currently receives the most attention is Machine Learning, where we learn from data, as opposed to previous approaches like Expert Systems (in the 80s) where experts wrote rules for the AI to follow. Python is currently the most widely used language for machine learning and has many libraries, e.g. Tensorflow and Pytorch, and an active community. To process the massive amounts of data, we need systems like Hadoop, Hive or Spark. Code for these is written in python, java or scala. Often, the core time-intensive subroutines are written in C.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The AI Winter of the 80s was not because we did not have the right language, but because we did not have the right algorithms, enough computational power and enough data. If you're trying to learn AI, spend your time studying algorithms and not languages.&lt;/p&gt;&#xA;" OwnerUserId="130" LastEditorUserId="28225" LastEditDate="2019-10-01T00:00:52.223" LastActivityDate="2019-10-01T00:00:52.223" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="132" PostTypeId="2" ParentId="94" CreationDate="2016-08-02T20:01:55.600" Score="5" Body="&lt;p&gt;You described a single-layer feedforward network. They can have multiple layers. The significance of the weights is that they make a linear transformation from the output of the previous layer and hand it to the node they are going to. To say it more simplistically, they specify how important (and in what way: negative or positive) is the activation of node they are coming from to activating the node they are going to.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your example, since there is only one layer (a row of input nodes and a row of output nodes) it is easy to explain what each node represents. However in multi-layer feedforward networks they can become abstract representations which makes it difficult to explain them and therefore explain what the weights that come to them or go out of them represent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another way of thinking about it is that they describe hyperplanes in the space of the output of the previous node layer. If each output from the previous layer represents a point in space, a hyperplane decides which part of the space should give a positive value to the plane's corresponding node in the next layer and which part should give a negative input to it. It actually cuts that space into two halves. If you consider the input space of a multi-layer feedforward network, the weights of the first layer parametrize hyperplanes, however in the next layers they can represent non-linear surfaces in the input space.&lt;/p&gt;&#xA;" OwnerUserId="143" LastEditorUserId="75" LastEditDate="2018-04-13T18:16:38.963" LastActivityDate="2018-04-13T18:16:38.963" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="134" PostTypeId="2" ParentId="111" CreationDate="2016-08-02T20:10:47.850" Score="54" Body="&lt;p&gt;The answer to a lot of those questions depends on how the device is programmed. A computer capable of driving around and recognizing where the road goes is likely to have the ability to visually distinguish a human from an animal, whether that be based on outline, image, or size. With sufficiently sharp image recognition, it might be able to count the number and kind of people in another vehicle. It could even use existing data on the likelihood of injury to people in different kinds of vehicles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ultimately, people disagree on the ethical choices involved. Perhaps there could be &quot;ethics settings&quot; for the user/owner to configure, like &quot;consider life count only&quot; vs. &quot;younger lives are more valuable.&quot; I personally would think it's not terribly controversial that a machine should damage itself before harming a human, but people disagree on how important pet lives are. If explicit kill-this-first settings make people uneasy, the answers could be determined from a questionnaire given to the user.&lt;/p&gt;&#xA;" OwnerUserId="75" LastEditorUserId="2989" LastEditDate="2016-10-14T09:49:20.523" LastActivityDate="2016-10-14T09:49:20.523" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="135" PostTypeId="2" ParentId="130" CreationDate="2016-08-02T20:12:35.360" Score="3" Body="&lt;p&gt;The graph that represents a deep Boltzmann machine can be any weighted undirected graph. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, the graph that represents a deep Belief network must be a connection of graphs that represent restricted Boltzmann machines. Those graphs are bipartite, so there are two groups of vertices in those graphs so that every edge connects two vertices from different groups. Those groups are usually the visible and hidden components of the machine. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Learning is hard and impractical in a general deep Boltzmann machine, but easier and practical in a restricted Boltzmann machine, and hence in a  deep Belief network, which is a connection of some of these machines. &lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="2444" LastEditDate="2020-05-16T14:34:39.193" LastActivityDate="2020-05-16T14:34:39.193" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="136" PostTypeId="1" CreationDate="2016-08-02T20:15:22.637" Score="1" ViewCount="123" Body="&lt;p&gt;A cellular automaton is a state machine that is controlled by external input. The input is given by geometrical space around a cell. In a square matrix, each automaton gets input from 4 surrounding cells, while a hexagon grid has 6 neighbor cells that can be used as automaton input. For example, a 4-cells input may be the string “1011”. This string specifies a state of the cellular automaton. The automaton will switch to a different state according to the lookup table. I want to know if increasing the number of input cells in a hexagon automaton will make the resulting computer more powerful.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;original message&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;I'd like to learn more about the differences between &lt;a href=&quot;https://en.wikipedia.org/wiki/Cellular_automaton#Related_automata&quot; rel=&quot;nofollow noreferrer&quot;&gt;related automata&lt;/a&gt; which can be based on hexagonal cells instead of squares (rule 34/2), like in &lt;a href=&quot;https://en.wikipedia.org/wiki/CoDi&quot; rel=&quot;nofollow noreferrer&quot;&gt;CoDi model&lt;/a&gt; which uses spiking neural network (SNN).&lt;/p&gt;&#xA;&lt;p&gt;Is using a plane tiled with regular &lt;a href=&quot;https://en.wikipedia.org/wiki/Hexagonal_tiling&quot; rel=&quot;nofollow noreferrer&quot;&gt;hexagons&lt;/a&gt; more efficient and reliable than using square cells? What is the difference and how do I know which one to use in which scenario?&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;In other words, the more efficiently flexible that it grows, the more difficult scenarios it can be used for (for me, hexagonal implicates more possibilities, because it can send/share the signal with/to more tiles). Or maybe one is more modern than the other, or they're both on the same level? In general, I'd like to learn the differences between them to know when I should use one over the other.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="18758" LastEditorDisplayName="user8248" LastEditDate="2022-06-21T03:51:55.767" LastActivityDate="2022-06-21T03:51:55.767" Title="Number of input variables for a cellular automaton (was: Squares or hexagonal?)" Tags="&lt;evolutionary-algorithms&gt;&lt;topology&gt;&lt;efficiency&gt;" AnswerCount="0" CommentCount="5" ContentLicense="CC BY-SA 4.0" />
  <row Id="137" PostTypeId="2" ParentId="123" CreationDate="2016-08-02T20:16:42.047" Score="6" Body="&lt;p&gt;First of all, for a detailed view of the argument, check out the &lt;a href=&quot;http://plato.stanford.edu/entries/chinese-room/&quot; rel=&quot;noreferrer&quot;&gt;SEP entry on the Chinese Room&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I consider the CRA as an indicator of you definition of intelligence. If the argument holds, yes, the person in the room understands Chinese. However, let's sum up the three replies discussed in the SEP entry:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The &lt;em&gt;man&lt;/em&gt; himself doesn't understand Chinese (he wouldn't be able to understand it when outside the room), but the &lt;em&gt;system&lt;/em&gt; man+room understands it. Accepting that reply suggests that there can exist an intelligent system which parts aren't themselves intelligent (which can be argued of the human body itself).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The system doesn't understand Chinese, as it cannot interact with the world in the same way a robot or a human could (i.e. it cannot learn, is limited in the set of questions it can answer)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The system doesn't understand Chinese (depending on your definition of &lt;em&gt;understanding&lt;/em&gt;), and you couldn't say a human performing the same feats as the Chinese room understands Chinese either.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;So whether the argument, or a variant of it holds, depends on your definitions of &lt;em&gt;intelligent&lt;/em&gt;, &lt;em&gt;understanding&lt;/em&gt;, on how you define the system, etc. The point being that the thought experiment is a nice way to differentiate between the definitions (and many, many debates have been held about them), in order to avoid talking past each other endlessly.&lt;/p&gt;&#xA;" OwnerUserId="149" LastActivityDate="2016-08-02T20:16:42.047" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="138" PostTypeId="2" ParentId="50" CreationDate="2016-08-02T20:32:12.480" Score="6" Body="&lt;p&gt;Generalization error is the error obtained by applying a model to data it has not seen before. So, if you want to measure generalization error, you need to remove a subset from your data and don't train your model on it. After training, you verify your model accuracy (or other performance measures) on the subset you have removed since your model hasn't seen it before. Hence, this subset is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Test_set&quot; rel=&quot;nofollow noreferrer&quot;&gt;test set&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additionally, another subset can also be used for parameter selection, which we call a &lt;a href=&quot;https://en.wikipedia.org/wiki/Test_set#Validation_set&quot; rel=&quot;nofollow noreferrer&quot;&gt;validation set&lt;/a&gt;. We can't use the training set for parameter tuning, since it does not measure generalization error, but we can't use the test set too since our parameter tuning would overfit test data. That's why we need a third subset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, in order to obtain more predictive performance measures, we can use many different train/test partitions and average the results. This is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&quot; rel=&quot;nofollow noreferrer&quot;&gt;cross-validation&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="144" LastEditorUserId="14723" LastEditDate="2018-04-11T23:42:57.393" LastActivityDate="2018-04-11T23:42:57.393" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="139" PostTypeId="2" ParentId="108" CreationDate="2016-08-02T20:37:36.760" Score="5" Body="&lt;p&gt;There's no objective reason to state that declarative languages are better suited for AI development. However, there's indeed a bias towards them in practice. Although most functional languages are impure (that is, they allow side effects), and such can't count as &quot;declarative&quot;, a few languages are purely functional (that is, they don't allow side effects), most prominently &lt;a href=&quot;https://en.wikipedia.org/wiki/Declarative_programming#Functional_programming&quot; rel=&quot;nofollow noreferrer&quot;&gt;Haskell&lt;/a&gt;. Purity is key here. In Haskell, &lt;a href=&quot;https://stackoverflow.com/a/4066401/5249858&quot;&gt;even I/O is pure&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The key difference between imperative languages and (purely) functional languages is in the way they describe the program. An imperative program describes &lt;em&gt;how&lt;/em&gt; to do stuff, that is, algorithms. It specifies the specific instructions that the machine must carry on in order to perform the computation. OTOH, purely functional languages describe &lt;em&gt;what&lt;/em&gt; is to be computed, that is, the relationship between the input and the output. In mathematics, &quot;function&quot; is just a fancy name for a relationship between an input and an output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again, in the mathematical sense, the only variability is that of the function's arguments. That is, the function's output depends solely on its input (arguments). This is known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Referential_transparency&quot; rel=&quot;nofollow noreferrer&quot;&gt;referential transparency&lt;/a&gt;. Referential transparency states that:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;span class=&quot;math-container&quot;&gt;$$&#xA;\forall f \in \varphi, \forall x \in \delta_f, fx = fx&#xA;$$&lt;/span&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where &lt;span class=&quot;math-container&quot;&gt;$\varphi$&lt;/span&gt; is the set of all functions, and &lt;span class=&quot;math-container&quot;&gt;$\delta_f$&lt;/span&gt; is &lt;span class=&quot;math-container&quot;&gt;$f$&lt;/span&gt;'s domain. For the typical imperative language's definition of &quot;function&quot;, the above doesn't hold. For instance, C's &lt;code&gt;getchar()&lt;/code&gt; does not always return the same value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say we want to calculate the set of the ten least prime numbers whose least significant digit is &lt;span class=&quot;math-container&quot;&gt;$3$&lt;/span&gt;. First, in mathematical notation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;span class=&quot;math-container&quot;&gt;$$&#xA;\mathbb{S} = \mathbb{G}_{10} = \{ x \mid x \in \{ p \mid p \in \mathbb{N} \setminus \{0, 1 \} \text{ and } (\forall q \in \mathbb{N} \setminus \{0, 1, p \}, p \text{ mod } q = 0) \},  x \text{ mod } 10 = 3 \}&#xA;$$&lt;/span&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where &lt;code&gt;G(n, s)&lt;/code&gt; is the set of the lesser &lt;code&gt;n&lt;/code&gt;th elements from &lt;code&gt;s&lt;/code&gt;. In mathematics, you don't worry about how is the set &lt;span class=&quot;math-container&quot;&gt;$\mathbb{S}$&lt;/span&gt; supposed to be computed, but rather about &lt;span class=&quot;math-container&quot;&gt;$\mathbb{S}$&lt;/span&gt;'s definition itself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, in Python (in imperative style):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def is_prime(n):&#xA;  for x in range(2, n):&#xA;    if n % x == 0:&#xA;      return False&#xA;&#xA;  return True&#xA;&#xA;def foo():&#xA;  s = set()&#xA;  n = 2&#xA;  while len(s) &amp;lt; 10:&#xA;    if is_prime(n) and n % 10 == 3:&#xA;       s.append(n)&#xA;&#xA;  return set(s)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In Python, we care about (and are responsible for) the algorithm being used to compute the set. We specify, pretty much in recipe-style, how to build the set from scratch. If there's an algorithm that may be better suited for checking whether a number is prime or odd, but we don't use it, it's our fault, not Python's.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, Haskell steps in:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import qualified Data.Set as Set&#xA;&#xA;isPrime :: Integer -&amp;gt; Bool&#xA;isPrime n = ( == 1 ) . length . filter ( == 0 ) . map ( n `mod` ) $ [ 2 .. n ]&#xA;&#xA;s = Set.fromList . take 10 . filter ( ( == 3 ) . ( `mod` 10 ) ) . filter isPrime $ [1..]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Haskell's version is a lot more like the mathematical model than Python is. We define the &lt;code&gt;isPrime&lt;/code&gt; function in terms of the constraints that a prime number must obey, not by describing a step-by-step algorithm to do such a check. Moreover, &lt;code&gt;s&lt;/code&gt; (the set we have been defining so far) is defined in terms of the constraints its members must obey, rather than in terms of an algorithm to compute &lt;code&gt;s&lt;/code&gt; itself. The compiler, more often than not &lt;a href=&quot;https://downloads.haskell.org/~ghc/7.8.3/docs/html/users_guide/&quot; rel=&quot;nofollow noreferrer&quot;&gt;The Glorious Glasgow Haskell Compilation System&lt;/a&gt; (a.k.a GHC), is the one responsible for generating an algorithm. GHC's optimizer is known to be one of the strongest in the world, not because its the best compiler of 'em all, but because Haskell's nature allows for this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Haskell (and other functional languages), in summary, have several features that make it look, taste, and behave like pure math:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Referential transparency and purity.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; rel=&quot;nofollow noreferrer&quot;&gt;Haskell is lazy&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://softwareengineering.stackexchange.com/questions/279316/what-exactly-makes-the-haskell-type-system-so-revered-vs-say-java&quot;&gt;A &lt;strong&gt;very&lt;/strong&gt; strong type system&lt;/a&gt;, with such exotic (but very useful!) stuff as &lt;a href=&quot;https://en.wikipedia.org/wiki/Recursive_data_type&quot; rel=&quot;nofollow noreferrer&quot;&gt;recursive&lt;/a&gt; (and &lt;a href=&quot;https://en.wikipedia.org/wiki/Algebraic_data_type&quot; rel=&quot;nofollow noreferrer&quot;&gt;algebraic&lt;/a&gt;) data types.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So, the bottom line is that AI researchers often prefer functional languages over imperative languages (or, more assertively, pure over impure languages), because they are attempting to &lt;em&gt;define&lt;/em&gt; artificial intelligence itself, by means of functions (relationship between an input and an output). At the end, we do this because we have no real algorithm for human-level intelligence to raise from a handful of transistors. Also, there's been a historical bias towards these kind of languages, starting with &lt;a href=&quot;https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)&quot; rel=&quot;nofollow noreferrer&quot;&gt;John McCarthy&lt;/a&gt;, Lisp's creator and a pioneer in early AI research.&lt;/p&gt;&#xA;" OwnerUserId="71" LastEditorUserId="2444" LastEditDate="2018-11-18T12:34:56.643" LastActivityDate="2018-11-18T12:34:56.643" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="140" PostTypeId="1" AcceptedAnswerId="144" CreationDate="2016-08-02T20:37:59.927" Score="8" ViewCount="270" Body="&lt;p&gt;A superintelligence is a machine that can surpass all intellectual activities by any human, and such a machine is often portrayed in science fiction as a machine that brings mankind to an end.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any machine is executed using an algorithm. By the Church-Turing thesis, any algorithm that can be executed by a modern computer can be executed by a Turing Machine. However, a human can easily simulate a Turing Machine. Doesn't this mean that a machine can't surpass all intellectual activities, since we can also execute the algorithm?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This argument is most likely flawed, since my intuition tells me that superintelligence is possible. However, it is not clear to me where the flaw is. Note that this is my own argument. &lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="2444" LastEditDate="2019-06-25T21:16:52.410" LastActivityDate="2019-06-25T21:16:52.410" Title="Does this argument refuting the existence of superintelligence work?" Tags="&lt;philosophy&gt;&lt;superintelligence&gt;" AnswerCount="3" CommentCount="2" FavoriteCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="141" PostTypeId="2" ParentId="74" CreationDate="2016-08-02T20:48:24.037" Score="38" Body="&lt;p&gt;The terms &lt;em&gt;strong&lt;/em&gt; and &lt;em&gt;weak&lt;/em&gt; don't actually refer to processing, or optimization power, or any interpretation leading to &quot;strong AI&quot; being &lt;em&gt;stronger&lt;/em&gt; than &quot;weak AI&quot;. It holds conveniently in practice, but the terms come from elsewhere. In 1980, &lt;a href=&quot;https://en.wikipedia.org/wiki/John_Searle&quot;&gt;John Searle&lt;/a&gt; coined the following statements:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;AI hypothesis, strong form: an AI system can &lt;em&gt;think&lt;/em&gt; and have a &lt;em&gt;mind&lt;/em&gt; (in the philosophical definition of the term);&lt;/li&gt;&#xA;&lt;li&gt;AI hypothesis, weak form: an AI system can only &lt;em&gt;act&lt;/em&gt; like it thinks and has a mind.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So &lt;em&gt;strong AI&lt;/em&gt; is a shortcut for an AI systems that verifies the &lt;em&gt;strong AI hypothesis&lt;/em&gt;. Similarly, for the weak form. The terms have then evolved: strong AI refers to AI that performs as well as humans (who have minds), weak AI refers to AI that doesn't.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem with these definitions is that they're fuzzy. For example, &lt;a href=&quot;https://en.wikipedia.org/wiki/AlphaGo&quot;&gt;AlphaGo&lt;/a&gt; is an example of weak AI, but is &quot;strong&quot; by Go-playing standards. A hypothetical AI replicating a human baby would be a strong AI, while being &quot;weak&quot; at most tasks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other terms exist: &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_general_intelligence&quot;&gt;Artificial General Intelligence&lt;/a&gt; (AGI), which has cross-domain capability (like humans), can learn from a wide range of experiences (like humans), among other features. Artificial Narrow Intelligence refers to systems bound to a certain range of tasks (where they may nevertheless have superhuman ability), lacking capacity to significantly improve themselves.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Beyond AGI, we find Artificial Superintelligence (ASI), based on the idea that a system with the capabilities of an AGI, without the physical limitations of humans would learn and improve far beyond human level.&lt;/p&gt;&#xA;" OwnerUserId="149" LastActivityDate="2016-08-02T20:48:24.037" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="142" PostTypeId="2" ParentId="16" CreationDate="2016-08-02T20:57:51.603" Score="8" Body="&lt;p&gt;In some iterative learning methods the more iterations you apply the more specific your model becomes about the training set. If there are too many iterations, your model will become too specifically trained for the training samples and will score less on other samples that are not seen during the training phase. This is called over-fitting, though over-fitting is not specific to iterative learning methods.&lt;/p&gt;&#xA;&lt;p&gt;One solution to prevent over-fitting in these iterative learning algorithms is early stopping. Normally a control group of samples called validation samples (validation set) are used to validate the model and notify when it starts to over-fit. The validation set is not used by the training algorithm, however, its corresponding outputs are known and after each iteration, its samples are employed to measure how well the model currently works. As soon as the performance on the validation set stops growing and starts to drop we stop iterating the training algorithm. This is called early stopping which can help to maximize the generalization power of our learned model.&lt;/p&gt;&#xA;&lt;p&gt;Note that if we use the training set itself for validation the performance will always increase because that is what the learning algorithm is designed to do. However, the learning algorithm does not know how specifically it should learn the training set and that is why we need methods like early stopping.&lt;/p&gt;&#xA;" OwnerUserId="143" LastEditorUserId="18758" LastEditDate="2022-05-29T04:37:24.917" LastActivityDate="2022-05-29T04:37:24.917" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="143" PostTypeId="2" ParentId="28" CreationDate="2016-08-02T21:01:10.770" Score="10" Body="&lt;ul&gt;&#xA;&lt;li&gt;An ability that is commonly attributed to intelligence is &lt;strong&gt;problem solving&lt;/strong&gt;. &lt;/li&gt;&#xA;&lt;li&gt;Another one is &lt;strong&gt;learning&lt;/strong&gt; (improving itself from experience).&lt;/li&gt;&#xA;&lt;li&gt;Artificial intelligence can be defined as &quot;replicating intelligence, or parts of it, at least in appearance, inside a computer&quot; (dodging the definition of intelligence itself).&lt;/li&gt;&#xA;&lt;li&gt;Genetic algorithms are computational &lt;strong&gt;problem solving&lt;/strong&gt; tools that find and improve solutions (they &lt;strong&gt;learn&lt;/strong&gt;).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thus, genetic algorithms are a kind of artificial intelligence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regarding scale, I don't see it as an important factor for defining G.A. as A.I or not. The same way we can simply classify different living forms as more or less intelligent instead of just saying intelligent or not intelligent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, let's just make an important distinction: our brains are the product of natural selection, but the brains themselves don't use the same principle in order to achieve intelligence.&lt;/p&gt;&#xA;" OwnerUserId="144" LastActivityDate="2016-08-02T21:01:10.770" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="144" PostTypeId="2" ParentId="140" CreationDate="2016-08-02T21:06:19.000" Score="2" Body="&lt;p&gt;I believe this argument is based on the fact that intelligence is a single dimension when it really isn't. Are machines and humans really on the same level if a machine can solve a complex problem in a millionth of the time a human can? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It also assumes that the Turing machine is still the best computational model for the time period that you are in, which is not necessarily true for the future, it is just true until this point in time. &lt;/p&gt;&#xA;" OwnerUserId="152" LastEditorUserId="29" LastEditDate="2016-08-05T13:35:09.893" LastActivityDate="2016-08-05T13:35:09.893" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="145" PostTypeId="1" CreationDate="2016-08-02T21:10:26.693" Score="14" ViewCount="5018" Body="&lt;p&gt;From Wikipedia:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;AIXI ['ai̯k͡siː] is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence.[2]&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Albeit non-computable, approximations are possible, such as &lt;em&gt;AIXItl&lt;/em&gt;. Finding approximations to AIXI could be an objective way for solving AI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is &lt;em&gt;AIXI&lt;/em&gt; really a big deal in artificial &lt;em&gt;general&lt;/em&gt; intelligence research? Can it be thought as a central concept for the field? If so, why don't we have more publications on this subject (or maybe we have and I'm not aware of them)?&lt;/p&gt;&#xA;" OwnerUserId="144" LastEditorUserId="2444" LastEditDate="2019-04-19T15:13:02.567" LastActivityDate="2021-07-19T20:20:42.770" Title="What is the relevance of AIXI on current artificial intelligence research?" Tags="&lt;models&gt;&lt;agi&gt;&lt;aixi&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="146" PostTypeId="1" CreationDate="2016-08-02T21:14:36.133" Score="3" ViewCount="406" Body="&lt;p&gt;In what ways can connectionist artificial intelligence (neural networks) be integrated with &lt;em&gt;Good Old-Fashioned A.I.&lt;/em&gt; (&lt;em&gt;GOFAI&lt;/em&gt;)? For instance, how could deep neural networks be integrated with knowledge bases or logical inference? One such example seems to be the &lt;a href=&quot;http://wiki.opencog.org/w/DestinOpenCog&quot; rel=&quot;nofollow&quot;&gt;OpenCog + Destin integration&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="144" LastEditorUserId="2444" LastEditDate="2021-12-27T13:28:20.837" LastActivityDate="2021-12-27T13:28:20.837" Title="In what ways can connectionist AI be integrated with GOFAI?" Tags="&lt;neural-networks&gt;&lt;reference-request&gt;&lt;symbolic-ai&gt;&lt;symbolic-computing&gt;&lt;neurosymbolic-ai&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="147" PostTypeId="1" CreationDate="2016-08-02T21:15:34.483" Score="2" ViewCount="359" Body="&lt;p&gt;It is proved that a recurrent neural net with rational weights can be a super-Turing machine. Can we achieve this in practice ?&lt;/p&gt;&#xA;" OwnerUserId="159" LastActivityDate="2016-09-03T17:38:06.577" Title="Can we ever achieve hypercomputation using recurrent neural networks?" Tags="&lt;neural-networks&gt;&lt;hypercomputation&gt;&lt;recurrent-neural-networks&gt;" AnswerCount="2" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="148" PostTypeId="1" AcceptedAnswerId="170" CreationDate="2016-08-02T21:16:44.013" Score="19" ViewCount="2656" Body="&lt;p&gt;Given the proven &lt;a href=&quot;https://en.wikipedia.org/wiki/Halting_problem&quot;&gt;halting problem&lt;/a&gt; for &lt;a href=&quot;https://en.wikipedia.org/wiki/Turing_machine&quot;&gt;Turing machines&lt;/a&gt;, can we infer limits on the ability of strong Artificial Intelligence?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2021-10-13T00:31:31.113" LastActivityDate="2021-10-13T00:31:31.113" Title="What limits, if any, does the halting problem put on Artificial Intelligence?" Tags="&lt;agi&gt;&lt;theory-of-computation&gt;&lt;halting-problem&gt;&lt;turing-machine&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="149" PostTypeId="2" ParentId="140" CreationDate="2016-08-02T21:18:16.763" Score="1" Body="&lt;p&gt;A quantum computer has a huge amount of internal state that even the machine can't get at directly. (You can only sample the matrix state.) The amount of that state goes up exponentially with each quantum bit involved in the system. Some operations get insane speedups from quantum computing: you just put the quantum wire through a quantum gate and you've updated the entire matrix at once.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simulating a quantum computer with a classical one would take exponentially longer for each qubit. With several dozen qubits, the machine's computing power for some tasks couldn't even be approached by a normal computer, much less a human mind.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Relevant: my answer on &lt;a href=&quot;https://ai.stackexchange.com/a/114/75&quot;&gt;To what extent can quantum computers help to develop Artificial Intelligence?&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that with quantum computers, you've gone beyond the normal zeroes and ones. You then need a &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_Turing_machine&quot; rel=&quot;nofollow noreferrer&quot;&gt;quantum Turing machine&lt;/a&gt;, which is a generalization of the classical one.&lt;/p&gt;&#xA;" OwnerUserId="75" LastEditorUserId="-1" LastEditDate="2017-04-13T12:53:10.013" LastActivityDate="2016-08-02T21:18:16.763" CommentCount="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="151" PostTypeId="1" CreationDate="2016-08-02T21:25:25.313" Score="1" ViewCount="58" Body="&lt;p&gt;By default using the &lt;a href=&quot;https://en.wikipedia.org/wiki/DeepDream&quot; rel=&quot;nofollow noreferrer&quot;&gt;DeepDream&lt;/a&gt; technique, you can create a dreamlike image out of two different images.&lt;/p&gt;&#xA;&lt;p&gt;Is it possible to easily enhance this technique to generate one image out of three?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="18758" LastEditDate="2022-06-21T03:49:38.623" LastActivityDate="2022-06-21T03:49:38.623" Title="Can DeepDream produce a &quot;dream&quot; from 3 images?" Tags="&lt;convolutional-neural-networks&gt;&lt;deepdream&gt;" AnswerCount="0" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="152" PostTypeId="1" AcceptedAnswerId="1728" CreationDate="2016-08-02T21:34:32.107" Score="7" ViewCount="608" Body="&lt;p&gt;Consider these neural style algorithms which produce some art work:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/alexjc/neural-doodle&quot;&gt;Neural Doodle&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/jcjohnson/neural-style&quot;&gt;neural-style&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Why is generating such images so slow and why does it take huge amounts of memory? Isn't there any method of optimizing the algorithm?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the mechanism or technical limitation behind this? Why we can't have a realtime processing?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are few user comments (&lt;a href=&quot;https://www.reddit.com/r/deepdream/comments/3jwl76/how_anyone_can_create_deep_style_images/&quot;&gt;How ANYONE can create Deep Style images&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;blockquote&gt;&#xA;  &lt;p&gt;Anything above 640x480 and we're talking days of heavy crunching and an insane amount of ram.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;blockquote&gt;&#xA;  &lt;p&gt;I tried doing a 1024pixel image and it still crashed with 14gigs memory, and 26gigs swap. So most of the VM space is just the swapfile. Plus it takes several hours potentially days cpu rendering this.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;blockquote&gt;&#xA;  &lt;p&gt;I tried 1024x768 and with 16gig ram and 20+ gig swap it was still dying from lack of memory.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;blockquote&gt;&#xA;  &lt;p&gt;Having a memory issue, though. I'm using the &quot;g2.8xlarge&quot; instance type.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastEditorUserId="145" LastEditDate="2016-08-18T13:24:43.510" LastActivityDate="2018-04-15T04:16:38.293" Title="Why is the generation of deep style images so slow and resource-hungry?" Tags="&lt;performance&gt;&lt;neural-doodle&gt;&lt;deepdreaming&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="153" PostTypeId="1" CreationDate="2016-08-02T21:36:28.053" Score="10" ViewCount="2747" Body="&lt;p&gt;Can autoencoders be used for supervised learning &lt;em&gt;without adding an output layer&lt;/em&gt;? Can we simply feed it with a concatenated input-output vector for training, and reconstruct the output part from the input part when doing inference? The output part would be treated as missing values during inference and some imputation would be applied.&lt;/p&gt;&#xA;" OwnerUserId="144" LastEditorUserId="2444" LastEditDate="2021-12-23T23:33:48.907" LastActivityDate="2021-12-23T23:33:48.907" Title="Can autoencoders be used for supervised learning?" Tags="&lt;neural-networks&gt;&lt;autoencoders&gt;&lt;supervised-learning&gt;" AnswerCount="2" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="154" PostTypeId="1" AcceptedAnswerId="158" CreationDate="2016-08-02T21:37:32.420" Score="35" ViewCount="26162" Body="&lt;p&gt;I'm aware that neural networks are probably not designed to do that, however asking hypothetically, is it possible to train the deep neural network (or similar) to solve math equations?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So given the 3 inputs: 1st number, operator sign represented by the number (1 - &lt;code&gt;+&lt;/code&gt;, 2 - &lt;code&gt;-&lt;/code&gt;, 3 - &lt;code&gt;/&lt;/code&gt;, 4 - &lt;code&gt;*&lt;/code&gt;, and so on), and the 2nd number, then after training the network should give me the valid results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example 1 (&lt;code&gt;2+2&lt;/code&gt;):&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Input 1: &lt;code&gt;2&lt;/code&gt;; Input 2: &lt;code&gt;1&lt;/code&gt; (&lt;code&gt;+&lt;/code&gt;); Input 3: &lt;code&gt;2&lt;/code&gt;; Expected output: &lt;code&gt;4&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Input 1: &lt;code&gt;10&lt;/code&gt;; Input 2: &lt;code&gt;2&lt;/code&gt; (&lt;code&gt;-&lt;/code&gt;); Input 3: &lt;code&gt;10&lt;/code&gt;; Expected output: &lt;code&gt;0&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Input 1: &lt;code&gt;5&lt;/code&gt;; Input 2: &lt;code&gt;4&lt;/code&gt; (&lt;code&gt;*&lt;/code&gt;); Input 3: &lt;code&gt;5&lt;/code&gt;; Expected output: &lt;code&gt;25&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;and so&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The above can be extended to more sophisticated examples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is that possible? If so, what kind of network can learn/achieve that?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-10-08T11:59:11.790" LastActivityDate="2022-10-25T14:10:55.127" Title="Is it possible to train the neural network to solve math equations?" Tags="&lt;neural-networks&gt;&lt;math&gt;&lt;applications&gt;" AnswerCount="6" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="155" PostTypeId="2" ParentId="154" CreationDate="2016-08-02T21:57:56.160" Score="9" Body="&lt;p&gt;Not really.&lt;/p&gt;&#xA;&lt;p&gt;Neural networks are good for determining non-linear relationships between inputs when there are hidden variables. In the examples above, the relationships are linear, and there are no hidden variables. But even if they were non-linear, a traditional ANN design would not be well suited to accomplish this.&lt;/p&gt;&#xA;&lt;p&gt;By carefully constructing the layers and tightly supervising the training, you could get a network to consistently produce the output 4.01, say, for the inputs: 2, 1 (+), and 2, but this is not only wrong, it's an inherently unreliable application of the technology.&lt;/p&gt;&#xA;" OwnerUserId="46" LastEditorUserId="2444" LastEditDate="2021-10-08T11:57:57.370" LastActivityDate="2021-10-08T11:57:57.370" CommentCount="3" ContentLicense="CC BY-SA 4.0" />
  <row Id="156" PostTypeId="1" CreationDate="2016-08-02T21:59:01.093" Score="19" ViewCount="2046" Body="&lt;p&gt;From Wikipedia:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?&lt;/p&gt;&#xA;" OwnerUserId="144" LastActivityDate="2016-10-14T13:08:35.790" Title="Are there any computational models of mirror neurons?" Tags="&lt;neural-networks&gt;&lt;models&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="157" PostTypeId="1" AcceptedAnswerId="267" CreationDate="2016-08-02T22:38:50.823" Score="8" ViewCount="184" Body="&lt;p&gt;If I have a paragraph I want to summarize, for example:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Ponzo and Fila went to the mall during the day. They walked for a long while, stopping at shops. They went to many shops. At first, they didn't buy anything. After going to a number of shops, they eventually bought a shirt, and a pair of pants.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Better summarized as:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;They shopped at the mall today and bought some clothes.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What is the best AI strategy to automate this process, if there is one? If there isn't, is it because it would be dependent on first having an external information resource that would inform any algorithm? Or is it because the problem is inherently contextual?&lt;/p&gt;&#xA;" OwnerUserId="46" LastEditorUserId="4302" LastEditDate="2018-10-08T12:44:30.960" LastActivityDate="2018-10-08T12:44:30.960" Title="What artificial intelligence strategies are useful for summarization?" Tags="&lt;algorithm&gt;&lt;natural-language-processing&gt;&lt;pattern-recognition&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="158" PostTypeId="2" ParentId="154" CreationDate="2016-08-02T22:50:52.560" Score="24" Body="&lt;p&gt;Yes, it has been done!&lt;/p&gt;&#xA;&lt;p&gt;However, the applications aren't to replace calculators or anything like that. The lab I'm associated with develops neural network models of equational reasoning to better understand how humans might solve these problems. This is a part of the field known as &lt;a href=&quot;https://web.archive.org/web/20201111232708/http://archive.is/mt4RO&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mathematical Cognition&lt;/a&gt;. Unfortunately, our website isn't terribly informative, but here's a &lt;a href=&quot;https://web.archive.org/web/20200331161444/http://web.stanford.edu/%7Ekmickey/pdf/MickeyMcClelland2014.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;link&lt;/a&gt; to an example of such work.&lt;/p&gt;&#xA;&lt;p&gt;Apart from that, recent work on extending neural networks to include external memory stores (e.g. Neural Turing Machines) was used to solve math problems as a good proof of concept. This is because many arithmetic problems involve long procedures with stored intermediate results. See the sections of &lt;a href=&quot;http://arxiv.org/pdf/1511.08228.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;this paper&lt;/a&gt; on long binary addition and multiplication.&lt;/p&gt;&#xA;" OwnerUserId="109" LastEditorUserId="2444" LastEditDate="2021-10-08T11:55:40.857" LastActivityDate="2021-10-08T11:55:40.857" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="159" PostTypeId="1" AcceptedAnswerId="238" CreationDate="2016-08-02T23:01:56.157" Score="1" ViewCount="559" Body="&lt;p&gt;What happens if you apply the same &lt;a href=&quot;https://en.wikipedia.org/wiki/DeepDream&quot; rel=&quot;nofollow noreferrer&quot;&gt;deep dream technique&lt;/a&gt; which produces &quot;dream&quot; visuals but to media streams such as audio files?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does changing image functions into audio and enhancing the logic would work, or will it no longer work/doesn't make any sense?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to create &quot;dream&quot; like audio based on the two samples.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="14723" LastEditDate="2018-04-14T18:06:52.290" LastActivityDate="2018-04-14T18:06:52.290" Title="Is it possible to apply deep dream technique for the audio streams?" Tags="&lt;convolutional-neural-networks&gt;&lt;deepdreaming&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="161" PostTypeId="2" ParentId="111" CreationDate="2016-08-02T23:31:57.630" Score="16" Body="&lt;p&gt;This is the well known &lt;a href=&quot;https://en.wikipedia.org/wiki/Trolley_problem&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Trolley Problem&lt;/em&gt;&lt;/a&gt;. As &lt;a href=&quot;https://ai.stackexchange.com/a/134/8&quot;&gt;Ben N&lt;/a&gt; said, people disagree on the right course of action for trolley problem scenarios, but it should be noted that with self-driving cars, reliability is so high that these scenarios are really unlikely. So, not much effort will be put into the problems you are describing, at least in the short term.&lt;/p&gt;&#xA;" OwnerUserId="130" LastEditorUserId="-1" LastEditDate="2017-04-13T12:53:10.013" LastActivityDate="2016-08-04T00:19:36.570" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="162" PostTypeId="2" ParentId="111" CreationDate="2016-08-02T23:44:57.983" Score="9" Body="&lt;p&gt;For a driverless car that is designed by a single entity, the best way for it to make decisions about whom to kill is by estimating and minimizing the probable liability.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It doesn't need to absolutely correctly identify all the potential victims in the area to have a defense for its decision, only to identify them as well as a human could be expected to.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It doesn't even need to know the age and physical condition of everyone in the car, as it can ask for that information and if refused, has the defense that the passengers chose not to provide it, and therefore took responsibility for depriving it of the ability to make a better decision.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It only has to have a viable model for minimizing exposure of the entity to lawsuits, which can then be improved over time to make it more profitable.&lt;/p&gt;&#xA;" OwnerUserId="46" LastActivityDate="2016-08-02T23:44:57.983" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="163" PostTypeId="2" ParentId="13" CreationDate="2016-08-02T23:56:02.343" Score="6" Body="&lt;p&gt;Well, I do not know what type of features you are giving to your neural network. However, in general, I would go with a single neural network. It seems that you have no limitation in resources for training your network and the only problem is resources while you apply your network. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The thing is that probably the two problems have things in common (e.g. both types of plates are rectangular). This means that if you use two networks, each has to solve the same sub-problem (the common part) again. If you use only one network the common part of the problem takes fewer cells/weights to be solved and the remaining weights/cells can be employed for better recognition.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the end, if I was in your place I would try both of them. I think that is the only way to be really sure what is the best solution. When speaking theoretically it is possible that we do not include some factors.&lt;/p&gt;&#xA;" OwnerUserId="143" LastEditorUserId="14723" LastEditDate="2018-04-12T02:23:00.733" LastActivityDate="2018-04-12T02:23:00.733" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="164" PostTypeId="2" ParentId="77" CreationDate="2016-08-03T00:16:14.550" Score="5" Body="&lt;p&gt;LISP is still used significantly, but less and less. There is still momentum due to so many people using it in the past, who are still active in the industry or research (anecdote: the last VCR was produced by a Japanese maker in July 2016, yes). The language is however used (to my knowledge) for the kind of AI that does not leverage Machine Learning, typically as the reference books from Russell and Norvig. These applications are still very useful, but Machine Learning gets all the steam these days.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another reason for the decline is that LISP practitioners have partially moved to Clojure and other recent languages.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are learning about AI technologies, LISP (or Scheme or Prolog) is good choice to understand what is going on with &quot;AI&quot; at large. But if you wish or have to be very pragmatic, Python or R are the community choices&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: The above lacks concrete example and reference. I am aware of some work in universities, and some companies inspired by or directly using LISP.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;To add on @Harsh's answer, LISP (and Scheme, and Prolog) has qualities that made it look like it was better suited for creating intelligent mechanisms---making AI as perceived in the 60s.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the qualities was that the language design leads the developer to think in a quite elegant way, to decompose a big problem into small problems, etc. Quite &quot;clever&quot;, or &quot;intelligent&quot; if you will. Compared to some other languages, there is almost no choice but to develop that way. LISP is a list processing language, and &quot;purely functional&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One problem, though, can be seen in work related to LISP. A notable one in the AI domain is the work on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Situation_calculus&quot; rel=&quot;noreferrer&quot;&gt;Situation Calculus&lt;/a&gt;, where (in short) one describes objects and rules in a &quot;world&quot;, and can let it evolve to compute &lt;em&gt;situations&lt;/em&gt;---states of the world. So it is a model for reasoning on situations. The main problem is called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Frame_problem&quot; rel=&quot;noreferrer&quot;&gt;frame problem&lt;/a&gt;, meaning this calculus cannot tell what does &lt;em&gt;not&lt;/em&gt; change---just what changes. Anything that is not defined in the world cannot be processed (note the difference here with ML). First implementations used LISPs, because that was the AI language then. And there were bound by the frame problem. But, as @Harsh mentioned, it is not LISP's fault: Any language would face the same framing issue (a conceptual problem of the Situation Calculus).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the language really does not matter from the AI / AGI / ASI perspective. The concepts (algorithms, etc.) are really what matters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Even in Machine Learning, the language is just a practical choice. Python and R are popular today, primarily due to their library ecosystem and the focus of key companies. But try to use Python or R to run a model for a RaspberryPI-based application, and you will face some severe limitations (but still possible, I am doing it :-)). So the language choice burns down to pragmatism.&lt;/p&gt;&#xA;" OwnerUserId="169" LastActivityDate="2016-08-03T00:16:14.550" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="165" PostTypeId="2" ParentId="140" CreationDate="2016-08-03T00:40:35.383" Score="1" Body="&lt;p&gt;The flaw in your argument is that &quot;surpass&quot; doesn't just mean that you should be able to run all algorithms, it includes a notion of complexity, i.e. how many time steps you will take to simulate an algorithm. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do you simulate an algorithm with a Turing machine? A &lt;a href=&quot;https://en.wikipedia.org/wiki/Turing_machine&quot; rel=&quot;nofollow noreferrer&quot;&gt;Turing machine&lt;/a&gt; consists of a finite state machine and an infinite tape. A Turing Machine does run an algorithm, determined by its initial state and the state transition matrix, but what I think you are talking about is Universal Turing Machines (UTM)  that can read &quot;code&quot; (which is usually a description of another Turing machine) written on a &quot;code segment&quot; of the tape and then simulate that machine on input data written on the &quot;data segment&quot; of the tape.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Turing machines can differ in the number of states in their finite state machines (and also in the alphabet they write on the tape but any finite alphabet is easily encoded in binary so this should not be the big reason for differences among Turing machines). So, you can have UTMs with bigger state machines and UTMs with smaller state machines. The bigger UTM could possibly surpass the smaller one if they use the same encoding for the &quot;code&quot; part of the tape.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can also play around with the code used to describe the TM being simulated. This code could be C++, for example, or could be a Neural network with the synapse strength written down as a matrix. Which description is better for computation depends on the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An example comparison among UTMs with different state machines: consider different compilers for the same language, say C++. Both of them will first compile C++ to assembly and then run another UTM which reads and executes assembly (your physical CPU). So, a better compiler will run the same code faster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Back to humans vs computers, humans are neural networks that run algorithms like those you would write in C++. This involves a costly and inefficient conversion of the algorithm into hand movements. A computer uses a compiler to convert C++ to assembly that it can run natively, so it's able to do a much more efficient implementation of C++ code. Alternately, humans have a ton of neurons, and the neural code, i.e. synapse strength, is hard to read, so current computers cannot run that code yet.&lt;/p&gt;&#xA;" OwnerUserId="130" LastEditorUserId="14723" LastEditDate="2018-04-15T04:14:15.697" LastActivityDate="2018-04-15T04:14:15.697" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="166" PostTypeId="2" ParentId="77" CreationDate="2016-08-03T01:08:53.837" Score="7" Body="&lt;p&gt;I definitely continue to often use Lisp when working on AI models.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You asked if it is being used for &lt;em&gt;substantial&lt;/em&gt; work.  That's too subjective for me to answer regarding my own work, but I queried one my AI models whether or not it considered itself substantial, and it replied with an affirmative response.  Of course, it's response is naturally biased as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Overall, a significant amount of AI research and development is conducted in Lisp.  Furthermore, even for non-AI problems, Lisp is sometimes used.  To demonstrate the power of Lisp, I engineered the first neural network simulation system written entirely in Lisp over a quarter century ago.&lt;/p&gt;&#xA;" OwnerUserId="156" LastActivityDate="2016-08-03T01:08:53.837" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="167" PostTypeId="1" AcceptedAnswerId="174" CreationDate="2016-08-03T01:55:30.377" Score="4" ViewCount="191" Body="&lt;p&gt;In &lt;a href=&quot;https://en.wikipedia.org/wiki/DeepDream&quot; rel=&quot;nofollow&quot;&gt;DeepDream&lt;/a&gt; wikipedia page it's suggested that a dreamlike images created by a convolutional neural network may be related to how visual cortex works in humans when they're tripping.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;How this is even possible?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How exactly convolutional neural networks have anything to do with human visual cortex?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="145" LastEditDate="2016-08-18T11:34:48.770" LastActivityDate="2016-08-18T11:34:48.770" Title="Why would neural network dream scenes mirror the hallucinations people experience when they're tripping?" Tags="&lt;convolutional-neural-networks&gt;&lt;deepdream&gt;&lt;computer-vision&gt;&lt;deepdreaming&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="169" PostTypeId="1" AcceptedAnswerId="176" CreationDate="2016-08-03T02:12:37.943" Score="1" ViewCount="120" Body="&lt;p&gt;This 2014 &lt;a href=&quot;https://medium.com/the-physics-arxiv-blog/first-demonstration-of-artificial-intelligence-on-a-quantum-computer-17a6b9d1c5fb&quot; rel=&quot;nofollow&quot;&gt;article&lt;/a&gt; saying that a Chinese team of physicists have trained a quantum computer to recognise handwritten characters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Why did they have to use a quantum computer&lt;/strong&gt; to do that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it just for fun and demonstration, or is it that recognising the handwritten characters is so difficult that standard (non-quantum) computers or algorithms cannot do that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If standard computers can achieve the same thing, what are the benefits of using quantum computers to do that then over standard methods?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="8" LastEditDate="2016-08-06T00:07:31.233" LastActivityDate="2016-08-06T00:07:31.233" Title="What are the challenges for recognising the handwritten characters?" Tags="&lt;quantum-computing&gt;&lt;handwritten-characters&gt;&lt;optical-character-recognition&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="170" PostTypeId="2" ParentId="148" CreationDate="2016-08-03T02:17:01.983" Score="11" Body="&lt;p&gt;Does the halting problem imply any limits on human cognition?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Yes, absolutely--that there are pieces of code a human could look at and not be sure whether or not it will halt in finite time. (Certainly there are pieces of code that a human can look at and say &quot;yes&quot; or &quot;no&quot; definitely, but we're talking about the ones that are actually quite difficult to analyze.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The halting problem means that there are types of code analysis that no computer could do, because it's mathematically impossible. But the realm of &lt;em&gt;possibility&lt;/em&gt; is still large enough to allow strong artificial intelligence (in the sense of code that can understand itself well enough to improve itself).&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-03T02:17:01.983" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="171" PostTypeId="2" ParentId="74" CreationDate="2016-08-03T02:23:28.267" Score="9" Body="&lt;p&gt;In contrast to the &lt;em&gt;philosophical&lt;/em&gt; definitions, which rely on terms like &quot;mind&quot; and &quot;think,&quot; there are also definitions that hinge on &lt;em&gt;observables&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That is, a Strong AI is an AI that understands itself well enough to self-improve. Even if it is philosophically not equivalent to a human, or unable to perform &lt;em&gt;all&lt;/em&gt; cognitive tasks that a human can, this AI can still generate a tremendous amount of optimization power / good decision-making, and its creation would be of historic importance (to put it lightly).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A Weak AI, in contrast, is an AI with no or limited ability to self-modify. A chessbot that runs on your laptop might have superhuman ability to play chess, but it can &lt;em&gt;only&lt;/em&gt; play chess, and while it might tune its weights or its architecture and slowly improve, it cannot modify itself in a deep enough way to generalize to other tasks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another way to think about this is that a Strong AI is an AI researcher in its own right, and a Weak AI is what AI researchers produce.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-03T02:23:28.267" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="172" PostTypeId="1" CreationDate="2016-08-03T03:33:35.193" Score="2" ViewCount="147" Body="&lt;p&gt;Is it possible that, at some time in the future, AIs will be able to initiatively develop themselves, rather than passively being developed by humanity?&lt;/p&gt;&#xA;" OwnerUserId="104" LastEditorUserId="2444" LastEditDate="2019-06-20T20:43:42.113" LastActivityDate="2019-06-20T20:48:44.383" Title="Could AIs self-develop in the future?" Tags="&lt;philosophy&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="173" PostTypeId="2" ParentId="148" CreationDate="2016-08-03T04:22:28.030" Score="13" Body="&lt;p&gt;The halting problem is an example of a general phenomenon known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Undecidable_problem&quot; rel=&quot;noreferrer&quot;&gt;Undecidability&lt;/a&gt;, which shows that there are problems no Turing machine can solve in finite time. Let's consider the generalization that it is undecidable whether a Turing Machine satisfies some non-trivial property P, called &lt;a href=&quot;https://en.wikipedia.org/wiki/Rice%27s_theorem&quot; rel=&quot;noreferrer&quot;&gt;Rice's theorem&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First note that the halting problem applies only if the Turing machine takes arbitrarily long input. If the input is bounded, it is possible to enumerate all possible cases and the problem is no longer undecidable. It might still be inefficient to calculate it, but then we are turning to the complexity theory, which should be a separate question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rice's theorem implies that an intelligence (a human) cannot be able to determine whether another intelligence (such as an AGI) possesses a certain property, such as being &lt;a href=&quot;https://en.wikipedia.org/wiki/Friendly_artificial_intelligence&quot; rel=&quot;noreferrer&quot;&gt;friendly&lt;/a&gt;. This does not mean that we cannot design a Friendly AGI, but it does mean that we cannot check whether an arbitrary AGI is friendly. So, while we can possibly create an AI which is guaranteed to be friendly, we also need to ensure that IT cannot create another AI which is unfriendly.&lt;/p&gt;&#xA;" OwnerUserId="130" LastEditorUserId="169" LastEditDate="2016-08-03T18:25:39.313" LastActivityDate="2016-08-03T18:25:39.313" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="174" PostTypeId="2" ParentId="167" CreationDate="2016-08-03T04:27:10.673" Score="7" Body="&lt;p&gt;The similarity of artificial neural networks and the human visual cortex goes very deep, and in many ways the human visual cortex was the inspiration for the techniques we use for the design and implementation of ANNs designed for image recognition. So in that direction, the similarity seems obvious to me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reverse direction, though, is a question about how the human mind works under the influence of LSD, which you'll probably get a better answer asking about in the &lt;a href=&quot;https://biology.stackexchange.com/&quot;&gt;biology&lt;/a&gt; or &lt;a href=&quot;https://cogsci.stackexchange.com/&quot;&gt;cognitive science&lt;/a&gt; stack exchange sites.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some brief details to add to the answer, though: the human visual cortex is arranged in layers that correspond to increasing layers of abstraction. In the eyes themselves, photons are detected by light-sensitive cells and added together to make what are essentially the color elements of pixels. Those are then routed to another layer which does something like edge detection, and then the next layer does something like shape detection, and so on up to higher level concepts like &quot;a cat's face.' If LSD lowers the activation threshold for those neurons, or makes them more excitable, then more things will be interpreted as having the higher level concept (and so a patch of rough texture may have a face jump out of it, for example).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The way that CNN &quot;deep dreaming' works is that the base image is amplified. That is, to make a particular patch look more like a dog, the shapes are nudged to be more dog-like, and the shapes nudge the edges, and the edges nudge the pixels.&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="-1" LastEditDate="2017-04-13T12:56:17.300" LastActivityDate="2016-08-03T04:27:10.673" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="176" PostTypeId="2" ParentId="169" CreationDate="2016-08-03T04:41:50.923" Score="6" Body="&lt;p&gt;Handwritten digit recognition is a standard benchmark in Machine Learning in the form of the &lt;a href=&quot;https://en.wikipedia.org/wiki/MNIST_database&quot;&gt;MNIST dataset&lt;/a&gt;. For example, &lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html&quot;&gt;scikit-learn&lt;/a&gt;, a python package for Machine Learning uses it as a tutorial example. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The paper you cite uses this standard task as a proof of concept, to show that their system works.&lt;/p&gt;&#xA;" OwnerUserId="130" LastActivityDate="2016-08-03T04:41:50.923" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="177" PostTypeId="2" ParentId="26" CreationDate="2016-08-03T04:48:19.643" Score="6" Body="&lt;p&gt;I think your question fits nowadays more in the field of &lt;a href=&quot;https://en.wikipedia.org/wiki/Human%E2%80%93robot_interaction&quot; rel=&quot;nofollow noreferrer&quot;&gt;Human-Robot Interaction&lt;/a&gt;, which relies largely on &lt;a href=&quot;http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-you-impressed-and-concerned/&quot; rel=&quot;nofollow noreferrer&quot;&gt;vision&lt;/a&gt; for recognition of gestures and follow movements, as well as &lt;em&gt;soft, natural&lt;/em&gt; movements as a response. Note that the movements of the face and hands belong to the most complex tasks, involving &lt;em&gt;many&lt;/em&gt; muscles at a time.&lt;/p&gt;&#xA;&lt;p&gt;I strongly recommend the film &lt;a href=&quot;http://www.plugandpray-film.de/en/trailer.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Plug &amp;amp; Pray&lt;/a&gt; to have an idea of what people are researching in this area.&lt;/p&gt;&#xA;&lt;p&gt;You may also find Eliza (which you can try &lt;a href=&quot;https://web.njit.edu/%7Eronkowit/eliza.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;) interesting. It is classical in the history of AI and pretends to mimic an analyst (psychology). (I am thinking of Eliza not because of its emotional intelligence, but because it was &lt;a href=&quot;http://www.alicebot.org/articles/wallace/eliza.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;apparently taken seriously&lt;/a&gt; by a couple of humans. Could this be taken as a sort of (approved) Turing test? What does it say about the humans it met?)&lt;/p&gt;&#xA;&lt;p&gt;On the &lt;em&gt;purely human&lt;/em&gt; end of the scale, I sometimes wonder about our (my) emotional intelligence myself. Would I want to implement such an intelligence in an artificial agent at all?&lt;/p&gt;&#xA;" OwnerUserId="70" LastEditorUserId="2444" LastEditDate="2021-02-01T01:03:23.217" LastActivityDate="2021-02-01T01:03:23.217" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="178" PostTypeId="2" ParentId="172" CreationDate="2016-08-03T04:52:30.067" Score="2" Body="&lt;p&gt;This is known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Intelligence_explosion&quot; rel=&quot;nofollow noreferrer&quot;&gt;intelligence explosion&lt;/a&gt; hypothesis or &lt;a href=&quot;https://wiki.lesswrong.com/wiki/Recursive_self-improvement&quot; rel=&quot;nofollow noreferrer&quot;&gt;recursive self-improvement&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="130" LastEditorUserId="2444" LastEditDate="2019-06-20T20:46:22.590" LastActivityDate="2019-06-20T20:46:22.590" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="179" PostTypeId="1" CreationDate="2016-08-03T05:15:39.443" Score="7" ViewCount="728" Body="&lt;p&gt;I have been wondering since a while ago about the &lt;a href=&quot;https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences&quot; rel=&quot;nofollow noreferrer&quot;&gt;theory of multiple intelligences&lt;/a&gt; and how they could fit in the field of Artificial Intelligence as a whole.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We hear from time to time about &lt;a href=&quot;https://www.theguardian.com/artanddesign/jonathanjonesblog/2016/feb/08/leonardo-da-vinci-mechanics-of-genius-science-museum-london&quot; rel=&quot;nofollow noreferrer&quot;&gt;Leonardo Da Vinci&lt;/a&gt; being a genius or &lt;a href=&quot;https://www.youtube.com/watch?v=xUHQ2ybTejU&quot; rel=&quot;nofollow noreferrer&quot;&gt;Bach's musical intelligence&lt;/a&gt;. These persons are commonly said to be (have been) &lt;em&gt;more intelligent&lt;/em&gt;. But the multiple intelligences speak about cooking or dancing or chatting as well, i.e. &lt;em&gt;coping with everyday tasks&lt;/em&gt; (at least that's my interpretation).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Are there some approaches on incorporating multiple intelligences into AI?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a related question: &lt;a href=&quot;https://ai.stackexchange.com/q/26/2444&quot;&gt;How could emotional intelligence be implemented?&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="70" LastEditorUserId="2444" LastEditDate="2021-12-12T15:56:26.503" LastActivityDate="2021-12-12T15:56:26.503" Title="How can the theory of multiple intelligences be incorporated into AI?" Tags="&lt;reference-request&gt;&lt;definitions&gt;&lt;emotional-intelligence&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="182" PostTypeId="1" AcceptedAnswerId="183" CreationDate="2016-08-03T05:31:30.353" Score="18" ViewCount="439" Body="&lt;p&gt;How do I decide the optimal number of layers for a neural network (feedforward or recurrent)?&lt;/p&gt;&#xA;" OwnerUserId="202" LastEditorUserId="2444" LastEditDate="2020-04-12T18:54:07.443" LastActivityDate="2020-04-12T18:54:07.443" Title="How do I decide the optimal number of layers for a neural network?" Tags="&lt;neural-networks&gt;&lt;recurrent-neural-networks&gt;&lt;feedforward-neural-networks&gt;&lt;hyperparameter-optimization&gt;&lt;hidden-layers&gt;" AnswerCount="2" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="183" PostTypeId="2" ParentId="182" CreationDate="2016-08-03T05:35:48.807" Score="7" Body="&lt;p&gt;There is a technique called &lt;code&gt;Pruning&lt;/code&gt; in neural networks, which is used just for this same purpose.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The pruning is done on the number of hidden layers. The process is very similar to the pruning process of decision trees. The pruning process is done as follows:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Train a large, densely connected, network with a standard training&#xA;algorithm&lt;/li&gt;&#xA;&lt;li&gt;Examine the trained network to assess the relative importance of the&#xA;weights&lt;/li&gt;&#xA;&lt;li&gt;Remove the least important weight(s)&lt;/li&gt;&#xA;&lt;li&gt;retrain the pruned network&lt;/li&gt;&#xA;&lt;li&gt;Repeat steps 2-4 until satisfied&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;However, there are &lt;a href=&quot;https://arxiv.org/abs/1510.00149&quot;&gt;several optimized methods&lt;/a&gt; for pruning neural nets, and it is also a &lt;a href=&quot;http://www.idiap.ch/ftp/reports/1997/rr97-03.pdf&quot;&gt;very active area of research&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="101" LastActivityDate="2016-08-03T05:35:48.807" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="184" PostTypeId="1" CreationDate="2016-08-03T06:03:28.903" Score="3" ViewCount="58" Body="&lt;p&gt;I am interested in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Emergence&quot; rel=&quot;nofollow&quot;&gt;emergence&lt;/a&gt; of properties in &lt;a href=&quot;https://en.wikipedia.org/wiki/Agent-based_model#Theory&quot; rel=&quot;nofollow&quot;&gt;agents&lt;/a&gt;, and, more generally in robotics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering if there is work on the emergence of time-related concepts, on the low-level representation of notions like &lt;em&gt;before&lt;/em&gt; and &lt;em&gt;after&lt;/em&gt;. I know, for example, that there is work on the emergence of &lt;a href=&quot;http://www.scholarpedia.org/article/Kohonen_network&quot; rel=&quot;nofollow&quot;&gt;spatial representation&lt;/a&gt; (similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot; rel=&quot;nofollow&quot;&gt;knn&lt;/a&gt;), or even &lt;a href=&quot;https://infoscience.epfl.ch/record/129415/files/Mitrietal_1.pdf&quot; rel=&quot;nofollow&quot;&gt;communication&lt;/a&gt;* but time seems to be a tricky concept. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This has everything to do with the &lt;em&gt;platform&lt;/em&gt;, i.e. the way that the representation would be coded in. We tend to favour ways that have some meaning or somehow mimic natural, well, yes, human structures, like the brain. I am not a neuroscientist and do not know that the sense of time &lt;em&gt;looks like&lt;/em&gt; in humans, or if it is even present in other living beings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is there some work on the (emergence of the) representation of &lt;em&gt;time&lt;/em&gt; in artificial agents?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;*I remember watching a really cool... Actually creepy video from these robots but cannot find it anymore. Does anyone have the link at hand?&lt;/p&gt;&#xA;" OwnerUserId="70" LastEditorUserId="70" LastEditDate="2016-09-02T21:11:12.350" LastActivityDate="2016-09-02T21:11:12.350" Title="Are there emergent models of time in robots?" Tags="&lt;knowledge-representation&gt;&lt;time&gt;&lt;embodied-cognition&gt;" AnswerCount="1" CommentCount="7" ContentLicense="CC BY-SA 3.0" />
  <row Id="185" PostTypeId="2" ParentId="172" CreationDate="2016-08-03T06:10:03.807" Score="2" Body="&lt;p&gt;Humans might create somewhere in the future a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity). Also, since humans can create machines as good as the ultraintelligent machine, this machine can create better machines, which in turn can create better machines, etcetera. This is known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Intelligence_explosion&quot; rel=&quot;nofollow noreferrer&quot;&gt;Intelligence explosion&lt;/a&gt;, and it is also called recursive self-improvement.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The existence, let alone the development, if an ultraintelligent machine is still hypothetical. We are nowhere close to creating an ultraintelligent machine.&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="2444" LastEditDate="2019-06-20T20:48:44.383" LastActivityDate="2019-06-20T20:48:44.383" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="186" PostTypeId="1" CreationDate="2016-08-03T06:20:12.393" Score="7" ViewCount="3940" Body="&lt;p&gt;Have there been proposed extensions to go beyond a Turing machine that solve the halting problem and if so, would those proposed extensions have value to advance strong Artificial Intelligence?  For example, does quantum computing go beyond the definition of a Turing machine and resolve the halting problem, and does that help in creating strong AI?&lt;/p&gt;&#xA;" OwnerUserId="55" LastActivityDate="2016-08-07T23:34:29.837" Title="Does a quantum computer resolve the halting problem and would that advance strong AI?" Tags="&lt;quantum-computing&gt;&lt;halting-problem&gt;&lt;agi&gt;" AnswerCount="2" CommentCount="5" ClosedDate="2016-08-15T03:28:50.017" ContentLicense="CC BY-SA 3.0" />
  <row Id="187" PostTypeId="2" ParentId="37" CreationDate="2016-08-03T06:37:01.983" Score="2" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;(this was intended as a comment, but turned out long and longer)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;A couple of points to elaborate on &lt;a href=&quot;https://ai.stackexchange.com/a/73/70&quot;&gt;Ben's answer&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It is possible to generate different models (out of existing data!) and then look for the model that best fit new data (e.g. with &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot; rel=&quot;nofollow noreferrer&quot;&gt;knn&lt;/a&gt;). Example: &#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;States = {&lt;em&gt;sleep&lt;/em&gt;, &lt;em&gt;eat&lt;/em&gt;, &lt;em&gt;walk&lt;/em&gt;, &lt;em&gt;work&lt;/em&gt;}&lt;/li&gt;&#xA;&lt;li&gt;Model 1: Most probable sequence on weekdays, say: sleep → sleep → eat → walk → work → work → eat → walk → sleep  → sleep&lt;/li&gt;&#xA;&lt;li&gt;Model 2: Most probable sequence on weekends, some: sleep → sleep → eat → walk → eat → walk → sleep → sleep&lt;/li&gt;&#xA;&lt;li&gt;New data arrives: Which sequence is more probable that it came from? Check model 1, check model 2. Which fits better? → Assign&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Note that the previous example is oversimplified. Also note that a &lt;em&gt;unit time&lt;/em&gt; is needed there (other than letters / words, for instance).&lt;/li&gt;&#xA;&lt;li&gt;You can &lt;em&gt;nest&lt;/em&gt; Markov models. That means that you generate a model (a set of probabilities for all the states) in a &quot;lower scale&quot; and then use it in a more abstract model. For example, you can nest your day-scale model to a month or year (to include holidays, for instance).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Also &lt;a href=&quot;http://blog.wolfram.com/2013/02/04/centennial-of-markov-chains/&quot; rel=&quot;nofollow noreferrer&quot;&gt;see this link for a nice introduction&lt;/a&gt; and &lt;a href=&quot;https://stats.stackexchange.com/questions/tagged/mcmc?sort=votes&amp;amp;pageSize=50&quot;&gt;some posts in crossvalidated&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;As for the question if artificial intelligence can be created by using this kind of methods, my personal (easy) answer would be &lt;strong&gt;no&lt;/strong&gt;, because they only relate data and probabilities and thus belong more to the statistics and machine learning branch. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A longer answer needs to take into account the &lt;a href=&quot;https://ai.stackexchange.com/questions/74/what-is-the-difference-between-strong-ai-and-weak-ai&quot;&gt;weak vs. strong AI question&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="70" LastEditorUserId="-1" LastEditDate="2017-04-13T12:53:10.013" LastActivityDate="2016-08-03T06:37:01.983" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="188" PostTypeId="2" ParentId="184" CreationDate="2016-08-03T07:00:16.047" Score="3" Body="&lt;p&gt;To my knowledge, this is very  much an open research issue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is a paper by Prof Leslie Smith, an acknowledged expert on neuromorphic perceptual coding, which explains the importance of the notion of perceptual time for Artificial General Intelligence and sketches an architecture from which a notion of 'now' might emerge: &lt;a href=&quot;http://www.cs.stir.ac.uk/~lss/recentpapers/perctime.pdf&quot; rel=&quot;nofollow&quot;&gt;Perceptual Time&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-03T07:00:16.047" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="189" PostTypeId="2" ParentId="26" CreationDate="2016-08-03T07:13:10.390" Score="21" Body="&lt;p&gt;Architectures for recognizing and generating emotion are typically somewhat complex and don't generally have short descriptions, so it's probably better to reference the literature rather than give a misleading soundbite:&lt;/p&gt;&#xA;&lt;p&gt;Some of the early work in &lt;em&gt;affective computing&lt;/em&gt; was done by &lt;a href=&quot;https://web.media.mit.edu/%7Epicard/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Rosalind W. Picard&lt;/a&gt;. There is a &lt;a href=&quot;http://affect.media.mit.edu/&quot; rel=&quot;nofollow noreferrer&quot;&gt;research group at MIT&lt;/a&gt; specializing in this area.&lt;/p&gt;&#xA;&lt;p&gt;Some of the more developed architectural ideas are due to Marvin Minsky.&#xA;A pre-publication draft of his book, &lt;em&gt;The Emotion Machine&lt;/em&gt;, is available via &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Emotion_Machine&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wikipedia&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Emotional intelligence would certainly seem to be a necessary component of passing the Turing test - indeed, in the original Turing test essay in &lt;a href=&quot;https://academic.oup.com/mind/article/LIX/236/433/986238&quot; rel=&quot;nofollow noreferrer&quot;&gt;Computing Machinery and Intelligence&lt;/a&gt; implied some degree of &amp;quot;Theory of Mind&amp;quot; about Mr. Pickwick's preferences:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Yet Christmas is a Winter’s day, and I do not think Mr. Pickwick would mind the comparison.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="42" LastEditorUserId="2444" LastEditDate="2021-02-01T00:57:02.330" LastActivityDate="2021-02-01T00:57:02.330" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="190" PostTypeId="2" ParentId="186" CreationDate="2016-08-03T07:36:08.923" Score="4" Body="&lt;p&gt;It depends a bit on what you mean by 'quantum computer'. The 'conventional' notion is that quantum computation buys a (in some cases, exponential) speedup - it doesn't change &lt;em&gt;what&lt;/em&gt; can be computed, just how quickly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In contrast, advocates of &lt;em&gt;hypercomputation&lt;/em&gt; claim that quantum effects may make it possible to do infinite computations in finite time. Note, however, that this is not a mainstream belief - the reknowned logician Martin Davis has written an article claiming that hypercomputation is &lt;a href=&quot;http://www1.maths.leeds.ac.uk/~pmt6sbc/docs/davis.myth.pdf&quot; rel=&quot;nofollow&quot;&gt;a myth&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Roger Penrose has also claimed that quantum vibrations in neural microtubules may be &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_mind&quot; rel=&quot;nofollow&quot;&gt;responsible for consciousness&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-03T07:36:08.923" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="191" PostTypeId="1" AcceptedAnswerId="1526" CreationDate="2016-08-03T07:57:21.743" Score="7" ViewCount="203" Body="&lt;p&gt;What was the first AI that was able to carry on a conversation, with real responses, such as in the famous &lt;a href=&quot;https://www.youtube.com/watch?v=WnzlbyTZsQY&quot; rel=&quot;nofollow&quot;&gt;'I am not a robot. I am a unicorn' case?&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A 'real response' constitutes a sort-of personalized answer to a specific input by a user.&lt;/p&gt;&#xA;" OwnerUserId="145" LastEditorUserId="4302" LastEditDate="2018-10-08T12:44:10.017" LastActivityDate="2018-10-08T12:44:10.017" Title="What was the first machine that was able to carry on a conversation?" Tags="&lt;natural-language-processing&gt;&lt;chat-bots&gt;&lt;history&gt;&lt;turing-test&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="193" PostTypeId="2" ParentId="146" CreationDate="2016-08-03T08:22:36.737" Score="1" Body="&lt;p&gt;A neural net with even a single hidden layer is capable of Universal function approximation - it can approximate any continuous function 'as closely as you like'.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hence, one option would be to look for GOFAI applications that would benefit from this property - for example, in state-space search approaches where the utility of a state is not readily defined in advance, and could instead be learned.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-03T08:22:36.737" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="194" PostTypeId="5" CreationDate="2016-08-03T08:44:31.107" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-03T08:44:31.107" LastActivityDate="2016-08-03T08:44:31.107" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="195" PostTypeId="4" CreationDate="2016-08-03T08:44:31.107" Score="0" Body="For questions regarding handwriting recognition" OwnerUserId="101" LastEditorUserId="101" LastEditDate="2016-08-04T02:54:15.227" LastActivityDate="2016-08-04T02:54:15.227" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="196" PostTypeId="2" ParentId="191" CreationDate="2016-08-03T08:49:27.637" Score="5" Body="&lt;p&gt;In 1986, the first PC therapist program was written by Joseph Weintraub. This program won the first Loebner Prize in 1991, and then again in 1992, 1993 and 1995. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In 1981 or 1982, Jabberwacky was founded, which is the foundation of the current Cleverbot. Jabberwacky  appeared on the internet in 1997, reaching the third place for the Loebner Prize in 2003, the second place in 2004, and won in 2005 and 2006. In 2008, Cleverbot was launched as an variant of Jabberwacky. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure these are really the earliest, but that also depends on what you want earliest (programming started, first conversation,  first decent conversation, etc.). Also, it depends on what you call a &quot;real response&quot;.&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2016-08-03T08:49:27.637" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="197" PostTypeId="1" CreationDate="2016-08-03T08:56:53.430" Score="11" ViewCount="629" Body="&lt;p&gt;This question stems from quite a few &quot;informal&quot; sources. Movies like &lt;em&gt;2001, A Space Odyssey&lt;/em&gt; and &lt;em&gt;Ex Machina&lt;/em&gt;; books like &lt;em&gt;Destination Void&lt;/em&gt; (Frank Herbert), and others suggest that general intelligence &lt;em&gt;wants&lt;/em&gt; to survive, and even learn the importance for it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There may be several arguments for survival. What would be the most prominent?&lt;/p&gt;&#xA;" OwnerUserId="169" LastEditorUserId="2444" LastEditDate="2019-11-22T23:30:37.760" LastActivityDate="2019-11-22T23:30:37.760" Title="Is there a strong argument that survival instinct is a prerequisite for creating an AGI?" Tags="&lt;philosophy&gt;&lt;agi&gt;" AnswerCount="3" CommentCount="4" FavoriteCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="198" PostTypeId="1" AcceptedAnswerId="1345" CreationDate="2016-08-03T09:01:05.790" Score="18" ViewCount="358" Body="&lt;p&gt;Identifying sarcasm is considered one of the most difficult open-ended problems in the domain of ML and NLP/NLU.&lt;/p&gt;&#xA;&lt;p&gt;So, was there any considerable research done on that front? If yes, then what is the accuracy like? Please, also, explain the NLP model briefly.&lt;/p&gt;&#xA;" OwnerUserId="101" LastEditorUserId="2444" LastEditDate="2021-12-22T18:05:11.497" LastActivityDate="2021-12-22T18:05:11.497" Title="What research has been done in the domain of &quot;identifying sarcasm in text&quot;?" Tags="&lt;natural-language-processing&gt;&lt;reference-request&gt;&lt;natural-language-understanding&gt;&lt;semantics&gt;" AnswerCount="2" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="199" PostTypeId="2" ParentId="41" CreationDate="2016-08-03T09:06:26.437" Score="3" Body="&lt;p&gt;It all depends of what your A.I. can do. Even humans cannot do everything.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If your AI program is so smart, ask it to take the general IQ tests for humans. Because the real IQ tests are made of several questions from different areas, so in that way you can measure IQ of your AI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is because the &lt;strong&gt;IQ&lt;/strong&gt; means the tests which are &lt;strong&gt;designed&lt;/strong&gt; to assess human intelligence.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;An intelligence quotient (IQ) is a total score derived from one of several standardized tests designed to assess human intelligence.&lt;sup&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Intelligence_quotient&quot; rel=&quot;nofollow noreferrer&quot;&gt;wiki&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So there is no any other way of measuring IQ without taking IQ test, otherwise it won't be IQ (very logical).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If your program is not so smart, you should look for specific tests related to the expertise or problem being solved. Ideally let it compete with humans who has the same expertise in that area, but it's important make the test on the same ground/level.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example the intelligence of &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)&quot; rel=&quot;nofollow noreferrer&quot;&gt;Deep Blue&lt;/a&gt; project was measured by playing chess with Kasparov. Then if world champion cannot win the game, who will?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're writing program to play a game, make it play with compete with humans and measure the intelligence in terms of score.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;The equivalent of IQ for AI is a Turing Test (like &lt;a href=&quot;https://ai.stackexchange.com/q/1397/8&quot;&gt;MIST&lt;/a&gt; and other), see:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://ai.stackexchange.com/q/15/8&quot;&gt;Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastEditorUserId="-1" LastEditDate="2017-04-13T12:53:10.013" LastActivityDate="2016-08-06T11:09:15.160" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="200" PostTypeId="2" ParentId="111" CreationDate="2016-08-03T09:17:14.977" Score="32" Body="&lt;p&gt;Personally, I think this might be an overhyped issue. Trolley problems only occur when the situation is optimized to prevent &quot;3rd options&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A car has brakes, does it not? &quot;But what if the brakes don't work?&quot; Well, then &lt;strong&gt;the car is not allowed to drive at all.&lt;/strong&gt; Even in regular traffic, human operators are taught that your speed should be limited as such that you can stop within the area you can see. Solutions like these will reduce the possibility of a trolley problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for animals... if there is no explicit effort to deal with humans on the road I think animals will be treated the same. This sounds implausible - roadkill happens often and human &quot;roadkill&quot; is unwanted, but animals are a lot smaller and harder to see than humans, so I think detecting humans will be easier, preventing a lot of the accidents.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other cases (bugs, faults while driving, multiple failures stacked onto each other), perhaps accidents will occur, they'll be analysed, and vehicles will be updated to avoid causing similar situations. &lt;/p&gt;&#xA;" OwnerUserId="74" LastEditorUserId="74" LastEditDate="2016-08-03T09:31:21.937" LastActivityDate="2016-08-03T09:31:21.937" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="201" PostTypeId="2" ParentId="70" CreationDate="2016-08-03T09:18:24.867" Score="13" Body="&lt;p&gt;Convolutional Nets (CNN) rely on mathematical convolution (e.g. 2D or 3D convolutions), which is commonly used for signal processing. Images are a type of signal, and convolution can equally be used on sound, vibrations, etc. So, in principle, CNNs can find applications to any signal, and probably more.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In practice, there exists already work on NLP (as mentioned by Matthew Graves), where some people process text with CNNs rather than recursive networks. Some other works apply to sound processing (no reference here, but I have yet unpublished work ongoing).&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Original contents: In answer to the original title question, which has changed now. Perhaps need to delete this one&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Research on adversarial networks (and related) show that even &lt;a href=&quot;http://arxiv.org/abs/1412.1897&quot; rel=&quot;noreferrer&quot;&gt;deep networks can easily be fooled&lt;/a&gt;, leading them to see a dog (or whatever object) in what appears to be random noise when a human look at it (the article has clear examples).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another issue is the generalization power of a neural network. Convolutional nets have amazed the world with their capability to generalize way better than other techniques. But if the network is only fed images of cats, it will recognize only cats (and probably see cats everywhere, as by adversarial network results). In other words, even CNs have a hard time generalizing too far &lt;em&gt;beyond&lt;/em&gt; what they learned from.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The recognition limit is hard to define precisely. I would simply say that the diversity of the learning data pushes the limit (I assume further detail should lead to more appropriate venue for discussion).&lt;/p&gt;&#xA;" OwnerUserId="169" LastEditorUserId="169" LastEditDate="2016-08-07T09:03:34.373" LastActivityDate="2016-08-07T09:03:34.373" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="202" PostTypeId="1" AcceptedAnswerId="206" CreationDate="2016-08-03T09:18:52.437" Score="1" ViewCount="297" Body="&lt;p&gt;I'd like to know more about &lt;a href=&quot;https://ai.stackexchange.com/q/26/8&quot;&gt;implementing emotional intelligence&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Given I'm implementing a chatbot and I'd like to introduce the levels of curiosity to measure whether user text input is interesting or not.&lt;/p&gt;&#xA;&lt;p&gt;A high level would mean the bot is asking more questions and is following the topic. A lower level of curiosity makes the bot not asking any questions and changing the topics.&lt;/p&gt;&#xA;&lt;p&gt;Less interesting content could mean the bot doesn't see any opportunity to learn something new or it doesn't understand the topic or doesn't want to talk about it, because of its low quality.&lt;/p&gt;&#xA;&lt;p&gt;How this possibly can be achieved? Are there any examples?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-09-23T16:19:11.397" LastActivityDate="2021-09-23T16:19:11.397" Title="How can you simulate level of curiosity for a chatbot?" Tags="&lt;chat-bots&gt;&lt;emotional-intelligence&gt;&lt;artificial-creativity&gt;&lt;artificial-curiosity&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="205" PostTypeId="1" AcceptedAnswerId="219" CreationDate="2016-08-03T10:03:58.587" Score="6" ViewCount="3558" Body="&lt;p&gt;I would like to learn more about whether it is possible and how to write a program that decompiles executable binary (an object file) to the C source. I'm not asking exactly 'how', but rather how this can be achieved.&lt;/p&gt;&#xA;&lt;p&gt;Given the following &lt;code&gt;hello.c&lt;/code&gt; file (as example):&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;&#xA;int main() {&#xA;  printf(&amp;quot;Hello World!&amp;quot;);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then after compilation (&lt;code&gt;gcc hello.c&lt;/code&gt;) I've got the binary file like:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;math-container&quot;&gt;$ hexdump -C a.out | head&#xA;00000000  cf fa ed fe 07 00 00 01  03 00 00 80 02 00 00 00  |................|&#xA;00000010  0f 00 00 00 b0 04 00 00  85 00 20 00 00 00 00 00  |.......... .....|&#xA;00000020  19 00 00 00 48 00 00 00  5f 5f 50 41 47 45 5a 45  |....H...__PAGEZE|&#xA;00000030  52 4f 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |RO..............|&#xA;00000040  00 00 00 00 01 00 00 00  00 00 00 00 00 00 00 00  |................|&#xA;00000050  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|&#xA;00000060  00 00 00 00 00 00 00 00  19 00 00 00 d8 01 00 00  |................|&#xA;00000070  5f 5f 54 45 58 54 00 00  00 00 00 00 00 00 00 00  |__TEXT..........|&#xA;$&lt;/span&gt; wc -c hello.c a.out &#xA;  60 hello.c&#xA;8432 a.out&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;For the learning dataset, I assume I'll have to have thousands of source code files along with its binary representation, so the algorithm can learn about moving parts on certain changes.&lt;/p&gt;&#xA;&lt;p&gt;How would you tackle this problem?&lt;/p&gt;&#xA;&lt;p&gt;My concerns (and sub-questions) are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Does my algorithm need to be aware of the header file, or it's &amp;quot;smart&amp;quot; enough to figure it out?&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;If it needs to know about the header, how do I tell my algorithm &amp;quot;here is the header file&amp;quot;?&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;What should be input/output mapping (whether some section to section or file to file)?&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Do I need to divide my source code into some sections?&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Do I need to know exactly how decompilers work or AI can figure it out for me?&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Should I have two neural networks, one for header, another for body it-self?&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;or more separate neural networks, each one for each logical component (e.g. byte-&amp;gt;C tag, etc.)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2020-11-19T15:17:41.320" LastActivityDate="2020-11-19T21:34:48.317" Title="How to write a C decompiler using AI?" Tags="&lt;neural-networks&gt;&lt;deep-learning&gt;&lt;reference-request&gt;&lt;c&gt;" AnswerCount="2" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="206" PostTypeId="2" ParentId="202" CreationDate="2016-08-03T10:12:58.320" Score="8" Body="&lt;p&gt;It's possible to implement a form of curiosity-driven behavior without requiring full 'emotional intelligence'. One elementary strategy would be to define some form of similarity measure on inputs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More generally, Jurgen Schmidhuber has pioneered work on 'Artificial Curiosity/Creativity' and 'Intrinsic Motivation' and has written a number of papers on the subject:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://people.idsia.ch/~juergen/curioussingapore/curioussingapore.html&quot;&gt;Artificial Curiosity&lt;/a&gt; &lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://people.idsia.ch/~juergen/ieeecreative.pdf&quot;&gt;Intrinsic Motivation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Here is a &lt;a href=&quot;https://www.youtube.com/watch?v=Ipomu0MLFaI&quot;&gt;video&lt;/a&gt; of a nice associated presentation.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-03T10:12:58.320" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="207" PostTypeId="1" CreationDate="2016-08-03T10:14:49.743" Score="4" ViewCount="98" Body="&lt;p&gt;Text summarization is a long-standing research problem that was &lt;em&gt;&quot;ignited&quot;&lt;/em&gt; by Luhn in 1958. However, a half century later, we still came nowhere close  to solving this problem (abstractive summarization). The reason for this might be because researchers are resorting to statistical (and sometimes linguistic) methods to find &amp;amp; extract the most salient parts of the text.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is summarization problem solvable using AI (neural networks to be precise)? &lt;/p&gt;&#xA;" OwnerDisplayName="user220" LastEditorUserId="4302" LastEditorDisplayName="user220" LastEditDate="2018-10-08T12:42:18.353" LastActivityDate="2018-10-08T12:42:18.353" Title="Can abstractive summarization be achieved using neural networks?" Tags="&lt;neural-networks&gt;&lt;natural-language-processing&gt;&lt;text-summarization&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="208" PostTypeId="2" ParentId="63" CreationDate="2016-08-03T10:26:17.493" Score="12" Body="&lt;p&gt;Formally, a single hidden layer is sufficient to approximate a continuous function to any desired degree of accuracy, so in that sense, you never need more than 1. This is called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_approximation_theorem&quot; rel=&quot;noreferrer&quot;&gt;Universal Approximation Theorem&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finding the best topology for a given problem is an open research problem. As far as I know, there are few universal 'rules of thumb' for this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a given problem, one option is to apply a &lt;em&gt;neuroevolutionary&lt;/em&gt; approach such as &lt;a href=&quot;https://www.cs.ucf.edu/~kstanley/neat.html&quot; rel=&quot;noreferrer&quot;&gt;NEAT&lt;/a&gt;, which attempts to find a topology that works well for the problem at hand.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="4398" LastEditDate="2017-11-05T17:58:56.897" LastActivityDate="2017-11-05T17:58:56.897" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="211" PostTypeId="1" AcceptedAnswerId="1664" CreationDate="2016-08-03T12:27:56.120" Score="4" ViewCount="695" Body="&lt;p&gt;What AI techniques does IBM use for its Watson platform (specifically, its natural language processing part)?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-06-29T23:05:19.380" LastActivityDate="2019-07-01T21:18:00.277" Title="What are the main AI technologies behind the Watson platform?" Tags="&lt;natural-language-processing&gt;&lt;watson&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="212" PostTypeId="1" CreationDate="2016-08-03T12:41:06.953" Score="1" ViewCount="154" Body="&lt;p&gt;I'm investigating the possibility of storing the semantic-lexical connections (such as the relationships to the other words such as phrases and other dependencies, its strength, part of speech, language, etc.) in order to provide analysis of the input text.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume this has been already done. If so, to avoid reinventing the wheel, is there any efficient method to store and manage such data in some common format which has been already researched and tested?&lt;/p&gt;&#xA;" OwnerUserId="8" LastActivityDate="2016-08-26T23:10:09.577" Title="How to store datasets of lexical connections?" Tags="&lt;algorithm&gt;&lt;models&gt;&lt;research&gt;&lt;storage&gt;&lt;lexical-recognition&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="214" PostTypeId="1" CreationDate="2016-08-03T13:01:20.740" Score="2" ViewCount="104" Body="&lt;p&gt;Which objective and measurable tests have been developed to test the intelligence of AI? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The classical test is the Turing Test, which has objective criteria and is measurable since it can be measured what percentage of the jury is fooled by the AI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for other, more modern tests. &lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2018-11-01T16:55:48.237" Title="Which objective tests have been developed to test the intelligence of AI?" Tags="&lt;intelligence-testing&gt;" AnswerCount="0" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="216" PostTypeId="2" ParentId="68" CreationDate="2016-08-03T13:35:29.993" Score="7" Body="&lt;p&gt;Over the last few years, evolutionary computation research has shown increasing interest in including some aspect of epigenetics. For example:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A 2008 paper by &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0020025508002880&quot; rel=&quot;nofollow&quot;&gt;Tanev and Yuta&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Work from &lt;a href=&quot;http://faculty.hampshire.edu/lspector/pubs/Epigenetics_2015_GECCO_final.pdf&quot; rel=&quot;nofollow&quot;&gt;Lee Spector's genetic programming group&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;A recent paper by &lt;a href=&quot;http://link.springer.com/chapter/10.1007/978-3-319-30668-1_9&quot; rel=&quot;nofollow&quot;&gt;Ricalde and Banzhaf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="42" LastEditorUserId="42" LastEditDate="2016-08-04T16:42:49.973" LastActivityDate="2016-08-04T16:42:49.973" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="217" PostTypeId="2" ParentId="207" CreationDate="2016-08-03T14:03:23.977" Score="2" Body="&lt;p&gt;The ability to re-frame summarization as a problem for ANN is rather dependent on what kind of output you're looking for: you mentioned 'salient parts of the text'.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One possibly is to use a deep learning approach that first chunks together words that belong in the same phrase as a single 'feature'.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another possibility is to identify both key words and relations between them. Here is some previous work on using neural nets for &lt;a href=&quot;https://lirias.kuleuven.be/bitstream/123456789/131932/1/41238.pdf&quot; rel=&quot;nofollow&quot;&gt;relational learning&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-03T14:03:23.977" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="218" PostTypeId="1" AcceptedAnswerId="1745" CreationDate="2016-08-03T14:17:17.257" Score="7" ViewCount="4472" Body="&lt;p&gt;I'm interested in implementing a program for natural language processing (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/ELIZA&quot; rel=&quot;nofollow noreferrer&quot;&gt;ELIZA&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assuming that I'm already &lt;a href=&quot;https://ai.stackexchange.com/q/212/8&quot;&gt;storing semantic-lexical connections&lt;/a&gt; between the words and its strength.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the methods of dealing with words which have very distinct meaning?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Few examples:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;'Are we on the same page?'&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 'page' in this context isn't a document page, but it's part of the phrase.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;'I'm living in Reading.'&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 'Reading' is a city (noun), so it's not a verb. Otherwise it doesn't make any sense. Checking for the capital letter would work in that specific example, but it won't work for other (like 'make' can be either verb or noun).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;'I've read something on the Facebook wall, do you want to know what?'&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 'Facebook wall' has nothing to do with wall at all.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In general, how algorithm should distinguish the word meaning and recognise the word within the context?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Detecting the word for different type of speech, so it should recognise whether it's a verb or noun.&lt;/li&gt;&#xA;&lt;li&gt;Detecting whether the word is part of phrase.&lt;/li&gt;&#xA;&lt;li&gt;Detecting word for multiple meaning.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What are the possible approaches to solve that problem in order to  identify the correct sense of a word with the context?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="-1" LastEditDate="2017-04-13T12:53:10.013" LastActivityDate="2016-10-08T00:11:37.960" Title="How to resolve lexical ambiguity in natural language processing?" Tags="&lt;natural-language-processing&gt;&lt;lexical-recognition&gt;" AnswerCount="2" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="219" PostTypeId="2" ParentId="205" CreationDate="2016-08-03T14:19:39.400" Score="4" Body="&lt;p&gt;In-between your input and desired output, there's obviously a huge space to search. The more relevant domain information you include as features, the higher chance that the Deep Learning (DL) algorithm can find the desired mapping.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this early stage in DL research, there aren't so many rules of thumb to tell you what features to explicitly encode - not least because it depends on the size of your training corpus. My suggestion would be: obtain (or generate) a large corpus of C code, train on that with the most naive feature representation that you think might work, then repeatedly gather data and add more feature preprocessing as necessary.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This following paper describes a DL approach to what is almost the 'reverse problem' to yours - &lt;a href=&quot;http://arxiv.org/pdf/1510.07211.pdf&quot; rel=&quot;nofollow&quot;&gt;generating the source code for a program described in natural language&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found the strength of the results reported in this paper surprising, but it does give me some hope that what you are asking might be possible.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-03T14:19:39.400" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="220" PostTypeId="1" AcceptedAnswerId="223" CreationDate="2016-08-03T14:23:50.760" Score="3" ViewCount="707" Body="&lt;p&gt;How does an unsupervised learning model learn, if it does not involve any target values?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-12-20T22:48:32.750" LastActivityDate="2021-12-20T22:48:32.750" Title="How does an unsupervised learning model learn?" Tags="&lt;machine-learning&gt;&lt;unsupervised-learning&gt;&lt;models&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="221" PostTypeId="1" CreationDate="2016-08-03T14:32:39.333" Score="4" ViewCount="355" Body="&lt;p&gt;Currently, many different organizations do cutting-edge AI research, and some innovations are shared freely (at a time lag) while others are kept private. I'm referring to this state of affairs as 'multipolar,' where instead of there being one world leader that's far ahead of everyone else, there are many competitors who can be mentioned in the same breath. (There's not only one academic center of AI research worth mentioning, there might be particularly hot companies but there's not only one worth mentioning, and so on.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But we could imagine instead there being one institution that mattered when it comes to AI (be it a company, a university, a research group, or a non-profit). This is what I'm referring to as &quot;monolithic.&quot; Maybe they have access to tools and resources no one else has access to, maybe they attract the best and brightest in a way that gives them an unsurmountable competitive edge, maybe returns to research compound in a way that means early edges can't be overcome, maybe they have some sort of government coercion preventing competitors from popping up. (For other industries, network or first-mover effects might be other good examples of why you would expect that industry to be monolithic instead of multipolar.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems like we should be able to use insights from social sciences like economics or organizational design or history of science in order to figure out, if not which path seems more likely, &lt;em&gt;how we would know&lt;/em&gt; which path seems more likely.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(For example, we may be able to measure how much returns to research compound, in the sense of one organization coming up with an insight meaning that organization is likely to come up with the next relevant insight, and knowing this number makes it easier to figure out where the boundary between the two trajectories is located.)&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="2444" LastEditDate="2021-12-31T10:46:38.533" LastActivityDate="2021-12-31T10:46:38.533" Title="How would we know if AI development will continue to be multipolar, or will become monolithic?" Tags="&lt;research&gt;" AnswerCount="1" CommentCount="2" ClosedDate="2021-12-31T11:40:47.060" ContentLicense="CC BY-SA 3.0" />
  <row Id="222" PostTypeId="2" ParentId="1" CreationDate="2016-08-03T14:39:02.827" Score="10" Body="&lt;p&gt;'Backprop' is short for 'backpropagation of error' in order to avoid confusion when using &lt;em&gt;backpropagation&lt;/em&gt; term.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically &lt;em&gt;backpropagation&lt;/em&gt; refers to the method for computing the gradient of the case-wise error function with respect to the weights for a feedforward network&lt;sup&gt;Werbos&lt;/sup&gt;. And &lt;em&gt;backprop&lt;/em&gt; refers to a training method that uses backpropagation to compute the gradient.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So we can say that a &lt;em&gt;backprop&lt;/em&gt; network is a feedforward network trained by &lt;em&gt;backpropagation&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 'standard backprop' term is a euphemism for the &lt;em&gt;generalized delta rule&lt;/em&gt; which is most widely used supervised training method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Source: &lt;a href=&quot;ftp://ftp.sas.com/pub/neural/FAQ2.html#A_backprop&quot; rel=&quot;noreferrer&quot;&gt;What is backprop?&lt;/a&gt; at FAQ of Usenet newsgroup comp.ai.neural-nets&lt;/p&gt;&#xA;&#xA;&lt;p&gt;References:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Werbos, P. J. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University.&lt;/li&gt;&#xA;&lt;li&gt;Werbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting,Wiley Interscience.&lt;/li&gt;&#xA;&lt;li&gt;Bertsekas, D. P. (1995), Nonlinear Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-14-0.&lt;/li&gt;&#xA;&lt;li&gt;Bertsekas, D. P. and Tsitsiklis, J. N. (1996), Neuro-Dynamic Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-10-8.&lt;/li&gt;&#xA;&lt;li&gt;Polyak, B.T. (1964), &quot;Some methods of speeding up the convergence of iteration methods,&quot; Z. Vycisl. Mat. i Mat. Fiz., 4, 1-17.&lt;/li&gt;&#xA;&lt;li&gt;Polyak, B.T. (1987), Introduction to Optimization, NY: Optimization Software, Inc.&lt;/li&gt;&#xA;&lt;li&gt;Reed, R.D., and Marks, R.J, II (1999), Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, Cambridge, MA: The MIT Press, ISBN 0-262-18190-8.&lt;/li&gt;&#xA;&lt;li&gt;Rumelhart, D.E., Hinton, G.E., and Williams, R.J. (1986), &quot;Learning internal representations by error propagation&quot;, in Rumelhart, D.E. and McClelland, J. L., eds. (1986), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1, 318-362, Cambridge, MA: The MIT Press.&lt;/li&gt;&#xA;&lt;li&gt;Werbos, P.J. (1974/1994), The Roots of Backpropagation, NY: John Wiley &amp;amp; Sons. Includes Werbos's 1974 Harvard Ph.D. thesis, Beyond Regression.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastActivityDate="2016-08-03T14:39:02.827" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="223" PostTypeId="2" ParentId="220" CreationDate="2016-08-03T14:41:02.937" Score="8" Body="&lt;p&gt;Supervised learning is typically an attempt to learn a mathematical function, &lt;span class=&quot;math-container&quot;&gt;$f(\bf X)=\bf y$&lt;/span&gt;. For this, you need both the input vector &lt;span class=&quot;math-container&quot;&gt;$\bf X$&lt;/span&gt; and the output vector &lt;span class=&quot;math-container&quot;&gt;$\bf y$&lt;/span&gt;. The model outputs have whatever dimensionality that the target values have.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unsupervised learning models instead learn a &lt;em&gt;structure&lt;/em&gt; from the data. A clustering model, for example, is learning both how many clusters exist in the data (a number that's not the same type as the inputs) and where those clusters are located (which is also a different type from the inputs). The output of running this model on a new datapoint &lt;span class=&quot;math-container&quot;&gt;$x$&lt;/span&gt; is not the same type as &lt;span class=&quot;math-container&quot;&gt;$x$&lt;/span&gt;, but instead a classification label.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similarly, time series models learn parameters that symbolize how vectors in the input relate to each other, rather than raw inputs themselves.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for how they learn, the structures are mathematical objects whose fitness is determined by the input data. The simplest possible unstructured unsupervised learning problem is probably &quot;what's the mean of the data?&quot;, and it should be clear how that's 'learned' through processing the input. More sophisticated models are just adding more pieces to that calculation.&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="2444" LastEditDate="2019-06-06T23:21:54.023" LastActivityDate="2019-06-06T23:21:54.023" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="224" PostTypeId="1" CreationDate="2016-08-03T14:58:03.663" Score="5" ViewCount="99" Body="&lt;p&gt;One of the most compelling applications for AI would be in augmenting human biological intelligence. What are some of the currently proposed methods for doing this aside from vague notions such as &quot;nanobots swimming around our brains and bodies&quot; or &quot;electrodes connected to our skulls&quot;?&lt;/p&gt;&#xA;" OwnerUserId="148" LastEditorUserId="10135" LastEditDate="2018-10-21T20:53:59.603" LastActivityDate="2018-10-21T20:53:59.603" Title="How could AI be used to augment human biological intelligence?" Tags="&lt;cyborg&gt;" AnswerCount="1" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="225" PostTypeId="1" AcceptedAnswerId="228" CreationDate="2016-08-03T15:25:50.843" Score="-4" ViewCount="835" Body="&lt;p&gt;Given a list of fixed numbers from a mathematical constant, such as &lt;span class=&quot;math-container&quot;&gt;$\pi$&lt;/span&gt;, is it is possible to train AI to attempt to predict the next numbers of this constant?&lt;/p&gt;&#xA;&lt;p&gt;Which AI or neural network would be more suitable for this task?&lt;/p&gt;&#xA;&lt;p&gt;Especially, the one which will work without memorizing the entire training set, but the one which will attempt to find some patterns or statistical association.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-12-13T11:15:52.547" LastActivityDate="2021-12-13T11:15:52.547" Title="What are the approaches to predict sequence of $\pi$ numbers?" Tags="&lt;reference-request&gt;&lt;recurrent-neural-networks&gt;&lt;algorithm-request&gt;&lt;sequence-modeling&gt;&lt;model-request&gt;" AnswerCount="2" CommentCount="4" ClosedDate="2016-08-03T18:21:18.760" ContentLicense="CC BY-SA 4.0" />
  <row Id="226" PostTypeId="2" ParentId="224" CreationDate="2016-08-03T15:27:15.343" Score="6" Body="&lt;p&gt;'Direct augmentation' of human intelligence, of the sort that you would see in science fiction, looks to be very hard. Most of our promising approaches deal with &lt;em&gt;avoiding damage&lt;/em&gt; rather than &lt;em&gt;adding capabilities&lt;/em&gt;--there's no drug that you can take now that will make you smarter to the degree that missing a night of sleep can make you dumber.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most informative area of current practice is probably game-playing, where '&lt;a href=&quot;http://bloomreach.com/2014/12/centaur-chess-brings-best-humans-machines/&quot;&gt;centaurs&lt;/a&gt;,' or humans working with computers, outcompete human players or computer players.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But a centaur player doesn't have a wire jutting out of their skull to jack into the computer; they're looking at a laptop screen. One of the reasons to be pessimistic about cyborg augmentation is because current I/O technology is already so good. Why install a new wire to put information into your visual cortex, when you come already equipped with two? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you could think code directly onto the screen, how much better would that be than typing code through a keyboard? Probably some, but I find it difficult to imagine that it'll be more than twice as good. So most human-computer intelligence augmentation will look like people using software, and software using human inputs, rather than humans and computers evolving together.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Transcranial Direct Current Stimulation (TDCS) and similar approaches cause temporary changes in mental abilities by raising or lowering the activation potentials of neurons in particular regions of the brain. (I've done it myself, a few years ago, and what weak effects I noticed were probably negative. Not too much surprise for a DIY setup!)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It looks like it has a number of useful implications. One article about TDCS that I found particularly striking was the journalist who tried it gushing about how their anxiety disappeared for a few days, presumably because the part of their brain behind the anxiety was dampened. One could imagine it being useful for the treatment of many different mental disorders.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, I'm pessimistic that it will translate into superior &lt;em&gt;peak&lt;/em&gt; performance, and I think that's the sort of thing that's more relevant for discussions of augmentation. (Is there TDCS that we could do that would make Terrence Tao better at doing mathematics?)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where improved AI methods will come into play is by improving our models of the brain, allowing us to better target interventions, much in the way that AI methods are improving our treatment of cancer (through superior diagnosis and targeting of radiotherapy, as two easy examples). These effects will all be indirect--for example, AI empowering an app or gadget that helps you sleep better won't &lt;em&gt;directly&lt;/em&gt; augment your intelligence, but will cause population-level increases in effective intelligence through reducing sleep deprivation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't talked yet about nootropics, chemicals that increase intelligence, but it's reasonable to expect that AI will improve drug discovery there like it improves drug discovery for anything else. But the same caveats apply--the effect of nootropics seem to be negatively correlated with intelligence (that is, the smarter someone already is, the harder it is to increase their intelligence further).&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="10" LastEditDate="2016-08-03T16:12:18.483" LastActivityDate="2016-08-03T16:12:18.483" CommentCount="5" ContentLicense="CC BY-SA 3.0" />
  <row Id="228" PostTypeId="2" ParentId="225" CreationDate="2016-08-03T15:32:55.467" Score="4" Body="&lt;p&gt;Pseudo-random number generators are specifically defined to defeat any form of prediction via 'black box' observation. Certainly, some (e.g. linear congruential) have weaknesses, but you are unlikely to have any success in general in predicting the output of a modern RNG. For devices based on chaotic physical systems (e.g. most national lotteries), there is no realistic possibility of prediction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Patterns or statistical association&quot; is a much weaker criterion than 'prediction'. Some very recent work has applied topological data analysis to visualize patterns within the infamous Randu RNG.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="42" LastEditDate="2016-08-03T15:52:47.397" LastActivityDate="2016-08-03T15:52:47.397" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="230" PostTypeId="2" ParentId="225" CreationDate="2016-08-03T15:42:13.340" Score="-1" Body="&lt;p&gt;You would probably have to pack recursive structures into finite-dimensional real vectors and there have been such attempts. The finite precision limits goes as far as the recursion can go.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The limitation of &lt;em&gt;feedforward&lt;/em&gt; neural networks is restricted to finite input and output spaces, so &lt;em&gt;recurrent&lt;/em&gt; may be more suitable for this task as in theory can process arbitrarily long strings of numbers, but it has much more practical difficulties than feedforward network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These kind of methods are open to debate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Source: &lt;a href=&quot;ftp://ftp.sas.com/pub/neural/FAQ.html&quot; rel=&quot;nofollow&quot;&gt;SAS FAQ&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;References:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Blair, 1997; Pollack, 1990; Chalmers, 1990; Chrisman, 1991; Plate, 1994; Hammerton, 1998; Hadley, 1999&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastEditorUserId="8" LastEditDate="2016-08-03T15:50:40.390" LastActivityDate="2016-08-03T15:50:40.390" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="232" PostTypeId="2" ParentId="197" CreationDate="2016-08-03T15:45:41.770" Score="5" Body="&lt;p&gt;The concept of 'survival instinct' probably falls in the category of what Marvin Minsky would call a 'suitcase word', i.e. it packages together a number of related phenomena into what at first appears to be a singular notion. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So it's quite possible that we can construct mechanisms that have the appearance of some kind of 'hard-coded' survival instinct, without that ever featuring as an explicit rule(s) in the design.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See the beautiful little book &lt;a href=&quot;https://mitpress.mit.edu/books/vehicles&quot; rel=&quot;noreferrer&quot;&gt;'Vehicles'&lt;/a&gt; by the neuroanatomist Valentino Braitenberg for a compelling narrative of how such 'top down' concepts as 'survival instinct' might evolve 'from the bottom up'.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, trying to ensure that intelligent artefacts place too high a priority on their survival might easily lead to a &lt;a href=&quot;https://xkcd.com/1613/&quot; rel=&quot;noreferrer&quot;&gt;Killbot Hellscape&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-03T15:45:41.770" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="233" PostTypeId="1" AcceptedAnswerId="236" CreationDate="2016-08-03T15:56:18.480" Score="12" ViewCount="606" Body="&lt;p&gt;Currently, most research done in artificial intelligence focuses on neural networks, which have been successfully used to solve many problems.  A good example would be &lt;a href=&quot;https://www.nature.com/articles/nature16961&quot; rel=&quot;nofollow noreferrer&quot;&gt;DeepMind's AlphaGo&lt;/a&gt;, which uses a convolutional neural network. There are many other examples, such as &lt;a href=&quot;https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Google Translate, which uses transformers&lt;/a&gt;, or &lt;a href=&quot;https://www.cs.toronto.edu/%7Evmnih/docs/dqn.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;DQN&lt;/a&gt;, which has been used to play Atari games.&lt;/p&gt;&#xA;&lt;p&gt;So, are any of the variants of neural networks the only way to reach &amp;quot;true&amp;quot; artificial intelligence (or AGI)?&lt;/p&gt;&#xA;" OwnerUserId="39" LastEditorUserId="2444" LastEditDate="2021-01-25T23:21:24.513" LastActivityDate="2021-01-25T23:21:24.513" Title="Are neural networks the only way to reach &quot;true&quot; artificial intelligence?" Tags="&lt;neural-networks&gt;&lt;agi&gt;&lt;human-like&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="234" PostTypeId="2" ParentId="197" CreationDate="2016-08-03T15:58:13.493" Score="6" Body="&lt;p&gt;Steve Omohudro wrote a paper called &lt;a href=&quot;https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf&quot; rel=&quot;noreferrer&quot;&gt;Basic AI Drives&lt;/a&gt; that steps through why we would expect an AI with narrow goals to find some basic, general concepts as instrumentally useful for their narrow goals. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, an AI designed to maximize stock market returns but whose design is silent on the importance of continuing to survive would realize that its continued survival is a key component of maximizing stock market returns, and thus take actions to keep itself operational.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, we should be skeptical of 'anthropomorphizing' AI and other code, but it seems like there &lt;em&gt;are&lt;/em&gt; reasons to expect this beyond &quot;well, humans behave this way, so it must be how all intelligence behaves.&quot;&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-03T15:58:13.493" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="235" PostTypeId="2" ParentId="147" CreationDate="2016-08-03T16:05:28.357" Score="4" Body="&lt;p&gt;I presume the proof the OP is referring to can be found in &lt;a href=&quot;http://link.springer.com/book/10.1007%2F978-1-4612-0707-8&quot; rel=&quot;nofollow&quot;&gt;this monograph&lt;/a&gt; by Hava Siegelmann?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In his article &lt;a href=&quot;http://www1.maths.leeds.ac.uk/~pmt6sbc/docs/davis.myth.pdf&quot; rel=&quot;nofollow&quot;&gt;'The Myth of Hypercomputation'&lt;/a&gt;, the eminent computer scientist Martin Davis explains (p8-9) that there is nothing 'super Turing' about this formulation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: It's looking like the claim about &lt;em&gt;rational&lt;/em&gt; weights being super-Turing is made in &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/25354762&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;  more recent paper by Siegelmann, which introduces an additional assumption of &lt;em&gt;plasticity&lt;/em&gt;, i.e. that weights can be dynamically updated.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="42" LastEditDate="2016-09-03T17:38:06.577" LastActivityDate="2016-09-03T17:38:06.577" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="236" PostTypeId="2" ParentId="233" CreationDate="2016-08-03T16:21:42.693" Score="13" Body="&lt;p&gt;If by true AI, you mean 'like human beings', the answer is - no-one knows what the appropriate computational mechanisms (neural or otherwise) are or indeed whether we are capable of constructing them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What Artificial Neural Nets (ANNs) do is essentially 'nonlinear regression' - perhaps this is not a sufficiently strong model to express humanlike behaviour. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Despite the 'Universal function approximation' property of ANNs, what if human intelligence depends on some as-yet-unguessed mechanism of the physical world?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With respect to your question about &quot;the only way&quot;:&#xA;Even if (physical) neural mechanisms somehow actually were the &lt;em&gt;only&lt;/em&gt; route to intelligence (e.g. via Penrose's quantum microtubules), how could that be proved? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Even in the formal world of mathematics, there's a saying that &quot;Proofs of non-existence are hard&quot;. It scarcely seems conceivable that, in the physical world, it would be possible to demonstrate that intelligence could not arise by any other mechanism.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moving back to computational systems, note that Stephen Wolfram made the interesting observation in his book &lt;a href=&quot;http://www.wolframscience.com/nksonline/toc.html&quot;&gt;'A New Kind of Science'&lt;/a&gt; that many of the apparently distinct mechanisms he observed seem to be capable of 'Universal Computation', so in that sense there's nothing very particular about ANNs.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-03T16:21:42.693" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="237" PostTypeId="1" AcceptedAnswerId="253" CreationDate="2016-08-03T16:22:36.073" Score="5" ViewCount="232" Body="&lt;p&gt;I'm interested in hardware implementation of ANNs (artificial neural networks). Are there any popular existing technology implementations in form of microchips which are purpose designed to run artificial neural networks? For example, a chip which is optimised for an application like image recognition or something similar?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="33" LastEditDate="2016-08-03T19:55:57.227" LastActivityDate="2016-08-06T01:22:37.030" Title="Are there any microchips specifically designed to run ANNs?" Tags="&lt;image-recognition&gt;&lt;hardware&gt;" AnswerCount="1" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="238" PostTypeId="2" ParentId="159" CreationDate="2016-08-03T16:46:33.883" Score="4" Body="&lt;p&gt;As far as I can see, there's no reason why you couldn't (for example) take the convolutional inputs to deepdream from adjacent sample points, rather than adjacent spatial positions, as is the case with image input.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given the 'self similar' nature of deep dream images, listening to this &lt;a href=&quot;https://vimeo.com/13541969&quot; rel=&quot;nofollow&quot;&gt;fractal granular synthesis&lt;/a&gt; technique might be of interest/inspiration.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-03T16:46:33.883" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="239" PostTypeId="2" ParentId="233" CreationDate="2016-08-03T17:17:16.590" Score="4" Body="&lt;p&gt;It depends on what you consider &quot;true artificial intelligence&quot;. But this probably means to be able to think like a human - and perhaps, do so in a more rational manner, as in the human brain emotion comes before ratio.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would seem that a neural network, or a genetic algorithm that evolves neural networks, is the closest way - mimicking humans.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, the traditional counter-argument to this is that we tried to do the same with flight. We tried to copy nature, mimick the birds - trying to fly by flapping wings. But eventually we made airplanes that did not rely on flapping their wings.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In AI, there are far more variables than in aerodynamics. So it is quite likely that a human-like intelligence can be attained by other methods than neural networks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the end, neural networks are one approach to machine learning. There are others, all governed by the rules for what can and cannot be learnt. (There is a field called Computational Learning Theory that covers this). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Although it is possible to extend learning systems beyond what can be learnt according to COLT, this means that such a learning system - neural network or otherwise - is essentially flawed, and will draw wrong conclusions at one point or another.&lt;/p&gt;&#xA;" OwnerUserId="66" LastActivityDate="2016-08-03T17:17:16.590" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="240" PostTypeId="1" AcceptedAnswerId="243" CreationDate="2016-08-03T17:22:05.433" Score="18" ViewCount="6527" Body="&lt;p&gt;I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?&lt;/p&gt;&#xA;" OwnerUserId="30" LastEditorUserId="2444" LastEditDate="2021-01-07T00:39:57.647" LastActivityDate="2021-01-07T23:37:54.823" Title="What exactly are genetic algorithms and what sort of problems are they good for?" Tags="&lt;genetic-algorithms&gt;&lt;applications&gt;&lt;evolutionary-algorithms&gt;" AnswerCount="5" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="241" PostTypeId="1" CreationDate="2016-08-03T17:24:18.480" Score="5" ViewCount="680" Body="&lt;p&gt;In detective novels, the point is often that the reader gets enough information to solve the crime themselves. This &amp;quot;puzzle&amp;quot; aspect of detective novels is part of the attraction.&lt;/p&gt;&#xA;&lt;p&gt;Often the difficulty for humans is to keep track of all the variables - events, items, motivations.  An AI would have an easier time keeping track of all the details, but would rely on real-world knowledge to prevent making crazy mistakes. For example, if it was stated that a character took the train, the AI would need to know that this is a method of transportation - that it changes the location property of an agent over time.&lt;/p&gt;&#xA;&lt;p&gt;Has an AI ever been able to solve a detective mystery?&lt;/p&gt;&#xA;" OwnerUserId="66" LastEditorUserId="2444" LastEditDate="2021-12-17T15:00:49.017" LastActivityDate="2021-12-17T15:00:49.017" Title="Has an AI ever solved a detective mystery?" Tags="&lt;natural-language-processing&gt;&lt;problem-solving&gt;&lt;commonsense-knowledge&gt;" AnswerCount="3" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="242" PostTypeId="2" ParentId="240" CreationDate="2016-08-03T17:27:28.260" Score="9" Body="&lt;p&gt;A genetic algorithm is an algorithm that randomly generates a number of attempted solutions for a problem. This set of attempted solutions is called the &quot;population&quot;.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It then tries to see how well these solutions solve the problem, using a given &lt;em&gt;fitness function&lt;/em&gt;. The attempted solutions with the best &lt;em&gt;fitness&lt;/em&gt; value are used to generate a new population. This can be done by making small changes to the attempted solutions (mutation) or by combining existing attempted solutions (crossover).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The idea is that, over time, an attempted solution emerges that has a high enough &lt;em&gt;fitness&lt;/em&gt; value to solve the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The inspiration for this came from the theory of evolution; the fittest solutions survive and procreate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose you were looking for the most efficient way to cut a number of shapes out of a piece of wood. You want to waste as little wood as possible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your attempted solutions would be random arrangements of these shapes on your piece of wood. &lt;em&gt;Fitness&lt;/em&gt; would be determined by how little wood would be left after cutting the shapes following this arrangement.&lt;br&gt;&#xA;The less wood is left, the better the attempted solution. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose you were trying to find a polynomial that passes through a number of points. Your attempted solutions would be random polynomials.&lt;br&gt;&#xA;To determine the &lt;em&gt;fitness&lt;/em&gt; of these polynomials, you determine how well they fit the given points. (In this particular case, you would probably use the least squares method to determine how well the polynomial fit the points).&#xA;Over a number of trials, you would get polynomials that fit the points better, until you had a polynomial that fit the points closely enough.&lt;/p&gt;&#xA;" OwnerUserId="66" LastEditorUserId="66" LastEditDate="2016-08-03T17:57:10.623" LastActivityDate="2016-08-03T17:57:10.623" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="243" PostTypeId="2" ParentId="240" CreationDate="2016-08-03T17:42:02.623" Score="16" Body="&lt;p&gt;Evolutionary algorithms are a family of optimization algorithms based on the principle of &lt;strong&gt;Darwinian natural selection&lt;/strong&gt;. As part of natural selection, a given environment has a population of individuals that compete for survival and reproduction. The ability of each individual to achieve these goals determines their chance to have children, in other words, to pass on their genes to the next generation of individuals, who, for genetic reasons, will have an increased chance of doing well, even better, in realizing these two objectives.&lt;/p&gt;&#xA;&lt;p&gt;This principle of continuous improvement over the generations is taken by evolutionary algorithms to optimize solutions to a problem. In the &lt;strong&gt;initial generation&lt;/strong&gt;, a population composed of different &lt;strong&gt;individuals&lt;/strong&gt; is generated randomly or by other methods. An individual is a solution to the problem, more or less good: the quality of the individual in regards to the problem is called &lt;strong&gt;fitness&lt;/strong&gt;, which reflects the adequacy of the solution to the problem to be solved. The higher the fitness of an individual, the higher it is likely to pass some or all of its genotype to the individuals of the next generation.&lt;/p&gt;&#xA;&lt;p&gt;An individual is coded as a &lt;strong&gt;genotype&lt;/strong&gt;, which can have any shape, such as a bit vector (&lt;strong&gt;genetic algorithms&lt;/strong&gt;) or a vector of real (evolution strategies). Each genotype is transformed into a &lt;strong&gt;phenotype&lt;/strong&gt; when assessing the individual, i.e. when its fitness is calculated. In some cases, the phenotype is identical to the genotype: it is called &lt;strong&gt;direct&lt;/strong&gt; &lt;strong&gt;coding&lt;/strong&gt;. Otherwise, the coding is called indirect. For example, suppose you want to optimize the size of a rectangular parallelepiped defined by its length, height and width. To simplify the example, assume that these three quantities are integers between 0 and 15. We can then describe each of them using a 4-bit binary number. An example of a potential solution may be to genotype 0001 0111 1010. The corresponding phenotype is a parallelepiped of length 1, height 7 and width 10.&lt;/p&gt;&#xA;&lt;p&gt;During the transition from the old to the new generation, the &lt;strong&gt;variation&lt;/strong&gt; &lt;strong&gt;operators&lt;/strong&gt;, whose purpose is to manipulate individuals, are applied. There are two distinct types of variation operators:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the &lt;strong&gt;mutation&lt;/strong&gt; &lt;strong&gt;operators&lt;/strong&gt;, which are used to introduce variations within the same individual, as genetic mutations;&lt;/li&gt;&#xA;&lt;li&gt;the &lt;strong&gt;crossover&lt;/strong&gt; &lt;strong&gt;operators&lt;/strong&gt;, which are used to cross at least two different genotypes, as genetic crosses from breeding.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Evolutionary algorithms have proven themselves in various fields such as operations research, robotics, biology, nuance, or cryptography. In addition, they can optimize multiple objectives simultaneously and can be used as black boxes because they do not assume any properties in the mathematical model to optimize. Their only real limitation is the computational complexity.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wweBO.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wweBO.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="4" LastEditorUserId="2444" LastEditDate="2021-01-07T00:38:51.873" LastActivityDate="2021-01-07T00:38:51.873" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="244" PostTypeId="2" ParentId="240" CreationDate="2016-08-03T17:43:29.303" Score="6" Body="&lt;p&gt;As observed in another answer, all you need to apply Genetic Algorithms (GAs) is to represent a potential solution to your problem in a form that is subject to crossover and mutation. Ideally, the fitness function will provide some kind of smooth feedback about the quality of a solution, rather than simply being a 'Needle in a Haystack'.&lt;/p&gt;&#xA;&lt;p&gt;Here are some characteristics of problems that Genetic Algorithms (and indeed &lt;a href=&quot;https://cs.gmu.edu/%7Esean/book/metaheuristics/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Metaheuristics&lt;/a&gt; in general) are good for:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;NP-complete - The number of possible solutions to the problem is&#xA;exponential, but checking the fitness of a solution is relatively&#xA;cheap (technically, with time polynomial in the input size).&lt;/li&gt;&#xA;&lt;li&gt;Black box - GAs work reasonably well even if you don't have a particularly&#xA;informed model of the problem to be solved. This means that these&#xA;approaches are also useful as a 'rapid prototyping' approach to&#xA;solving problems.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;However, despite their widespread use for the purpose, note that GAs are actually &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.161.5655&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;not&lt;/em&gt; function optimizers&lt;/a&gt; - GA mechanisms tend not to explore 'outlying' regions of the search space in the hope of finding some distant high quality solution, but rather to cluster around more easily attainable peaks in the 'fitness landscape'.&lt;/p&gt;&#xA;&lt;p&gt;More detail on the applicability of GAs is given in a famous early paper &lt;a href=&quot;https://link.springer.com/article/10.1007/BF00993046&quot; rel=&quot;nofollow noreferrer&quot;&gt;&amp;quot;What makes a problem hard for a Genetic Algorithm?&amp;quot;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="42" LastEditDate="2021-01-07T23:37:54.823" LastActivityDate="2021-01-07T23:37:54.823" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="246" PostTypeId="2" ParentId="240" CreationDate="2016-08-03T17:58:32.980" Score="7" Body="&lt;p&gt;This answer requests a practical example of how one might be used, which I will attempt to provide in addition to the other answers. They seem to due a very good job of explaining what a genetic algorithm is. So, this will give an example.&lt;/p&gt;&#xA;&lt;p&gt;Let's say you have a neural network (although they are not the only application of it), which, from some given inputs, will yield some outputs. A genetic algorithm can create a population of these, and by seeing which output is the best, breed and kill off members of the population. Eventually, this should optimise the neural network if it is complicated enough.&lt;/p&gt;&#xA;&lt;p&gt;Here is a demonstration I've made, which despite being badly coded, might help you understand. &lt;a href=&quot;http://khrabanas.github.io/projects/evo/evo.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://khrabanas.github.io/projects/evo/evo.html&lt;/a&gt;&#xA;Hit the evolve button and mess around with the goals.&lt;/p&gt;&#xA;&lt;p&gt;It uses a simple genetic algorithm to breed, mutate, and decide which individuals of the population survive. Depending on how the input variables are set, the network will be able to get to some level of closeness to them. In this fashion, the population will likely eventually become a homogeneous group, whose outputs resemble the goals.&lt;/p&gt;&#xA;&lt;p&gt;The genetic algorithm is trying to create a &amp;quot;neural network&amp;quot; of sorts, that by taking in RGB, will yield an output color. First, it generates a random population. It then by taking 3 random members from the population, selecting the one with the lowest fitness and removing it from the population. The fitness is equal to the difference in the top goal squared + the difference in the bottom goal squared. It then breeds the two remaining ones together and adds the child to the same place in the population as the dead member. When mating occurs, there is a chance a mutation will occur. This mutation will change one of the values randomly.&lt;/p&gt;&#xA;&lt;p&gt;As a side note, due to how it is set up, it is impossible for it to be totally correct in many cases, though it will reach relative closeness.&lt;/p&gt;&#xA;" OwnerUserId="244" LastEditorUserId="2444" LastEditDate="2021-01-07T00:53:28.160" LastActivityDate="2021-01-07T00:53:28.160" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="247" PostTypeId="1" CreationDate="2016-08-03T18:05:24.997" Score="10" ViewCount="751" Body="&lt;p&gt;In 1969, Seymour Papert and Marvin Minsky showed that Perceptrons could not learn the XOR function.&lt;/p&gt;&#xA;&lt;p&gt;This was solved by the backpropagation network with at least one hidden layer. This type of network can learn the XOR function.&lt;/p&gt;&#xA;&lt;p&gt;I believe I was once taught that every problem that could be learned by a backpropagation neural network with multiple hidden layers, could also be learned by a backpropagation neural network with a single hidden layer. (Although possibly a nonlinear activation function was required).&lt;/p&gt;&#xA;&lt;p&gt;However, it is unclear to me what the limits are to backpropagation neural networks themselves. Which patterns &lt;strong&gt;cannot&lt;/strong&gt; be learned by a neural network trained with gradient descent and backpropagation?&lt;/p&gt;&#xA;" OwnerUserId="66" LastEditorUserId="2444" LastEditDate="2021-01-22T14:35:47.293" LastActivityDate="2021-01-22T14:35:47.293" Title="What are the learning limitations of neural networks trained with backpropagation?" Tags="&lt;neural-networks&gt;&lt;machine-learning&gt;&lt;backpropagation&gt;&lt;universal-approximation-theorems&gt;&lt;computational-learning-theory&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="248" PostTypeId="1" AcceptedAnswerId="5803" CreationDate="2016-08-03T18:05:26.937" Score="18" ViewCount="4062" Body="&lt;p&gt;Over the last 50 years, the rise/fall/rise in popularity of neural nets has acted as something of a 'barometer' for AI research.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's clear from the questions on this site that people are interested in applying Deep Learning (DL) to a wide variety of difficult problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I therefore have two questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Practitioners - What do you find to be the main obstacles to&#xA;applying DL 'out of the box' to your problem? &lt;/li&gt;&#xA;&lt;li&gt;Researchers - What&#xA;techniques do you use (or have developed) that might help address&#xA;practical issues? Are they within DL or do they offer an&#xA;alternative approach?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="42" LastEditorUserId="95" LastEditDate="2016-08-04T14:09:54.380" LastActivityDate="2018-03-26T13:17:52.717" Title="Issues with and alternatives to Deep Learning approaches?" Tags="&lt;deep-learning&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="249" PostTypeId="1" AcceptedAnswerId="1286" CreationDate="2016-08-03T18:12:22.133" Score="2" ViewCount="68" Body="&lt;p&gt;Is it possible for &lt;em&gt;unsupervised learning&lt;/em&gt; to learn about high-level, class-specific features given only unlabelled images? For example detecting human or animal faces? If so, how?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="8" LastEditDate="2016-08-04T00:31:01.093" LastActivityDate="2016-08-04T06:25:20.620" Title="Is it possible for 'unsupervised learning' model to recognize features on unlabelled images?" Tags="&lt;image-recognition&gt;&lt;unsupervised-learning&gt;" AnswerCount="1" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="250" PostTypeId="2" ParentId="92" CreationDate="2016-08-03T18:18:58.077" Score="60" Body="&lt;p&gt;First up, those images (even the first few) aren't complete trash despite being junk to humans; they're actually finely tuned with various advanced techniques, including another neural network.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The deep neural network is the pre-trained network modeled on AlexNet provided by &lt;a href=&quot;https://github.com/BVLC/caffe&quot;&gt;Caffe&lt;/a&gt;. To evolve images, both the directly encoded and indirectly encoded images, we use the &lt;a href=&quot;https://github.com/jbmouret/sferes2&quot;&gt;Sferes&lt;/a&gt; evolutionary framework. The entire code base to conduct the evolutionary experiments can be download [sic] &lt;a href=&quot;https://github.com/Evolving-AI-Lab/fooling&quot;&gt;here&lt;/a&gt;. The code for the images produced by gradient ascent is available &lt;a href=&quot;https://github.com/Evolving-AI-Lab/fooling/tree/master/caffe/ascent&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Images that are actually random junk were correctly recognized as nothing meaningful:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In response to an unrecognizable image, the networks could have output a low confidence for each of the 1000 classes, instead of an extremely high confidence value for one of the classes. In fact, they do just that for randomly generated images (e.g. those in generation 0 of the evolutionary run)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The original goal of the researchers was to use the neural networks to automatically generate images that look like the real things (by getting the recognizer's feedback and trying to change the image to get a more confident result), but they ended up creating the above art. Notice how even in the static-like images there are little splotches - usually near the center - which, it's fair to say, are triggering the recognition.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;We were not trying to produce adversarial, unrecognizable images. Instead, we were trying to produce recognizable images, but these unrecognizable images emerged.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Evidently, these images had just the right distinguishing features to match what the AI looked for in pictures. The &quot;paddle&quot; image does have a paddle-like shape, the &quot;bagel&quot; is round and the right color, the &quot;projector&quot; image is a camera-lens-like thing, the &quot;computer keyboard&quot; is a bunch of rectangles (like the individual keys), and the &quot;chainlink fence&quot; legitimately looks like a chain-link fence to me.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Figure 8. Evolving images to match DNN classes produces a tremendous diversity of images. Shown are images selected to showcase diversity from 5 evolutionary runs. The diversity suggests that the images are non-random, but that instead evolutions producing [sic] discriminative features of each target class.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Further reading: &lt;a href=&quot;http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf&quot;&gt;the original paper&lt;/a&gt; (large PDF)&lt;/p&gt;&#xA;" OwnerUserId="75" LastActivityDate="2016-08-03T18:18:58.077" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="251" PostTypeId="5" CreationDate="2016-08-03T18:35:44.733" Score="0" Body="&lt;p&gt;Fuzzy logic is an alternative to boolean logic.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In boolean logic, every statement is either true or false. Usually &quot;1&quot; is used to represent &quot;true&quot; and &quot;0&quot; is used to represent &quot;false&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fuzzy logic is an extension of this concept, where statements do not have such absolute values. Instead, the truth value is a real number in the range [0..1]. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As such, fuzzy logic is a real-number version of multi-valued logic.&lt;/p&gt;&#xA;" OwnerUserId="66" LastEditorUserId="66" LastEditDate="2016-08-04T02:52:30.443" LastActivityDate="2016-08-04T02:52:30.443" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="252" PostTypeId="4" CreationDate="2016-08-03T18:35:44.733" Score="0" Body="Fuzzy logic is a variant of boolean logic, where the values are real numbers between 0 and 1 (inclusive), rather than only the integer numbers 0 and 1. Use this tag for questions  that involve this real-valued logic." OwnerUserId="66" LastEditorUserId="66" LastEditDate="2016-08-04T02:52:57.023" LastActivityDate="2016-08-04T02:52:57.023" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="253" PostTypeId="2" ParentId="237" CreationDate="2016-08-03T18:37:03.843" Score="6" Body="&lt;p&gt;In May 2016 Google announced a custom ASIC which was is specifically built for machine learning&lt;sup&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/TensorFlow#DistBelief&quot; rel=&quot;nofollow&quot;&gt;wiki&lt;/a&gt;&lt;/sup&gt; and tailored for &lt;a href=&quot;https://en.wikipedia.org/wiki/TensorFlow&quot; rel=&quot;nofollow&quot;&gt;TensorFlow&lt;/a&gt;. It is using &lt;a href=&quot;https://en.wikipedia.org/wiki/Tensor_processing_unit&quot; rel=&quot;nofollow&quot;&gt;tensor processing unit&lt;/a&gt; (TPU) which is a programmable microprocessor designed to accelerate artificial neural networks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://web.stanford.edu/group/brainsinsilicon/goals.html&quot; rel=&quot;nofollow&quot;&gt;NeuroCores&lt;/a&gt;, 12x14 sq-mm chips which can be interconnected in a binary tree, see: &lt;a href=&quot;https://en.wikipedia.org/wiki/TensorFlow#DistBelief&quot; rel=&quot;nofollow&quot;&gt;Neurogrid&lt;/a&gt;, a supercomputer which can provide an option for brain simulations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/TrueNorth&quot; rel=&quot;nofollow&quot;&gt;TrueNorth&lt;/a&gt;, a neuromorphic CMOS chip produced by IBM, which has 4096 cores in the current chip, each can simulate 256 programmable silicon &quot;neurons&quot;, giving a total of over a million neurons.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Further readings: &lt;a href=&quot;https://en.wikipedia.org/wiki/Neuromorphic_engineering&quot; rel=&quot;nofollow&quot;&gt;Neuromorphic engineering&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Vision_processing_unit&quot; rel=&quot;nofollow&quot;&gt;Vision processing unit&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Category:AI_accelerators&quot; rel=&quot;nofollow&quot;&gt;AI accelerators&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;As a side note, you can always use an FPGA based piece of hardware which you can implement selected genetic algorithm (GA) directly in hardware. For example the &lt;a href=&quot;https://en.wikipedia.org/wiki/CoDi#Implementation_in_Hardware&quot; rel=&quot;nofollow&quot;&gt;CoDi model&lt;/a&gt; was implemented in the FPGA based CAM-Brain Machine (CBM)&lt;sup&gt;&lt;a href=&quot;http://link.springer.com/article/10.1023%2FA%3A1011286308522&quot; rel=&quot;nofollow&quot;&gt;2001&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="8" LastEditDate="2016-08-06T01:22:37.030" LastActivityDate="2016-08-06T01:22:37.030" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="254" PostTypeId="5" CreationDate="2016-08-03T18:45:06.577" Score="0" Body="&lt;p&gt;Humans know a lot about the real world implicitly.&lt;br&gt;&#xA;If someone is said to have &quot;filled their tank&quot;, it is implied that they went to a gas station, filled the tank of their car, and paid for it.&lt;br&gt;&#xA;An AI does not have this knowledge implicitly; it needs to be taught that this is how things work. It may be taught so explicitly by a teacher or it may learn this from scraping the net, but the knowledge does not come from real-world experiences like humans have.&lt;br&gt;&#xA;For this reason, acquiring and applying knowledge about the real world is a field of interest in Artificial Intelligence.&lt;/p&gt;&#xA;" OwnerUserId="66" LastEditorUserId="66" LastEditDate="2016-08-04T02:51:34.397" LastActivityDate="2016-08-04T02:51:34.397" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="255" PostTypeId="4" CreationDate="2016-08-03T18:45:06.577" Score="0" Body="World knowledge is knowledge about the real world. Humans have implicit knowledge of the real world, but an AI needs to have explicit resources to explain how the real world works. Use this tag for questions about how artificially intelligent systems deal with their need to acquire or apply knowledge about the real world." OwnerUserId="66" LastEditorUserId="66" LastEditDate="2016-08-04T02:52:21.053" LastActivityDate="2016-08-04T02:52:21.053" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="256" PostTypeId="5" CreationDate="2016-08-03T18:58:09.953" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-03T18:58:09.953" LastActivityDate="2016-08-03T18:58:09.953" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="257" PostTypeId="4" CreationDate="2016-08-03T18:58:09.953" Score="0" Body="For questions related to Gödel's incompleteness theorems, which are two theorems of mathematical logic that demonstrate the inherent limitations of every formal axiomatic system capable of modeling basic arithmetic." OwnerUserId="66" LastEditorUserId="2444" LastEditDate="2020-05-13T10:41:37.200" LastActivityDate="2020-05-13T10:41:37.200" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="258" PostTypeId="1" AcceptedAnswerId="3936" CreationDate="2016-08-03T19:03:20.587" Score="7" ViewCount="377" Body="&lt;p&gt;On the Wikipedia page we can read the basic structure of an artificial neuron (a model of biological neurons) which consist:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Dendrites - acts as the input vector,&lt;/li&gt;&#xA;&lt;li&gt;Soma - acts as the summation function,&lt;/li&gt;&#xA;&lt;li&gt;Axon - gets its signal from the summation behavior which occurs inside the soma.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I've checked &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot; rel=&quot;nofollow noreferrer&quot;&gt;deep learning Wikipedia page&lt;/a&gt;, but I couldn't find any references to dendrites, soma or axons.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which type of artificial neural network implements or can mimic such a model most closely?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2020-03-09T01:15:07.373" LastActivityDate="2020-03-09T01:27:59.503" Title="Which artificial neural network can mimic biological neurons the most?" Tags="&lt;artificial-neuron&gt;&lt;biology&gt;&lt;neuroscience&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="259" PostTypeId="5" CreationDate="2016-08-03T19:27:06.623" Score="0" Body="&lt;p&gt;Declarative programming is a computer programming paradigm where the focus is on declaring what should be accomplished, rather than explaining how it should be accomplished.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A good example of a declarative programming language is Prolog, which is based upon predicate logic. Here is a simple Prolog program:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dog(brutus).&#xA;dog(pluto).&#xA;dog(X):-barks(X).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This program states (declares) 3 things: (1) that brutus is dog, (2) that pluto is a dog, and (3) that if something is a dog, it barks.&lt;br&gt;&#xA;(Note that the names of the dogs are in small letters; Prolog uses capital letters to identify variables).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can now ask this program to tell us who barks:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;barks(X)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and it will give us the solutions:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X = brutus ;&#xA;X = pluto ;&#xA;no more solutions&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note how we did not tell our program &lt;strong&gt;how&lt;/strong&gt; to arrive at this result; this is done by the Prolog system itself.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Declarative programming, by focusing on the &quot;what&quot; rather than the &quot;how&quot;, is useful for building knowledge bases.&lt;/p&gt;&#xA;" OwnerUserId="66" LastEditorUserId="66" LastEditDate="2016-08-04T02:53:38.350" LastActivityDate="2016-08-04T02:53:38.350" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="260" PostTypeId="4" CreationDate="2016-08-03T19:27:06.623" Score="0" Body="Declarative programming is a programming paradigm where the focus is on what must be accomplished, rather than how it is to be accomplished. Hence it is more about &quot;declaring&quot; than about implementing algorithms. Use this tag for questions about how declarative programming is used in AI systems." OwnerUserId="66" LastEditorUserId="145" LastEditDate="2016-08-23T00:15:33.337" LastActivityDate="2016-08-23T00:15:33.337" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="262" PostTypeId="5" CreationDate="2016-08-03T19:57:38.687" Score="0" Body="&lt;p&gt;Problem solving in artificial intelligence is the study of how an AI can solve a given problem. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The usual approach to problem solving is state search. The problem was described as an initial state, conditions for a final state, and a set of transition rules. A transition rule changes takes a state as input and outputs a new state.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The solution of the problem then consists of applying the right transitions, until a state is reached where that satisfies the condition for a final state.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a concrete example, there is the problem of the Farmer, the Goat, the Cabbage and the Wolf. The farmer must row each of these to the other side of a river, but his boat is only big enough that he can transport only one of them at a time. If he leaves the goat with the cabbage, the goat will eat the cabbage; if he leaves the wolf with the goat, the wolf will eat the goat.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The initial state has the farmer, cabbage, goat and wolf on one side of the river. A final state has them all on the other side. The transition rules are all &quot;row the cabbage OR the goat OR the wolf from the current side to the other&quot;.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are several state search algorithms, where the purpose is to arrive at a final state in an efficient manner.  &lt;/p&gt;&#xA;" OwnerUserId="66" LastEditorUserId="1671" LastEditDate="2018-10-15T16:57:37.207" LastActivityDate="2018-10-15T16:57:37.207" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="263" PostTypeId="4" CreationDate="2016-08-03T19:57:38.687" Score="0" Body="For questions about AI problem solving in terms of approaches, theory, logic, and other aspects where the problem is well defined and the objective is to find a solution to the problem." OwnerUserId="66" LastEditorUserId="4302" LastEditDate="2018-10-15T16:55:35.850" LastActivityDate="2018-10-15T16:55:35.850" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="264" PostTypeId="2" ParentId="172" CreationDate="2016-08-03T20:03:22.657" Score="0" Body="&lt;p&gt;It depends what you mean by 'develop themselves' - in a rather limited sense, an online machine learning approach such as Genetic Algorithms 'develops itself' to provide better solutions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is already a theoretical model that represents the &lt;em&gt;ultimate&lt;/em&gt; concept of development: Juergen Schmidhuber's &lt;a href=&quot;http://arxiv.org/abs/cs/0309048&quot; rel=&quot;nofollow&quot;&gt;Goedel Machine&lt;/a&gt; is constructed so as to self-modify when it can prove that this modification is optimal.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-03T20:03:22.657" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="265" PostTypeId="2" ParentId="241" CreationDate="2016-08-03T20:03:26.733" Score="0" Body="&lt;p&gt;Not exactly a detective mystery, but according to a slide dated June 2012 from a NSA PowerPoint presentation (see: Glenn Greenwald’s site), NSA used some kind of &lt;em&gt;Skynet&lt;/em&gt; AI technology to analyze and detect suspicious patterns from location and communication data in order to create a watch list of suspected terrorists. This helped to track associated members of Al-Qa’ida as well as the Muslim Brotherhood. And I'm sure their AI solved a lot of mysteries and found some controversial figures.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Source: &lt;a href=&quot;https://theintercept.com/2015/05/08/u-s-government-designated-prominent-al-jazeera-journalist-al-qaeda-member-put-watch-list/&quot; rel=&quot;nofollow&quot;&gt;U.S. Government Designated Prominent Al Jazeera Journalist as a member of AI Qaeda&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For more details check: &lt;a href=&quot;https://theintercept.com/document/2015/05/08/skynet-courier/&quot; rel=&quot;nofollow&quot;&gt;SKYNET: Courier Detection via Machine Learning&lt;/a&gt; for courier detection data and charts generated by analyzing GSM metadata using machine learning algorithms. Also &lt;a href=&quot;https://theintercept.com/document/2015/05/08/skynet-applying-advanced-cloud-based-behavior-analytics/&quot; rel=&quot;nofollow&quot;&gt;Applying Advanced Cloud-based Behavior Analytics&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="8" LastActivityDate="2016-08-03T20:03:26.733" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="266" PostTypeId="2" ParentId="247" CreationDate="2016-08-03T21:08:03.797" Score="2" Body="&lt;p&gt;While I'm not familiar with any explicit statements regarding what a Multilayer Perceptron (MLP) &lt;em&gt;cannot&lt;/em&gt; learn, I can provide some further detail on the positive statements you made about MLP capabilities:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A MLP with a single hidden layer is capable of what is commonly termed &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_approximation_theorem&quot; rel=&quot;nofollow noreferrer&quot;&gt;'Universal Function Approximation'&lt;/a&gt;, i.e. it can approximate any bounded continuous function to an arbitrary degree of accuracy. With two hidden layers, the boundness restriction is removed &lt;a href=&quot;http://link.springer.com/article/10.1007/BF02551274&quot; rel=&quot;nofollow noreferrer&quot;&gt;[Cybenko, 1988]&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/089360809190009T&quot; rel=&quot;nofollow noreferrer&quot;&gt;This paper&lt;/a&gt; goes on to demonstrate that this is true for a wide range of activation functions (not necessarily nonlinear). 3 layer MLPs are also capable of representing any boolean function (although they may require an exponential number of neurons).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See also &lt;a href=&quot;https://cstheory.stackexchange.com/questions/7894/universal-function-approximation&quot;&gt;this interesting answer&lt;/a&gt; on CS SE about other Universal approximators.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="-1" LastEditDate="2017-04-13T12:32:34.750" LastActivityDate="2016-08-03T21:08:03.797" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="267" PostTypeId="2" ParentId="157" CreationDate="2016-08-03T22:41:48.160" Score="7" Body="&lt;p&gt;The following post has a bit of math, which I hope helps to explain the problem better. Unfortunately it seems, this SE site does not support LaTex:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Document summarization is very much an open problem in AI research. One way this task is currently handled is called &quot;extractive summarization&quot;. The basic strategy is as follows: Split this document into sentences and we will present as a summary a subset of sentences which together cover all the important details in the post. Assign sentence $i$, $1 \leq i \leq n$, a variable $z_i \in \{ 0, 1 \}$, where $z_i = 1$ indicates the sentence was selected and $z_i = 0$ means the sentence was left out. Then, $z_i z_j = 1$ if and only if both sentences were chosen. We will also define the importance of each sentence $w_i$ for the sentence $i$ and interaction terms $w_{i,j}$ between sentences $i$ and $j$. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let $x_i$ be the feature vectors for sentence $i$. $w_i = w(x_i)$ captures how important it is to include this sentence (or the topics covered by it) while $w_{i,j} = w(x_i,x_j)$ indicates the amount of overlap between sentences in our summary. Finally we put all this in a minimization problem:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;\begin{aligned} &#xA;\underset{z_i}{\text{maximize }} &amp;amp; \sum_{i} w_i z_i - w_{i,j} z_i z_j \\&#xA;\text{s.t. } &amp;amp; z_i = 0 \text{ or } 1&#xA;\end{aligned}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This tries to maximize the total weight of the sentences covered and tries to minimize the amount of overlap. This is an integer programming problem similar to finding the lowest weight independent set in a graph and many techniques exist to solve such problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This design, in my opinion, captures the fundamental problems in text summarization and can be extended in many ways. We will discuss those in a bit, but first, we need to completely specify the features $w$. $w_i = w(x_i)$ could be a function only of the sentence $i$, but it could also depends on the place of the sentence in the document or its context (Is the sentence at the beginning of a paragraph? Does it share common words with the title? What is its length? Does it mention any proper nouns? etc)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$w_{i,j} = w(x_i,x_j)$ is a similarity measure. It measures how many repetitions there will be if we include both words in the sentence. It can be defined by looking at common words between sentences. We can also extract topics or concepts from each sentence and see how many are common among them, and use language features like pronouns to see if one sentence expands on another.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To improve the design, first, we could do &lt;em&gt;keyphrase extraction&lt;/em&gt;, i.e. identify key phrases in the text and choose to define the above problem in terms of those instead of trying to pick sentences. That is a similar problem to what Google does to summarize news articles in their search results, but I am not aware of the details of their approach. We could also break the sentences up further into concepts and try to establish the semantic meaning of the sentences ( Ponzo and Fila are people P1 and P2, a mall is a place P, P1 and P2 went to the place P at time T (day). Mode of transport walking.... and so on). To do this, we would need to use a semantic ontology or other common-sense knowledge databases. However, all the parts of this last semantic classification problem are open and I have not seen anyone make satisfactory progress on it yet. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;We could also tweak the loss function above so that instead of the setting the tradeoff between the sentence importance $w_i$ and the diversity score $w_{i,j}$ by hand, we could learn it from data. One way to do this is to use Conditional Random Fields to model the data, but many others surely exist.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope this answer explained the basic problems that need to be solved to make progress towards good summarization systems. This is an active field of research and you will find the most recent papers via Google Scholar, but first read the &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_summarization&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wikipedia page&lt;/a&gt; to learn the relevant terms&lt;/p&gt;&#xA;" OwnerUserId="130" LastEditorUserId="1641" LastEditDate="2018-08-14T18:21:04.997" LastActivityDate="2018-08-14T18:21:04.997" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="268" PostTypeId="2" ParentId="258" CreationDate="2016-08-03T23:08:25.990" Score="2" Body="&lt;p&gt;Most artificial neurons model biological neurons but in a very simplistic way. Nowadays, the main aim is to achieve better performance at prediction tasks. However, there is a body of literature in neuroscience that looks at &lt;a href=&quot;https://en.wikipedia.org/wiki/Models_of_neural_computation&quot; rel=&quot;nofollow noreferrer&quot;&gt;computational models of neurons&lt;/a&gt;. Neurons are complicated cells and our understanding of neurons is still not complete. &lt;/p&gt;&#xA;" OwnerUserId="130" LastEditorUserId="2444" LastEditDate="2020-03-09T01:27:59.503" LastActivityDate="2020-03-09T01:27:59.503" CommentCount="0" CommunityOwnedDate="2020-11-16T17:23:30.743" ContentLicense="CC BY-SA 4.0" />
  <row Id="269" PostTypeId="5" CreationDate="2016-08-03T23:13:25.643" Score="0" Body="&lt;p&gt;&lt;a href=&quot;http://python.org&quot; rel=&quot;nofollow&quot;&gt;Python&lt;/a&gt; is a &lt;a href=&quot;https://wiki.python.org/moin/Why%20is%20Python%20a%20dynamic%20language%20and%20also%20a%20strongly%20typed%20language&quot; rel=&quot;nofollow&quot;&gt;dynamic and strongly typed&lt;/a&gt; programming language that is used for &lt;a href=&quot;http://python.org/about/apps&quot; rel=&quot;nofollow&quot;&gt;a wide range of applications&lt;/a&gt;. It is a general-purpose, high-level programming language that is designed to emphasize usability.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Python allows programmers to express concepts in fewer lines of code than would be possible in many other languages such as &lt;a href=&quot;/questions/tagged/c&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;c&amp;#39;&quot; rel=&quot;tag&quot;&gt;c&lt;/a&gt;, and the language has constructs intended to be used to create clear programs in a variety of domains.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Two similar but incompatible versions of Python are in widespread use (2 and 3). Please consider mentioning the version and implementation that you are using when asking a question about Python.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Python supports multiple programming paradigms, including object-oriented, imperative and functional programming styles. It features a fully dynamic type system and automatic memory management, similar to that of &lt;a href=&quot;/questions/tagged/scheme&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;scheme&amp;#39;&quot; rel=&quot;tag&quot;&gt;scheme&lt;/a&gt;, &lt;a href=&quot;/questions/tagged/ruby&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;ruby&amp;#39;&quot; rel=&quot;tag&quot;&gt;ruby&lt;/a&gt;, &lt;a href=&quot;/questions/tagged/perl&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;perl&amp;#39;&quot; rel=&quot;tag&quot;&gt;perl&lt;/a&gt; and &lt;a href=&quot;/questions/tagged/tcl&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;tcl&amp;#39;&quot; rel=&quot;tag&quot;&gt;tcl&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like other &lt;a href=&quot;http://en.wikipedia.org/wiki/Dynamic_programming_language&quot; rel=&quot;nofollow&quot;&gt;dynamic languages&lt;/a&gt;, Python is often used as a &lt;a href=&quot;http://stackoverflow.com/tags/scripting/info&quot;&gt;scripting language&lt;/a&gt;, but is also used in a wide range of non-scripting contexts. Using third-party tools, Python code can be packaged into standalone executable programs. Python interpreters are available for many operating systems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;/questions/tagged/cpython&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;cpython&amp;#39;&quot; rel=&quot;tag&quot;&gt;cpython&lt;/a&gt;, the reference implementation of Python, is free and open source software and has a community-based development model, as do nearly all of its alternative implementations. There are a wide variety of implementations more suited for specific environments or tasks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The philosophy of Python is succinctly formulated in &lt;a href=&quot;http://python.org/dev/peps/pep-0020&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;The Zen of Python&lt;/em&gt;&lt;/a&gt; written by Tim Peters, which can be revealed by issuing this command at the interactive interpreter:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import this&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The documentation can also be accessed offline for your installation of Python in the following manner:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Going into &lt;code&gt;Your_Python_install_dir/Doc&lt;/code&gt;. There is a complete Python documentation present for the version of Python installed on your computer.&lt;/li&gt;&#xA;&lt;li&gt;Running &lt;code&gt;pydoc x&lt;/code&gt; or &lt;code&gt;python -m pydoc x&lt;/code&gt; from the command prompt or terminal displays documentation for module &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Unlike many other languages Python uses an indentation based syntax and this may take some getting used to for programmers familiar with braces for syntax.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from __future__ import braces&#xA;  File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1&#xA;SyntaxError: not a chance&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To help with the transition it is a recommendation to use a properly configured text-editor created for programmers or an IDE. Python comes with a basic IDE called &lt;a href=&quot;http://docs.python.org/2/library/idle.html&quot; rel=&quot;nofollow&quot;&gt;IDLE&lt;/a&gt; to get you started. Other popular examples are the charity-ware &lt;a href=&quot;/questions/tagged/vim&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;vim&amp;#39;&quot; rel=&quot;tag&quot;&gt;vim&lt;/a&gt;, the free GNU &lt;a href=&quot;/questions/tagged/emacs&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;emacs&amp;#39;&quot; rel=&quot;tag&quot;&gt;emacs&lt;/a&gt;, &lt;a href=&quot;/questions/tagged/eclipse&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;eclipse&amp;#39;&quot; rel=&quot;tag&quot;&gt;eclipse&lt;/a&gt;+&lt;a href=&quot;/questions/tagged/pydev&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;pydev&amp;#39;&quot; rel=&quot;tag&quot;&gt;pydev&lt;/a&gt; or &lt;a href=&quot;/questions/tagged/pycharm&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;pycharm&amp;#39;&quot; rel=&quot;tag&quot;&gt;pycharm&lt;/a&gt;. Take a look at this &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_integrated_development_environments_for_Python#Python&quot; rel=&quot;nofollow&quot;&gt;IDE comparison list&lt;/a&gt; for many other alternatives.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Tagging recommendation:&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Use the &lt;a href=&quot;/questions/tagged/python&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;python&amp;#39;&quot; rel=&quot;tag&quot;&gt;python&lt;/a&gt; tag for all Python related questions. If you believe your question includes issues specific to individual versions, use &lt;a href=&quot;/questions/tagged/python-3.x&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;python-3.x&amp;#39;&quot; rel=&quot;tag&quot;&gt;python-3.x&lt;/a&gt; or &lt;a href=&quot;/questions/tagged/python-2.7&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;python-2.7&amp;#39;&quot; rel=&quot;tag&quot;&gt;python-2.7&lt;/a&gt; in addition to the main &lt;a href=&quot;/questions/tagged/python&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;python&amp;#39;&quot; rel=&quot;tag&quot;&gt;python&lt;/a&gt; tag. If you believe your question may be even more specific, you can include a version specific tag such as &lt;a href=&quot;/questions/tagged/python-3.5&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;python-3.5&amp;#39;&quot; rel=&quot;tag&quot;&gt;python-3.5&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, consider including the tag for the specific implementation if you are using one other than &lt;a href=&quot;/questions/tagged/cpython&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;cpython&amp;#39;&quot; rel=&quot;tag&quot;&gt;cpython&lt;/a&gt; - the use of &lt;a href=&quot;/questions/tagged/cpython&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;cpython&amp;#39;&quot; rel=&quot;tag&quot;&gt;cpython&lt;/a&gt; is assumed unless explicitly stated otherwise.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;References&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Official documentation for the current stable versions: &lt;a href=&quot;http://docs.python.org/2.7/&quot; rel=&quot;nofollow&quot;&gt;2.7.x&lt;/a&gt; and &lt;a href=&quot;http://docs.python.org/3.5/&quot; rel=&quot;nofollow&quot;&gt;3.5.x&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Release notes for the current stable versions: &lt;a href=&quot;https://www.python.org/downloads/release/python-2711/&quot; rel=&quot;nofollow&quot;&gt;2.7.11&lt;/a&gt; and &lt;a href=&quot;https://www.python.org/download/releases/3.5.1&quot; rel=&quot;nofollow&quot;&gt;3.5.1&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Python_%28programming_language%29&quot; rel=&quot;nofollow&quot;&gt;Python (programming language)&lt;/a&gt; (Wikipedia)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://wiki.python.org/moin/BeginnersGuide/Programmers&quot; rel=&quot;nofollow&quot;&gt;Python for Programmers&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.tutorialspoint.com/python/python_quick_guide.htm&quot; rel=&quot;nofollow&quot;&gt;Python - Quick Guide&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://intellipaat.com/tutorial/python-tutorial/introduction/&quot; rel=&quot;nofollow&quot;&gt;Getting started with Python&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://docs.python.org/3.5/howto/pyporting.html&quot; rel=&quot;nofollow&quot;&gt;Porting Python 2 Code to Python 3&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;The non-profit &lt;a href=&quot;http://www.python.org/psf/&quot; rel=&quot;nofollow&quot;&gt;Python Software Foundation&lt;/a&gt; manages &lt;a href=&quot;http://en.wikipedia.org/wiki/CPython&quot; rel=&quot;nofollow&quot;&gt;CPython&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.python.org/psf/&quot; rel=&quot;nofollow&quot;&gt;PSF&lt;/a&gt; License Agreement for Python &lt;a href=&quot;http://docs.python.org/2.7/license.html&quot; rel=&quot;nofollow&quot;&gt;2.7.x&lt;/a&gt; and &lt;a href=&quot;http://docs.python.org/3.5/license.html&quot; rel=&quot;nofollow&quot;&gt;3.5.x&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/documentation/python&quot;&gt;Stackoverflow Documentation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Popular &lt;a href=&quot;http://wiki.python.org/moin/WebFrameworks&quot; rel=&quot;nofollow&quot;&gt;web frameworks&lt;/a&gt; based on Python&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;If your question has to do with any of these frameworks, please ensure you include the appropriate tag.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.djangoproject.com&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;Django&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/django&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;django&amp;#39;&quot; rel=&quot;tag&quot;&gt;django&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Web framework for perfectionists (with deadlines). Django makes it easier to build better Web apps more quickly and with less code. Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. It lets you build high-performing, elegant Web applications quickly. Django focuses on automating as much as possible and adhering to the DRY (Don't Repeat Yourself) principle.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://flask.pocoo.org&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;Flask&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/flask&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;flask&amp;#39;&quot; rel=&quot;tag&quot;&gt;flask&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Flask is a micro-framework for Python based on Werkzeug, Jinja 2 and good intentions.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.tornadoweb.org/&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;Tornado&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/tornado&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;tornado&amp;#39;&quot; rel=&quot;tag&quot;&gt;tornado&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tornado is a Python web framework and asynchronous networking library. By using non-blocking network I/O, Tornado can scale to tens of thousands of open connections, making it ideal for long polling, WebSockets, and other applications that require a long-lived connection to each user.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.cherrypy.org&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;CherryPy&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/cherrypy&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;cherrypy&amp;#39;&quot; rel=&quot;tag&quot;&gt;cherrypy&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;CherryPy is a pythonic, object-oriented web framework that enables developers to build web applications in much the same way they would build any other object-oriented Python program. This results in smaller source code developed in less time. CherryPy has been in use for over 7 years and it is being used in production by many sites, from the simplest to the most demanding.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.pylonsproject.org/projects/pyramid/about&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;Pyramid&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/pyramid&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;pyramid&amp;#39;&quot; rel=&quot;tag&quot;&gt;pyramid&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A lightweight Web framework emphasizing flexibility and rapid development. It combines the very best ideas from the worlds of Ruby, Python and Perl, providing a structured but extremely flexible Python web framework. It's also one of the first projects to leverage the emerging WSGI standard, which allows extensive re-use and flexibility but only if you need it.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://webpy.org&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;web.py&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/web.py&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;web.py&amp;#39;&quot; rel=&quot;tag&quot;&gt;web.py&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;web.py is a web framework for Python that is as simple as it is powerful. web.py is in the public domain; you can use it for whatever purpose with absolutely no restrictions. web.py lets you write web apps in Python.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://grok.zope.org&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;Grok&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/grok&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;grok&amp;#39;&quot; rel=&quot;tag&quot;&gt;grok&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Built on the existing Zope 3 libraries, but aims to provide an easier learning curve and a more agile development experience. Grok does this by placing an emphasis on convention over configuration and DRY (Don't Repeat Yourself).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://bottlepy.org/docs/dev/index.html&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;Bottle&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/bottle&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;bottle&amp;#39;&quot; rel=&quot;tag&quot;&gt;bottle&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bottle is a fast, simple and lightweight WSGI micro web-framework for Python. It is distributed as a single file module and has no dependencies other than the Python Standard Library.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;Popular Mathematical/Scientific computing libraries in Python&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.numpy.org&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;NumPy&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/numpy&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;numpy&amp;#39;&quot; rel=&quot;tag&quot;&gt;numpy&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;NumPy is the fundamental package for scientific computing with Python. It contains among other things:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a powerful N-dimensional array object&lt;/li&gt;&#xA;&lt;li&gt;sophisticated (broadcasting) functions&lt;/li&gt;&#xA;&lt;li&gt;tools for integrating C/C++ and Fortran code&lt;/li&gt;&#xA;&lt;li&gt;useful linear algebra, Fourier transform, and random number capabilities&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;These features also make it possible to use NumPy in general-purpose database applications.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://scipy.org&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;SciPy&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/scipy&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;scipy&amp;#39;&quot; rel=&quot;tag&quot;&gt;scipy&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SciPy is an open source library for the Python programming language consisting of mathematical algorithms and functions often used in science and engineering. SciPy includes algorithms and tools for tasks such as optimization, clustering, discrete Fourier transforms, linear algebra, signal processing and multi-dimensional image processing. SciPy is closely related to NumPy and depends on many NumPy functions, including a multidimensional array that is used as the basic data structure in SciPy.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://matplotlib.org/&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;matplotlib&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/matplotlib&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;matplotlib&amp;#39;&quot; rel=&quot;tag&quot;&gt;matplotlib&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;matplotlib is a plotting library for the Python programming language and its NumPy numerical mathematics extension. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like wxPython, Qt, or GTK. There is also a procedural &quot;pylab&quot; interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://pandas.pydata.org/&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;pandas&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/pandas&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;pandas&amp;#39;&quot; rel=&quot;tag&quot;&gt;pandas&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;pandas, the Python Data Analysis Library, is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://deeplearning.net/software/theano/&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;theano&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/theano&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;theano&amp;#39;&quot; rel=&quot;tag&quot;&gt;theano&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Theano is a Python-C-based library widely-used library suitable for highly computational mathematical tasks due to the optimizations it does on the interface Python code making it highly optimized using its C-based routines. It is a very popular library for machine-learning researchers as well. It features a highly optimized automatic differentiation, easing the implementations of highly complicated functions and computing their gradients without any errors.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.blender.org/&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;Blender&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/blender&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;blender&amp;#39;&quot; rel=&quot;tag&quot;&gt;blender&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Blender is a free and open source 3D animation suite. It supports the entirety of the 3D pipeline—modeling, rigging, animation, simulation, rendering, compositing and motion tracking, even video editing and game creation.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://scikit-learn.org/stable/&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;scikit-learn&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;/questions/tagged/scikit-learn&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;scikit-learn&amp;#39;&quot; rel=&quot;tag&quot;&gt;scikit-learn&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;scikit-learn is a free and open source machine learning library written in Python. It supports training and testing many different kinds of machine learning models, along with some basic data processing techniques.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Community&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;Chat Rooms&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Chat about Python with other Stack Overflow users in the &lt;a href=&quot;http://chat.stackoverflow.com/rooms/6/python&quot;&gt;Python chat room&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Other Sites&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://python.org/community/lists/#tutor&quot; rel=&quot;nofollow&quot;&gt;Tutor mailing list&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://python.org/community/lists/#python-help&quot; rel=&quot;nofollow&quot;&gt;python-help mailing list&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.pycon.org&quot; rel=&quot;nofollow&quot;&gt;PyCon&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.pythonweekly.com&quot; rel=&quot;nofollow&quot;&gt;Python Weekly&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://pycoders.com&quot; rel=&quot;nofollow&quot;&gt;Pycoder's Weekly&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://groups.google.com/forum/#!forum/comp.lang.python&quot; rel=&quot;nofollow&quot;&gt;Python Google Group&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Free Python programming Books&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://en.wikibooks.org/wiki/Non-Programmer%27s_Tutorial_for_Python_2.6&quot; rel=&quot;nofollow&quot;&gt;Wikibooks' Non-Programmers Tutorial for Python 2.6&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikibooks.org/wiki/Non-Programmer%27s_Tutorial_for_Python_3&quot; rel=&quot;nofollow&quot;&gt;Wikibooks' Non-Programmers Tutorial for Python 3&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://docs.python.org/tutorial&quot; rel=&quot;nofollow&quot;&gt;The Official Python Tutorial&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.itmaybeahack.com/homepage/books/python.html&quot; rel=&quot;nofollow&quot;&gt;Building Skills in Python Version 2.6&lt;/a&gt; (Steven F. Lott)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.swaroopch.org/notes/Python&quot; rel=&quot;nofollow&quot;&gt;A Byte of Python&lt;/a&gt; (Swaroop C H.)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.brpreiss.com/books/opus7/html/book.html&quot; rel=&quot;nofollow&quot;&gt;Data Structures and Algorithms in Python&lt;/a&gt; (Bruno R. Preiss)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://interactivepython.org/runestone/static/pythonds/index.html&quot; rel=&quot;nofollow&quot;&gt;Problem Solving with Algorithms and Data Structures using python&lt;/a&gt; (Brad Miller and David Ranum)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.diveintopython.net&quot; rel=&quot;nofollow&quot;&gt;Dive into Python&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.diveinto.org/python3&quot; rel=&quot;nofollow&quot;&gt;Dive into Python 3&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.greenteapress.com/thinkpython/thinkCSpy&quot; rel=&quot;nofollow&quot;&gt;How to Think Like a Computer Scientist: Learning with Python&lt;/a&gt; (Allen Downey, Jeff Elkner and Chris Meyers)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://inventwithpython.com&quot; rel=&quot;nofollow&quot;&gt;Invent Your Own Computer Games With Python&lt;/a&gt; (Al Sweigart)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://learnpythonthehardway.org&quot; rel=&quot;nofollow&quot;&gt;Learn Python The Hard Way&lt;/a&gt; (Zed A. Shaw)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://inventwithpython.com/pygame&quot; rel=&quot;nofollow&quot;&gt;Making Games with Python &amp;amp; Pygame&lt;/a&gt; (Albert Sweigart)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.nltk.org/book&quot; rel=&quot;nofollow&quot;&gt;Natural Language Processing with Python&lt;/a&gt; (Steven Bird, Ewan Klein, and Edward Loper)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://openbookproject.net/pybiblio&quot; rel=&quot;nofollow&quot;&gt;Python Bibliotheca&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://openbookproject.net/py4fun&quot; rel=&quot;nofollow&quot;&gt;Python for Fun&lt;/a&gt; (Chris Meyers)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.briggs.net.nz/snake-wrangling-for-kids.html&quot; rel=&quot;nofollow&quot;&gt;Snake Wrangling For Kids&lt;/a&gt; (Jason R. Briggs)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.greenteapress.com/thinkpython/thinkpython.pdf&quot; rel=&quot;nofollow&quot;&gt;Think Python (PDF file)&lt;/a&gt; (Allen Downey)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://python3porting.com&quot; rel=&quot;nofollow&quot;&gt;Porting to Python 3&lt;/a&gt; (Lennart Regebro)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Interactive Python learning&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://pythonmonk.com&quot; rel=&quot;nofollow&quot;&gt;Python Monk&lt;/a&gt; - Interactive Python learning in the browser&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.codecademy.com/tracks/python&quot; rel=&quot;nofollow&quot;&gt;Codeacademy&lt;/a&gt; - Learn the fundamentals of Python and dynamic programming&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.codeskulptor.org&quot; rel=&quot;nofollow&quot;&gt;CodeSkulptor&lt;/a&gt; - Interactive online IDEfor Python programming&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.coursera.org/course/interactivepython&quot; rel=&quot;nofollow&quot;&gt;Coursera&lt;/a&gt; - Online course for introduction to interactive Python programming&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.checkio.org&quot; rel=&quot;nofollow&quot;&gt;CheckiO&lt;/a&gt; - Game world you can explore using your Python programming skills&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.repl.it/languages/Python&quot; rel=&quot;nofollow&quot;&gt;Repl.it&lt;/a&gt; - Online interpreter for Python that it allow saving code for later demonstration&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.jetbrains.com/pycharm-edu/&quot; rel=&quot;nofollow&quot;&gt;PyCharm Edu&lt;/a&gt; - Desktop application that offers interactive Python learning.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://interactivepython.org/&quot; rel=&quot;nofollow&quot;&gt;Interactive Python&lt;/a&gt; - Includes a modified, interactive version of How to Think Like a Computer Scientist.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Python Online Courses&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://class.coursera.org/pythonlearn-001/auth/auth_redirector?type=login&amp;amp;subtype=normal&quot; rel=&quot;nofollow&quot;&gt;Programming for Everybody&lt;/a&gt; - Introduction to programming using Python.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://class.coursera.org/interactivepython-004/auth/auth_redirector?type=login&amp;amp;subtype=normal&quot; rel=&quot;nofollow&quot;&gt;An Introduction to Interactive Programming in Python&lt;/a&gt; - The name explains itself.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Python Video Tutorials&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=4Mf0h3HphEA&amp;amp;list=PLEA1FEF17E1E5C0DA&quot; rel=&quot;nofollow&quot;&gt;Programming in Python&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=GE3IHS1wAZI&amp;amp;list=PL_RGaFnxSHWpX_byHyTEj9hecPngl2DqR&quot; rel=&quot;nofollow&quot;&gt;Python for Beginners&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Python for Scientists&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://nbviewer.ipython.org/gist/rpmuller/5920182&quot; rel=&quot;nofollow&quot;&gt;A Crash Course in Python for Scientists&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="5" LastEditorUserId="5" LastEditDate="2016-08-04T02:53:41.727" LastActivityDate="2016-08-04T02:53:41.727" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="270" PostTypeId="4" CreationDate="2016-08-03T23:13:25.643" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-03T23:13:25.643" LastActivityDate="2016-08-03T23:13:25.643" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="271" PostTypeId="2" ParentId="92" CreationDate="2016-08-03T23:25:53.977" Score="11" Body="&lt;p&gt;An important question that does not yet have a satisfactory answer in neural network research is how DNNs come up with the predictions they offer. DNNs effectively work (though not exactly) by matching patches in the images to a &quot;dictionary&quot; of patches, one stored in each neuron (see &lt;a href=&quot;http://research.google.com/archive/unsupervised_icml2012.html&quot;&gt;the youtube cat paper&lt;/a&gt;). Thus, it may not have a high level view of the image since it only looks at patches, and images are usually downscaled to much lower resolution to obtain the results in current generation systems. Methods which look at how the components of the image interact may be able to avoid these problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some questions to ask for this work are: How confident were the networks when they made these predictions? How much volume do such adversarial images occupy in the space of all images?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some work I am aware of in this regard comes from Dhruv Batra and Devi Parikh's Lab at Virginia Tech who look into this for question answering systems: &lt;a href=&quot;http://arxiv.org/pdf/1606.07356v1.pdf&quot;&gt;Analyzing the Behavior of Visual Question Answering Models&lt;/a&gt; and &lt;a href=&quot;https://filebox.ece.vt.edu/~dbatra/papers/gmpb_icmlvis16.pdf&quot;&gt;Interpreting Visual Question Answering models&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;More such work is needed, and just as the human visual system does also get fooled by such &quot;optical illusions&quot;, these problems may be unavoidable if we use DNNs, though AFAIK nothing is yet known either way, theoretically or empirically. &lt;/p&gt;&#xA;" OwnerUserId="130" LastActivityDate="2016-08-03T23:25:53.977" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="272" PostTypeId="2" ParentId="233" CreationDate="2016-08-04T00:57:01.833" Score="3" Body="&lt;p&gt;To have any chance at answering this, you'd first need a rigorous definition of &quot;true artificial intelligence&quot;, which we don't have.  And even if you had that, the best answer would probably be &quot;nobody knows.&quot;  We don't even understand exactly how human intelligence (which is probably the best model of intelligence we have available to study) works.  What we do know (or think we know) is that ANN's are at best a very superficial replica of brain function.  It may turn out that they're absolutely the wrong path to achieving &quot;true artificial intelligence&quot; although I expect most people would be surprised if that turned out to be the case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What probably wouldn't be so surprising would be if some other technique emerged which is better than ANN's, OR if it turns out that you need an ensemble of techniques.  Personally I think it's close to self-evident that the brain works largely in a probabilistic fashion, but it's also clear that we do sometimes use symbolic processing / deductive logic / rules / etc.  And right now, ANN's don't give you much in the way of reasoning, deduction, etc.  So we may ultimately find that we have to combine a probabilistic approach like ANN's with other techniques - maybe Inductive Logic Programming or something of that nature. &lt;/p&gt;&#xA;" OwnerUserId="33" LastActivityDate="2016-08-04T00:57:01.833" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1274" PostTypeId="1" CreationDate="2016-08-04T01:39:20.737" Score="1" ViewCount="153" Body="&lt;p&gt;Have there been any studies which attempted to use AI algorithms to detect human thoughts or emotions based on brain activity, such as using &lt;a href=&quot;https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface#EEG-based&quot; rel=&quot;nofollow&quot;&gt;BCI/EEG devices&lt;/a&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By this, I mean simple guesses such as whether the person was happy or angry, or what object (e.g. banana, car) they were thinking about.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If so, did any of those studies show some degree of success?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="145" LastEditDate="2016-08-17T13:03:50.990" LastActivityDate="2016-08-23T15:25:23.607" Title="Are there any studies which attempt to use AI to guess the human emotion based on the brainwaves?" Tags="&lt;research&gt;&lt;signal-processing&gt;" AnswerCount="2" CommentCount="5" ContentLicense="CC BY-SA 3.0" />
  <row Id="1275" PostTypeId="5" CreationDate="2016-08-04T04:07:42.000" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-04T04:07:42.000" LastActivityDate="2016-08-04T04:07:42.000" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1276" PostTypeId="4" CreationDate="2016-08-04T04:07:42.000" Score="0" Body="For questions about algorithm's that exhibit characteristics of intelligence, or are critical components in systems that exhibit intelligence, and problem-solving and goal-based algorithms in general." OwnerUserId="145" LastEditorUserId="1671" LastEditDate="2018-10-15T17:00:09.787" LastActivityDate="2018-10-15T17:00:09.787" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1277" PostTypeId="5" CreationDate="2016-08-04T04:10:33.107" Score="0" Body="&lt;p&gt;A programming language is a formal language that specifies a set of instructions that can be used to produce various kinds of output. Programming languages generally consist of instructions for a computer. Programming languages can be used to create programs that implement specific algorithms.&lt;/p&gt;&#xA;" OwnerDisplayName="user9947" LastEditorUserId="1671" LastEditDate="2018-03-15T21:41:53.403" LastActivityDate="2018-03-15T21:41:53.403" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1278" PostTypeId="4" CreationDate="2016-08-04T04:10:33.107" Score="0" Body="For questions about the use of various programming languages to be used in AI and ML programming." OwnerDisplayName="user9947" LastEditorDisplayName="user9947" LastEditDate="2018-03-15T21:41:57.557" LastActivityDate="2018-03-15T21:41:57.557" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1279" PostTypeId="5" CreationDate="2016-08-04T04:18:57.577" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-04T04:18:57.577" LastActivityDate="2016-08-04T04:18:57.577" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1280" PostTypeId="4" CreationDate="2016-08-04T04:18:57.577" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-04T04:18:57.577" LastActivityDate="2016-08-04T04:18:57.577" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1281" PostTypeId="5" CreationDate="2016-08-04T04:38:36.787" Score="0" Body="&lt;p&gt;The &lt;strong&gt;Turing test&lt;/strong&gt; was designed in 1950 by Alan Turing, to evaluate a machine's ability to exhibit &lt;a href=&quot;/questions/tagged/natural-language&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;natural-language&amp;#39;&quot; rel=&quot;tag&quot;&gt;natural-language&lt;/a&gt;. He introduced the test in his essay Computing Machinery and Intelligence, which can be read online &lt;a href=&quot;http://www.loebner.net/Prizef/TuringArticle.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The test has a human and an AI have a conversation, which is limited to on-screen text, and then a judge will look over the transcript, and try to see if they can tell who was who in the conversation.&lt;/p&gt;&#xA;" OwnerUserId="145" LastEditorUserId="29" LastEditDate="2016-08-30T19:44:09.247" LastActivityDate="2016-08-30T19:44:09.247" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1282" PostTypeId="4" CreationDate="2016-08-04T04:38:36.787" Score="0" Body="For questions about the Turing Test, a test designed to see whether a third-party person can identify, from a written transcript, who was the AI and who was the human is an human/AI conversation." OwnerUserId="145" LastEditorUserId="29" LastEditDate="2016-08-30T19:44:22.377" LastActivityDate="2016-08-30T19:44:22.377" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1283" PostTypeId="5" CreationDate="2016-08-04T04:41:55.367" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-04T04:41:55.367" LastActivityDate="2016-08-04T04:41:55.367" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1284" PostTypeId="4" CreationDate="2016-08-04T04:41:55.367" Score="0" Body="practice of decomposing a complex spoken sentence it into smaller lexical segments taking the meaning to each segment, and combining those meanings according to language and grammar rules to understand its meaning" OwnerUserId="1256" LastEditorUserId="1256" LastEditDate="2016-08-04T13:18:18.887" LastActivityDate="2016-08-04T13:18:18.887" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1285" PostTypeId="1" AcceptedAnswerId="1315" CreationDate="2016-08-04T05:07:03.323" Score="13" ViewCount="452" Body="&lt;p&gt;Has there been any attempts to deploy AI with blockchain technology? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any decentralized examples of AI networks with no central point of control with AI nodes acting independently (but according to a codified set of rules) creating, validating and storing the same shared decentralized database in many locations around the world?&lt;/p&gt;&#xA;" OwnerUserId="1256" LastEditorUserId="2444" LastEditDate="2021-10-13T01:18:57.967" LastActivityDate="2021-10-13T10:11:30.930" Title="Are there any decentralized examples of AI systems with blockchain technology?" Tags="&lt;reference-request&gt;&lt;multi-agent-systems&gt;&lt;blockchain&gt;" AnswerCount="6" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="1286" PostTypeId="2" ParentId="249" CreationDate="2016-08-04T06:25:20.620" Score="1" Body="&lt;p&gt;I am assuming each image contains a single object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is possible, however, it is not as easy as you might think. Firstly, you need extract as many features as possible: original image, &lt;a href=&quot;https://en.wikipedia.org/wiki/Local_binary_patterns&quot; rel=&quot;nofollow&quot;&gt;LBP&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-invariant_feature_transform&quot; rel=&quot;nofollow&quot;&gt;SIFT&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_moment#Moment_invariants&quot; rel=&quot;nofollow&quot;&gt;moments&lt;/a&gt;, contour descriptors to name a few. Than concatenate these features into a single feature vector. After this step, use clustering. You will need a lot of samples to compensate for the number of features. After clustering, use a correlation method to find which features are related to each cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you need features to classify within a cluster, you could do a second clustering with full set of features and apply the same method. The features that are selected for a cluster will not be suitable to classify within the cluster.&lt;/p&gt;&#xA;" OwnerUserId="210" LastActivityDate="2016-08-04T06:25:20.620" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1288" PostTypeId="1" CreationDate="2016-08-04T07:34:06.557" Score="14" ViewCount="5573" Body="&lt;p&gt;In their famous book entitled &lt;a href=&quot;https://rads.stackoverflow.com/amzn/click/com/0262631113&quot; rel=&quot;noreferrer&quot; rel=&quot;nofollow noreferrer&quot;&gt;Perceptrons: An Introduction to Computational Geometry&lt;/a&gt;, Minsky and Papert show that a perceptron can't solve the XOR problem. This contributed to the first AI winter, resulting in funding cuts for neural networks. However, now we know that a multilayer perceptron can solve the XOR problem easily.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Backprop wasn't known at the time, but did they know about manually building multilayer perceptrons? Did Minsky &amp;amp; Papert know that multilayer perceptrons could solve XOR at the time they wrote the book, albeit not knowing how to train it?&lt;/p&gt;&#xA;" OwnerUserId="144" LastEditorUserId="2444" LastEditDate="2021-01-18T23:34:13.700" LastActivityDate="2023-08-31T13:03:48.640" Title="Did Minsky and Papert know that multi-layer perceptrons could solve XOR?" Tags="&lt;multilayer-perceptrons&gt;&lt;history&gt;&lt;perceptron&gt;&lt;ai-winter&gt;&lt;xor-problem&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1289" PostTypeId="1" CreationDate="2016-08-04T07:41:16.090" Score="5" ViewCount="329" Body="&lt;p&gt;According to Wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_general_intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;Artificial general intelligence(AGI)&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Artificial general intelligence (AGI) is the intelligence of a&#xA;  (hypothetical) machine that could successfully perform any&#xA;  intellectual task that a human being can. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;According to the below image (a screenshot of a picture from &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=251030664B23757C5D420F9ED97AB725?doi=10.1.1.136.7883&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;When will computer hardware match the human brain?&lt;/a&gt; (1998) by Hans Moravec, and Kurzweill also uses this diagram in his book &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Singularity_Is_Near&quot; rel=&quot;nofollow noreferrer&quot;&gt;The Singularity Is Near: When Humans Transcend Biology&lt;/a&gt;), today's artificial intelligence is the same as that of lizards.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/gddKB.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/gddKB.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's assume that within 10-20 years, we, humans, are successful in creating an AGI, that is, an AI with human-level intelligence and emotions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At that point, could we destroy an AGI without its consent? Would this be considered murder?&lt;/p&gt;&#xA;" OwnerUserId="39" LastEditorUserId="2444" LastEditDate="2020-03-10T02:33:58.997" LastActivityDate="2023-04-12T07:41:10.560" Title="Can we destroy an artificial general intelligence without its consent?" Tags="&lt;philosophy&gt;&lt;agi&gt;&lt;ethics&gt;" AnswerCount="4" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="1290" PostTypeId="1" AcceptedAnswerId="2229" CreationDate="2016-08-04T07:43:33.010" Score="12" ViewCount="1015" Body="&lt;p&gt;DeepMind has published a lot of works on deep learning in the last years, most of them are state-of-the-art on their respective tasks. But how much of this work has actually been reproduced by the AI community? For instance, the Neural Turing Machine paper seems to be very hard to reproduce, according to other researchers.&lt;/p&gt;&#xA;" OwnerUserId="144" LastEditorUserId="2444" LastEditDate="2021-06-09T12:25:27.293" LastActivityDate="2021-06-09T12:25:27.293" Title="How much of Deep Mind's work is actually reproducible?" Tags="&lt;neural-networks&gt;&lt;deep-learning&gt;&lt;research&gt;&lt;deepmind&gt;&lt;neural-turing-machine&gt;" AnswerCount="2" CommentCount="3" ContentLicense="CC BY-SA 4.0" />
  <row Id="1292" PostTypeId="2" ParentId="1290" CreationDate="2016-08-04T07:58:33.453" Score="4" Body="&lt;p&gt;&lt;em&gt;I tend to think this question is border-line and may get close. A few comments for now, though.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;wrongx&#xA;There are (at least) two issues with reproducing the work of a company like DeepMind:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Technicalities missing from publications.&lt;/li&gt;&#xA;&lt;li&gt;Access to the same level of data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Technicalities should be workable. Some people have reproduced some of the &lt;a href=&quot;https://github.com/kristjankorjus/Replicating-DeepMind&quot; rel=&quot;nofollow noreferrer&quot;&gt;Atari&lt;/a&gt; gaming stunts. AlphaGo is seemingly more complex and will require more work, yet that should be feasible at some point in the future (individuals may lack computing resources today).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Data can be more tricky. Several companies open their data sets, but data is also the nerve of the competition...&lt;/p&gt;&#xA;" OwnerUserId="169" LastEditorUserId="-1" LastEditDate="2018-04-03T19:47:18.407" LastActivityDate="2018-04-03T19:47:18.407" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="1293" PostTypeId="2" ParentId="1289" CreationDate="2016-08-04T08:06:28.653" Score="6" Body="&lt;p&gt;Firstly, an AGI could conceivably exhibit all of the observable properties of intelligence without being conscious. Although that may seem counter-intuitive, at present we have no physical theory that allows us to detect consciousness (philosophically speaking, a &lt;a href=&quot;http://plato.stanford.edu/entries/zombies/&quot; rel=&quot;nofollow noreferrer&quot;&gt;'Zombie'&lt;/a&gt; is indistinguishable from a non-Zombie - see the writing of &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0316180661&quot; rel=&quot;nofollow noreferrer&quot;&gt;Daniel Dennett&lt;/a&gt; and &lt;a href=&quot;http://consc.net/books/tcm/&quot; rel=&quot;nofollow noreferrer&quot;&gt;David Chalmers&lt;/a&gt; for more on this). Destroying a non-conscious entity has the same moral cost as destroying a chair.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, note that 'destroy' doesn't necessarily mean the same for entities with &lt;em&gt;persistent substrate&lt;/em&gt;, i.e. their 'brain state' can be reversibly serialized to some other storage medium and/or multiple copies of them can co-exist. So if by 'destroy' we simply mean 'switch off', then an AGI might conceivably be reassured of a subsequent re-awakening. Douglas Hofstadter gives an interesting description of such an 'episodic consciousness' in &lt;a href=&quot;http://themindi.blogspot.co.uk/2007/02/chapter-26-conversation-with-einsteins.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&quot;A Conversation with Einstein's Brain&quot;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If by 'destroy', we mean 'irrevocably erase with no chance of re-awakening', then (unless we have a physical test which proves it is &lt;em&gt;not&lt;/em&gt; conscious) destroying an entity with a seemingly human-level awareness is clearly morally tantamount to murder. To believe otherwise would be &lt;em&gt;substrate-ist&lt;/em&gt; - a moral stance which may one day be seen as antiquated as racism.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="3989" LastEditDate="2016-12-03T07:46:15.380" LastActivityDate="2016-12-03T07:46:15.380" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="1294" PostTypeId="1" AcceptedAnswerId="1313" CreationDate="2016-08-04T08:28:06.277" Score="43" ViewCount="21427" Body="&lt;p&gt;Geoffrey Hinton has been researching something he calls &quot;capsules theory&quot; in neural networks. What is it? How do capsule neural networks work?&lt;/p&gt;&#xA;" OwnerUserId="144" LastEditorUserId="2444" LastEditDate="2020-06-09T11:27:33.617" LastActivityDate="2020-06-09T15:25:50.717" Title="How do capsule neural networks work?" Tags="&lt;neural-networks&gt;&lt;capsule-neural-network&gt;" AnswerCount="6" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="1295" PostTypeId="1" AcceptedAnswerId="1299" CreationDate="2016-08-04T08:32:36.610" Score="12" ViewCount="464" Body="&lt;p&gt;During my research, I've stumbled upon &quot;complex-valued neural networks&quot;, which are neural networks that work with complex-valued inputs (probably weights too). What are the advantages (or simply the applications) of this kind of neural network over real-valued neural networks?&lt;/p&gt;&#xA;" OwnerUserId="144" LastActivityDate="2022-07-11T00:33:52.520" Title="What are the advantages of complex-valued neural networks?" Tags="&lt;neural-networks&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1296" PostTypeId="1" CreationDate="2016-08-04T08:39:43.740" Score="18" ViewCount="5648" Body="&lt;p&gt;In &lt;a href=&quot;https://web.archive.org/web/20190519181402/http://fabelier.org/novelty-search-and-open-ended-evolution-by-ken-stanley/&quot; rel=&quot;noreferrer&quot;&gt;this article&lt;/a&gt;, the author claims that guiding evolution by novelty alone (without explicit goals) can solve problems even better than using explicit goals. In other words, using a novelty measure as a fitness function for a genetic algorithm works better than a goal-directed fitness function. How is that possible?&lt;/p&gt;&#xA;" OwnerUserId="144" LastEditorUserId="2444" LastEditDate="2019-11-22T00:48:42.640" LastActivityDate="2019-11-22T00:48:42.640" Title="How does novelty search work?" Tags="&lt;genetic-algorithms&gt;&lt;evolutionary-algorithms&gt;&lt;novelty-search&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1297" PostTypeId="1" AcceptedAnswerId="1327" CreationDate="2016-08-04T09:22:21.360" Score="3" ViewCount="102" Body="&lt;p&gt;Quote from this &lt;a href=&quot;https://ai.meta.stackexchange.com/a/46/8&quot;&gt;Eric's meta post&lt;/a&gt; about modelling and implementation:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;They are not exactly the same, although strongly related. This was a very difficult lesson to learn among mathematicians and early programmers, notably in the 70s (mathematical proofs can demand a lot of non-trivial programming work to make them &amp;quot;computable&amp;quot;, as in runnable on a computer).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;If they're not the same, what is the difference?&lt;/p&gt;&#xA;&lt;p&gt;How we can say when we're talking about AI implementation, and when about modelling? It's suggested above it's not an easy task. So where we can draw the line when we talk about it?&lt;/p&gt;&#xA;&lt;p&gt;I'm asking in general, not specifically for this site, that's why I haven't posted this question in meta&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="18758" LastEditDate="2022-07-11T00:30:07.977" LastActivityDate="2022-07-11T00:30:07.977" Title="How to distinguish AI modeling from implementation?" Tags="&lt;models&gt;&lt;comparison&gt;&lt;implementation&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1298" PostTypeId="2" ParentId="241" CreationDate="2016-08-04T09:52:17.587" Score="4" Body="&lt;p&gt;I might be wrong, but I do not believe that something of the scope you describe would be possible with the current state of technology. It would require a lot of things which are still in relatively early stages of research.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For one, just extracting relevant information from text is a huge task by itself. Doubly so with a novel which contains a large amount of unimportant details.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It might perhaps be easier if the input was presented in the form of some sort of list of important facts. But it would still be rather difficult for the AI to connect them and find a solution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an example, let's say that we have these two facts:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Alice died of a snake bite.&lt;/li&gt;&#xA;&lt;li&gt;Bob was seen buying a couple of mice recently.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;To a human, it seems obvious that the mice were bought to feed a venomous snake. However, it would probably require a tremendous effort to teach an AI to make such connections.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Disclaimer: I don't do text processing myself, so I'm not quite up-to-date on the current state-of-the-art. It's possible that some of these things have already been done in some form. If anyone knows more about the subject, please correct me if I'm wrong.&lt;/p&gt;&#xA;" OwnerUserId="30" LastEditorUserId="30" LastEditDate="2016-08-04T09:59:50.663" LastActivityDate="2016-08-04T09:59:50.663" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="1299" PostTypeId="2" ParentId="1295" CreationDate="2016-08-04T10:28:59.957" Score="10" Body="&lt;p&gt;According to &lt;a href=&quot;http://link.springer.com/chapter/10.1007/3-540-44989-2_118&quot; rel=&quot;nofollow noreferrer&quot;&gt;this paper&lt;/a&gt;, complex-valued ANNs (C-ANNs) can solve problems such as XOR and symmetry detection with a smaller number of layers than real ANNs (for both of these a 2 layer C-ANN suffices, whereas a 3-layer R-ANN is required).&lt;/p&gt;&#xA;&lt;p&gt;I believe that it is still an open question as to how useful this result is in practice (e.g. whether it actually makes finding the right topology easier), so at present, the key practical advantage of C-ANNs is when they are a closer model for the problem domain.&lt;/p&gt;&#xA;&lt;p&gt;Application areas are then where complex values arise naturally, e.g. in optics, signal processing/FFT or electrical engineering.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="18758" LastEditDate="2022-07-11T00:33:52.520" LastActivityDate="2022-07-11T00:33:52.520" CommentCount="5" ContentLicense="CC BY-SA 4.0" />
  <row Id="1300" PostTypeId="2" ParentId="1296" CreationDate="2016-08-04T11:10:36.090" Score="8" Body="&lt;p&gt;As explained in an answer to &lt;a href=&quot;https://ai.stackexchange.com/questions/240/what-exactly-are-genetic-algorithms-and-what-sort-of-problems-are-they-good-for/244#244&quot;&gt;this AI SE question&lt;/a&gt;, GAs are 'satisficers' rather than 'optimizers' and tend not to explore 'outlying' regions of the search space. Rather, the population tends to cluster in regions that are 'fairly good' according to the fitness function.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In contrast, I believe the thinking is that novelty affords a kind of dynamic fitness, tending to push the population away from previously discovered areas.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="-1" LastEditDate="2017-04-13T12:53:10.013" LastActivityDate="2016-08-04T11:10:36.090" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1301" PostTypeId="1" AcceptedAnswerId="1337" CreationDate="2016-08-04T12:10:26.593" Score="1" ViewCount="60" Body="&lt;p&gt;Given pictures with multiple features such as faces, can a single AI algorithm detect all of them, or for better reliability is it preferred to use separate instances?&lt;/p&gt;&#xA;&lt;p&gt;In other words, I'm talking about an attempt of finding all possible human faces in the same picture by a single neural network.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="18758" LastEditDate="2022-07-11T00:24:11.950" LastActivityDate="2022-07-11T00:24:11.950" Title="Do you need single or multiple networks to detect multiple faces?" Tags="&lt;deep-neural-networks&gt;&lt;algorithm&gt;&lt;image-recognition&gt;" AnswerCount="1" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="1303" PostTypeId="1" AcceptedAnswerId="1305" CreationDate="2016-08-04T12:28:07.427" Score="2" ViewCount="3563" Body="&lt;p&gt;I read some information&lt;sup&gt;1&lt;/sup&gt; about attempts to build neural networks in the PHP programming language. Personally I think PHP is not the right language to do so at all probably because it's a high-level language, I assume low level language are way more suitable for AI in terms of performance and scalability. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a good/logical reason why you should or shouldn't use PHP as a language to write AI in?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/em&gt; &lt;a href=&quot;http://www.developer.com/lang/php/creating-neural-networks-in-php.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.developer.com/lang/php/creating-neural-networks-in-php.html&lt;/a&gt; and &lt;a href=&quot;https://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there&quot;&gt;https://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there&lt;/a&gt; &lt;/p&gt;&#xA;" OwnerUserId="217" LastEditorUserId="-1" LastEditDate="2017-05-23T12:39:33.010" LastActivityDate="2016-10-31T09:06:48.290" Title="Can PHP be considered as a serious programming language for AI?" Tags="&lt;neural-networks&gt;&lt;programming-languages&gt;" AnswerCount="2" CommentCount="0" ClosedDate="2016-08-04T14:55:53.457" ContentLicense="CC BY-SA 3.0" />
  <row Id="1305" PostTypeId="2" ParentId="1303" CreationDate="2016-08-04T12:35:10.167" Score="6" Body="&lt;p&gt;&lt;em&gt;Question on-topicness questionable, but...&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;The most logical reason why PHP is unsuited for neural networks is that PHP is, well, intended to be used for server side webpages. It can connect to various external resources, such as databases, via native language features. It is very much a glue language, and not a processing language. PHP is also mostly stateless, only allowing you to store state in either clients, file storage or databases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As such, it's &lt;strong&gt;not&lt;/strong&gt; suitable for this sort of thing - not because PHP is a high level language, but rather because it's so request based and focused towards creating pages to serve to clients.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That won't stop people from trying, though - there are various esoteric programming languages out there in which regular programming would be an insane task or not possible at all - but from a ease of development perspective, making a neural network in PHP makes no sense.&lt;/p&gt;&#xA;" OwnerUserId="74" LastEditorUserId="74" LastEditDate="2016-10-31T09:06:48.290" LastActivityDate="2016-10-31T09:06:48.290" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1306" PostTypeId="1" AcceptedAnswerId="1312" CreationDate="2016-08-04T12:40:22.857" Score="2" ViewCount="199" Body="&lt;p&gt;I've found &lt;a href=&quot;http://link.springer.com/chapter/10.1007%2F978-1-4613-1009-9_2&quot; rel=&quot;nofollow&quot;&gt;this old scientific paper from 1988&lt;/a&gt; about introduction of AI into nuclear power fields.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Were or still are there any dangers by application of such algorithm? Are nuclear power plants or human life in risk if the algorithm will fail?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Especially applications to the core, like cooling systems and other components which can be affected in negative way.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="8" LastEditDate="2016-08-04T14:10:15.490" LastActivityDate="2017-08-05T06:27:25.440" Title="What are the dangers of AI applications for nuclear industry?" Tags="&lt;applications&gt;" AnswerCount="1" CommentCount="5" ContentLicense="CC BY-SA 3.0" />
  <row Id="1307" PostTypeId="2" ParentId="1303" CreationDate="2016-08-04T12:50:23.533" Score="3" Body="&lt;p&gt;Actually, yes. Remember, that due to the history of PHP development, some very good things has formed what we have now:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;From a simple/laggy/limited interpreter in PHP 3, we have now three mainstream lines coming one-by-one like v5/v6/v7 with &lt;em&gt;full bytecode&lt;/em&gt; supported.   &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;In PHP v7 you don't even need a bytecode cache due to HHVM, old Zend VM is a hell-good-debugged and using a cacher like XCache you can achieve a true native execution speed &lt;strong&gt;and&lt;/strong&gt; payload&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The PHP language interface allows &lt;strong&gt;any&lt;/strong&gt; external C/C++ library &lt;em&gt;just to be added&lt;/em&gt; as a module via very simple wrapper that can be written by the person that just red Kerrigan&amp;amp;Richie and Straustrup base books on C and C++. This is amazing feature, exclusive to PHP as far as I know&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;In PHP v7 you're welcome to use &lt;em&gt;native&lt;/em&gt; multi-threading and even CUDA-based things, if you wish to do it. I did it, so I can confirm that it works&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="1263" LastActivityDate="2016-08-04T12:50:23.533" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1308" PostTypeId="1" AcceptedAnswerId="7901" CreationDate="2016-08-04T12:58:01.810" Score="6" ViewCount="98" Body="&lt;p&gt;Since we've self-driving cars already, would we have self-flying commercial flights in the near future? Basically the AI which can do take off, flying, landing and parking.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="8" LastEditDate="2018-09-08T15:09:45.913" LastActivityDate="2018-09-10T03:41:13.010" Title="Is AI capable to replace pilots entirely on the commercial flights?" Tags="&lt;applications&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1310" PostTypeId="2" ParentId="241" CreationDate="2016-08-04T13:40:05.177" Score="2" Body="&lt;p&gt;Generally agree with @Inquisitive Lurker, but I think we also have a wide range of potential abilities/requirements. As with computer chess or Go, where there's a big difference between &quot;beating an honest novice human &quot; and &quot;beating all humans&quot;; there's a big difference between solving a simple kids' mystery and a complex adult novel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I don't think there would be any problem writing a program that could solve a problem that is listed as a list of statements, or laid out as a (very young) children's book. However something like an Agatha Christie or John Le Carre's &quot;Tinker Tailor Soldier Spy&quot; (relatively simple solution, but the story is told in a complex manner) are far in the future.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sometimes an alternative approach might work. For example a neural network could probably solve all Colombo mysteries at the &quot;Who did it?&quot; level without a full &quot;Why?&quot; explanation, after only reading a few Colombo mysteries. The same is true for most kids!&lt;/p&gt;&#xA;" OwnerUserId="132" LastActivityDate="2016-08-04T13:40:05.177" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1311" PostTypeId="2" ParentId="1274" CreationDate="2016-08-04T14:00:13.253" Score="6" Body="&lt;p&gt;As per this &lt;a href=&quot;http://www.dailymail.co.uk/sciencetech/article-2095214/As-scientists-discover-translate-brainwaves-words--Could-machine-read-innermost-thoughts.html&quot; rel=&quot;nofollow&quot;&gt;site&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Researchers recorded the complex patterns of electrical activity generated by someone’s brain, as the subject listened to someone talking.&#xA;  By feeding those brainwave patterns into a computer, they were able to translate them back into actual words — the same words that the volunteer had been hearing.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&lt;strong&gt;The scientists behind the work believe they can now go further and read the unspoken thoughts of people using electrodes placed against the brain.&lt;/strong&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;In the experiment, each patient listened to a recording of spoken words for five to ten minutes, while the net of electrodes placed under their skull monitored activity in a part of the brain involved in understanding speech called &lt;a href=&quot;https://en.wikipedia.org/wiki/Wernicke%27s_area&quot; rel=&quot;nofollow&quot;&gt;Wernicke’s area&lt;/a&gt;.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;In one experiment, volunteers looked at black-and-white photographs while the scanner monitored activity in part of the brain that handles vision called the primary visual cortex.&#xA;  A computer predicted accurately the image that the person was looking at purely from the pattern of brain activity.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So AI might be able to read our emotions as well in near future.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found that &lt;a href=&quot;http://electronics.howstuffworks.com/gadgets/high-tech-gadgets/google-glass-detect-emotions.htm&quot; rel=&quot;nofollow&quot;&gt;google glasses can detect people's emotion&lt;/a&gt; via facial expression, voice tone e.t.c, (just like us), obviously not what they are thinking in their brain.&lt;/p&gt;&#xA;" OwnerUserId="72" LastEditorUserId="145" LastEditDate="2016-08-23T15:25:23.607" LastActivityDate="2016-08-23T15:25:23.607" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1312" PostTypeId="2" ParentId="1306" CreationDate="2016-08-04T14:04:35.743" Score="3" Body="&lt;p&gt;Any technology in the nuclear industry represents variance--it may be an improvement in safety or efficiency, or it may contain some unseen defect that allows a catastrophe to happen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the simple &lt;em&gt;possibility&lt;/em&gt; of harm isn't enough to swing the decision one way or the other. The application of AI methods--whether to the real-time control of plant variables, or the early detection of problems, or to the design of plants and their components--seems likely to be as beneficial as in other realms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, check out the &lt;a href=&quot;http://www.lasar.polimi.it/?page_id=798&quot; rel=&quot;nofollow&quot;&gt;publication list&lt;/a&gt; of a lab active in this area. Their paper I'm most familiar with is one in which they build a fault detector paired with a fault library classifier, so that the operators can be alerted not just that something is abnormal but what fault has probably occurred. This is done in such a way that standardized plants (such as, say, the French nuclear system) can share records with each other, meaning that &lt;em&gt;any&lt;/em&gt; plant has the experience of &lt;em&gt;every&lt;/em&gt; plant at their fingertips.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-04T14:04:35.743" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1313" PostTypeId="2" ParentId="1294" CreationDate="2016-08-04T14:16:57.367" Score="33" Body="&lt;p&gt;It appears to not be published yet; the best available online are &lt;a href=&quot;http://cseweb.ucsd.edu/~gary/cs200/s12/Hinton.pdf&quot; rel=&quot;noreferrer&quot;&gt;these slides&lt;/a&gt; for &lt;a href=&quot;https://www.youtube.com/watch?v=TFIMqt0yT2I&quot; rel=&quot;noreferrer&quot;&gt;this talk&lt;/a&gt;. (Several people reference an earlier talk with &lt;a href=&quot;http://techtv.mit.edu/collections/bcs/videos/30698-what-s-wrong-with-convolutional-nets&quot; rel=&quot;noreferrer&quot;&gt;this link&lt;/a&gt;, but sadly it's broken at time of writing this answer.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My impression is that it's an attempt to formalize and abstract the creation of subnetworks inside a neural network. That is, if you look at a standard neural network, layers are fully connected (that is, every neuron in layer 1 has access to every neuron in layer 0, and is itself accessed by every neuron in layer 2). But this isn't obviously useful; one might instead have, say, &lt;em&gt;n&lt;/em&gt; parallel stacks of layers (the 'capsules') that each specializes on some separate task (which may itself require more than one layer to complete successfully).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I'm imagining its results correctly, this more sophisticated graph topology seems like something that could easily increase both the effectiveness and the interpretability of the resulting network.&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="6050" LastEditDate="2017-03-27T18:15:19.623" LastActivityDate="2017-03-27T18:15:19.623" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1314" PostTypeId="1" AcceptedAnswerId="1316" CreationDate="2016-08-04T14:18:10.547" Score="10" ViewCount="5631" Body="&lt;p&gt;How much processing power is needed to emulate the human brain? More specifically, the neural simulation, such as communication between the neurons and processing certain data in real-time.&lt;/p&gt;&#xA;&lt;p&gt;I understand that this may be a bit of speculation and it's not possible to be accurate, but I'm sure there is some data available or research studies that attempted to estimate it based on our current understanding of the human brain.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-26T01:53:46.537" LastActivityDate="2021-01-26T01:55:25.793" Title="How powerful a computer is required to simulate the human brain?" Tags="&lt;hardware&gt;&lt;neuromorphic-engineering&gt;&lt;neuroscience&gt;&lt;brain&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1315" PostTypeId="2" ParentId="1285" CreationDate="2016-08-04T14:23:37.070" Score="5" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Swarm_intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;Swarm intelligence&lt;/a&gt; is the term for systems where relatively simple agents work together to solve a complicated problem in a decentralized fashion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, distributed computing methods are very important for dealing with problems at scale, and many of them embrace decentralization in a deep way. (Given the reality of hardware failure and the massive size of modern datasets relative to individual nodes, the less work is passed through a central bottleneck, the better.) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;While there are people interested in doing computation on the blockchain, it seems to me like it's unlikely to be competitive with computation in dedicated clusters (like AWS).&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="2444" LastEditDate="2019-10-10T13:22:42.030" LastActivityDate="2019-10-10T13:22:42.030" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="1316" PostTypeId="2" ParentId="1314" CreationDate="2016-08-04T14:35:16.053" Score="11" Body="&lt;p&gt;H+ magazine wrote an estimate &lt;a href=&quot;http://hplusmagazine.com/2009/04/07/brain-chip/&quot; rel=&quot;noreferrer&quot;&gt;in 2009&lt;/a&gt; that seems broadly comparable to other things I've seen; they think the human brain is approximately 37 petaflops. A supercomputer larger than that 37 petaflop estimate &lt;a href=&quot;https://en.wikipedia.org/wiki/Sunway_TaihuLight&quot; rel=&quot;noreferrer&quot;&gt;exists today&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But emulation is hard. See &lt;a href=&quot;https://stackoverflow.com/questions/471973/what-makes-building-game-console-emulators-so-hard&quot;&gt;this SO question about hardware emulation&lt;/a&gt; or &lt;a href=&quot;http://www.tested.com/tech/gaming/2712-why-perfect-hardware-snes-emulation-requires-a-3ghz-cpu/&quot; rel=&quot;noreferrer&quot;&gt;this article&lt;/a&gt; on emulating the SNES, in which they require &lt;strong&gt;140 times&lt;/strong&gt; the processing power of the SNES chip to get it right. &lt;a href=&quot;http://gizmodo.com/an-83-000-processor-supercomputer-only-matched-one-perc-1045026757&quot; rel=&quot;noreferrer&quot;&gt;This 2013 article&lt;/a&gt; claims that a second of human brain activity took 40 minutes to emulate on a 10 petaflop computer (a &lt;em&gt;2400-times&lt;/em&gt; slowdown, not the 4-times slowdown one might naively expect).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And all this assumes that neurons are relatively simple objects! It could be that the amount of math we have to do to model a single neuron is actually much more than the flops estimate above. Or it could be the case that dramatic simplifications can be made, and if we knew what the brain was actually trying to accomplish we could do it much more cleanly and simply. (One advantage that ANNs have, for example, is that they are doing computations with much more precision than we expect biological neurons to have. But this means emulation is &lt;em&gt;harder&lt;/em&gt;, not easier, while replacement &lt;em&gt;is&lt;/em&gt; easier.)&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="-1" LastEditDate="2017-05-23T12:39:33.010" LastActivityDate="2016-08-04T14:35:16.053" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1317" PostTypeId="2" ParentId="1314" CreationDate="2016-08-04T14:36:01.980" Score="2" Body="&lt;p&gt;Not just how much, but what kind of processing power : there're specially-crafted &lt;a href=&quot;http://www.research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml#fbid=V01grppeOIs&quot; rel=&quot;nofollow&quot;&gt;dedicated chips&lt;/a&gt;, and it has a &lt;a href=&quot;http://www.dailymail.co.uk/sciencetech/article-3516047/IBM-reveals-brain-supercomputer-neurosynaptic-chip-replicate-16-million-neurons-work-using-hearing-aid-battery.html&quot; rel=&quot;nofollow&quot;&gt;practical applications&lt;/a&gt;, so it's not a lab-only project&lt;/p&gt;&#xA;" OwnerUserId="1263" LastActivityDate="2016-08-04T14:36:01.980" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1318" PostTypeId="1" AcceptedAnswerId="1820" CreationDate="2016-08-04T15:10:41.230" Score="2" ViewCount="430" Body="&lt;p&gt;&lt;strong&gt;The Situation:&lt;/strong&gt;&#xA;A self-driving car is traveling at it's maximum speed, 25 mph (40 km/h), in the middle of an empty street with the ability to change lanes on both sides. There are two passengers, one in the front and another in the back.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Someone jumps from the side of the road directly into the path of the car. A collision would occur in 50 meters. &lt;a href=&quot;http://www.brake.org.uk/rsw/15-facts-a-resources/facts/1255-speed&quot; rel=&quot;nofollow&quot;&gt;Breaking distance&lt;/a&gt; at this speed is about 24m.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The Question:&lt;/strong&gt; Is it known how the current implementation of the Google Car AI would react, or is it currently a matter of speculation? A step-by-step explanation of the AI's decisioning process would be preferred.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Possible Answers:&lt;/strong&gt; The car could activate its brakes immediately, coming to a halt as quickly as possible. This would be sooner than a human could stop, as people require time to recognize the possibility of a collision, and then physically slam on the brake. (&lt;em&gt;thinking distance&lt;/em&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively, the car could continue traveling forward, processing the situation. (Similar to a humans &lt;em&gt;thinking distance&lt;/em&gt;). The person may continue to move, either out of the way, or still into danger of being hit. In this case, the car may decide to change lanes in an attempt to pass around the person.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly and most unlikely, the car will not alter its course and proceed to drive forward.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;Do not attempt to do it to check;)&lt;/sup&gt;&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="1812" LastEditDate="2016-08-31T23:17:29.040" LastActivityDate="2016-09-01T01:52:21.400" Title="What would happen if someone jumped in the front of a Google car?" Tags="&lt;autonomous-vehicles&gt;&lt;decision-theory&gt;" AnswerCount="1" CommentCount="5" ContentLicense="CC BY-SA 3.0" />
  <row Id="1319" PostTypeId="2" ParentId="70" CreationDate="2016-08-04T15:22:05.297" Score="5" Body="&lt;p&gt;The simple answer is &quot;no, they aren't limited to images&quot;: CNNs are also being used for natural language processing. (See &lt;a href=&quot;http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt; for an introduction.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't seen them applied to graphical data yet, but I haven't looked; there are some obvious things to try and so I'm optimistic that it would work.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-04T15:22:05.297" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1320" PostTypeId="1" AcceptedAnswerId="1324" CreationDate="2016-08-04T15:25:37.293" Score="1" ViewCount="259" Body="&lt;p&gt;Artificial intelligence is present in many games, both current and older games. How can such intelligence understand what to do? I mean, how can it behave like a human in a game, allowing you to play against itself, or that AI plays against itself?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In games like Age of Empires, for example.&lt;/p&gt;&#xA;" OwnerUserId="173" LastEditorUserId="145" LastEditDate="2016-08-09T20:36:48.157" LastActivityDate="2016-08-13T03:12:52.877" Title="How does artificial intelligence work in games?" Tags="&lt;gaming&gt;" AnswerCount="2" CommentCount="0" ClosedDate="2016-08-15T14:22:03.317" ContentLicense="CC BY-SA 3.0" />
  <row Id="1321" PostTypeId="2" ParentId="1320" CreationDate="2016-08-04T15:33:07.120" Score="2" Body="&lt;p&gt;Most of the existing AI bots which can play games use deep search from possible space and choose the best move. This is done by most of the chess, Go, Tic-Tac-Toe, etc bots.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, there has been a recent breakthrough where (deep)neural nets with deep search techniques like monte-carlo search, etc; which might be more human-like and demonstrate a much more complex game behaviour than the above bots. One such example is the Google's Alpha-Go bot.&lt;/p&gt;&#xA;" OwnerUserId="101" LastActivityDate="2016-08-04T15:33:07.120" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="1322" PostTypeId="2" ParentId="240" CreationDate="2016-08-04T15:48:10.133" Score="7" Body="&lt;p&gt;There are a number of good answers here explaining what genetic algorithms are, and giving example applications. I'm adding some general purpose advice on what they are good for, but also cases where you should NOT use them.&lt;/p&gt;&#xA;&lt;p&gt;If my tone seems harsh, it is because using GAs in any of the cases in the &lt;strong&gt;inappropriate&lt;/strong&gt; section below will lead to your paper being &lt;em&gt;instantly&lt;/em&gt; rejected from any top-tier journal.&lt;/p&gt;&#xA;&lt;p&gt;First, your problem MUST be an optimization problem. You need to define a &amp;quot;fitness function&amp;quot; that you are trying to optimize and you need to have a way to measure it.&lt;/p&gt;&#xA;&lt;h3&gt;Good&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Crossover functions are easy to define and natural&lt;/strong&gt;: When dealing with certain kinds of data, crossover/mutation functions might be easy to define. For example strings (eg. DNA or gene sequences) can be mutated easily by splicing two candidate strings to obtain a new one (this is why nature uses genetic algorithms!). Trees (like phylogenetic trees or parse trees) can be spliced too, by replacing a branch of one tree with a branch from another. Shapes (like airplane wings or boat shapes) can be mutated easily by drawing a grid on the shape and combining different grid sections from the parents to obtain a child. Usually this means your problem is composed of different parts and putting together parts from distinct solutions is a valid candidate solution.&lt;/li&gt;&#xA;&lt;li&gt;This means that if your problem is defined in a vector space where the coordinates don't have any special meaning, GAs are not a good choice. If it is hard to formulate your problem as a GA, it is not worth it.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Black Box evaluation&lt;/strong&gt;: If for a candidate, your fitness function is evaluated outside the computer, GAs are a good idea. For example, if you are testing a wing shape in an air tunnel, genetic algorithms will help you generate good candidate shapes to try.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Exception: Simulations&lt;/strong&gt;. If your fitness function is measuring how well a nozzle design performs and requires simulating the fluid dynamics for each nozzle shape, GAs may work well for you. They may also work if you are simulating a physical system through time and are interested in how well your design performs over the course of the operation eg. &lt;a href=&quot;https://www.youtube.com/watch?v=dRthdBr46cs&quot; rel=&quot;nofollow noreferrer&quot;&gt;modelling locomotion patterns&lt;/a&gt;. However, methods that use partial differential equations as constraints are being developed in the literature, eg. &lt;a href=&quot;https://www.siam.org/meetings/op08/Heinkenschloss.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;PDE constrained optimization&lt;/a&gt;, so this may change in the future.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3&gt;Inappropriate&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;You can calculate a gradient&lt;/strong&gt; for your function: If you have access to the gradient of your function, you can do gradient descent, which is in general much more efficient than GAs. Gradient descent may have issues with local minima (as will GAs) but many methods have been studied to mitigate this.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;You know the fitness function in closed form&lt;/strong&gt;: Then, you can probably calculate the gradient. Many languages have libraries supporting &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_differentiation&quot; rel=&quot;nofollow noreferrer&quot;&gt;automatic differentiation&lt;/a&gt;, so you don't even need to do it manually. If your function is not differentiable, then you can use &lt;a href=&quot;https://en.wikipedia.org/wiki/Subgradient_method&quot; rel=&quot;nofollow noreferrer&quot;&gt;subgradient descent&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Your optimization problem is of a known form, like a &lt;strong&gt;linear program or a quadratic program&lt;/strong&gt;: GAs (and black box optimization methods in general) are very inefficient in terms of the number of candidates they need to evaluate, and are best avoided if possible.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Your solution space is small&lt;/strong&gt;: If you can grid your search space efficiently, you can guarantee that you have found the best solution, and can make contour plots of the solution space to see if there is a region you need to explore further.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Finally, if you are considering a GA, consider more recent work in Evolutionary Strategies. I am biased towards &lt;a href=&quot;https://en.wikipedia.org/wiki/CMA-ES&quot; rel=&quot;nofollow noreferrer&quot;&gt;CMA-ES&lt;/a&gt;, which I think is a good simple algorithm that captures the notion of a gradient in the fitness landscape in a way that traditional GAs do not.&lt;/p&gt;&#xA;" OwnerUserId="130" LastEditorUserId="2444" LastEditDate="2021-01-07T00:43:15.233" LastActivityDate="2021-01-07T00:43:15.233" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="1323" PostTypeId="1" AcceptedAnswerId="1325" CreationDate="2016-08-04T15:49:13.793" Score="3" ViewCount="87" Body="&lt;p&gt;&lt;a href=&quot;https://cs.stackexchange.com/a/60535/54605&quot;&gt;At a related question in Computer Science SE&lt;/a&gt;, a user told:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Neural networks typically require a large training set.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Is there a way to define the boundaries of the &amp;quot;optimal&amp;quot; size of a training set in the general case?&lt;/p&gt;&#xA;&lt;p&gt;When I was learning about fuzzy logic, I've heard some rules of thumb that involved examining the mathematical composition of the problem and using that to define the number of fuzzy sets.&lt;/p&gt;&#xA;&lt;p&gt;Is there such a method that can be applicable for an already defined neural network architecture?&lt;/p&gt;&#xA;" OwnerUserId="1270" LastEditorUserId="2444" LastEditDate="2021-12-13T14:40:27.813" LastActivityDate="2021-12-13T14:40:27.813" Title="Is there a way to define the boundaries of the optimal size of a training set?" Tags="&lt;neural-networks&gt;&lt;training&gt;&lt;optimization&gt;&lt;sample-complexity&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1324" PostTypeId="2" ParentId="1320" CreationDate="2016-08-04T16:04:43.973" Score="5" Body="&lt;p&gt;There are many different kinds of AI used in games; AI for historical board games (like chess or Go) tends to be much better than AI for computer games (such as Starcraft or Civilization), in large part because there's more academic interest in developing strategies for those games.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The basic structure of a game-playing AI is that it takes in game state inputs and outputs an action; typically, the internals also contain some sort of goal and some sort of future prediction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But beyond that, there's tremendous amounts of variability. Some AI are little more than scripted reflexes, some are built like control systems, some do actual optimization and forward thinking.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Getting into the details of &lt;em&gt;how&lt;/em&gt; the many different approaches work is probably beyond the scope of this site, though.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-04T16:04:43.973" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1325" PostTypeId="2" ParentId="1323" CreationDate="2016-08-04T16:09:38.327" Score="3" Body="&lt;p&gt;For a finite value to be 'optimal,' typically you need some benefit from more paired up with some cost for more, and eventually the lines cross because the benefit decreases and the cost increases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most models will have a reduction in error with more training data, that asymptotically approaches the best the model can do. See this image (from &lt;a href=&quot;http://blog.revolutionanalytics.com/2015/09/why-big-data-learning-curves.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;) as an example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Jx1AZ.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Jx1AZ.png&quot; alt=&quot;Decreasing error with increasing training set size&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The costs of training data are also somewhat obvious; data is costly to obtain, to store, and to move. (Assuming model complexity stays constant, the actual cost of storing, moving, and using the model remains the same, since the weights in the model are just being tuned.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So at some point the slope of the error-reduction curve becomes horizontal enough that more data points are costlier than they're worth, and that's the optimal amount of training data.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-04T16:09:38.327" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1326" PostTypeId="2" ParentId="1323" CreationDate="2016-08-04T16:15:38.400" Score="1" Body="&lt;p&gt;In general, the larger the training set, the better. See &lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf&quot; rel=&quot;nofollow&quot;&gt;The Unreasonable effectiveness of Data&lt;/a&gt;, though this article is quite dated (written in 2009). Xavier Amatriain, a researcher at Netflix has a &lt;a href=&quot;https://www.quora.com/In-machine-learning-is-more-data-always-better-than-better-algorithms&quot; rel=&quot;nofollow&quot;&gt;Quora answer&lt;/a&gt; where he discusses that more data can sometimes hurt algorithms. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For deep neural networks in particular, it does not seem that we have hit these limits yet. &lt;/p&gt;&#xA;" OwnerUserId="130" LastActivityDate="2016-08-04T16:15:38.400" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1327" PostTypeId="2" ParentId="1297" CreationDate="2016-08-04T16:24:09.050" Score="4" Body="&lt;p&gt;One good way of differentiating modelling and implementation is to consider that models occupy a much higher level of abstraction. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;To continue with the mathematical example: even though experimental mathematics might be dependent on computation, the program can be considered as one possible realization of the necessary conditions of a more abstract existence proof.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Over the last 25 years, software engineering methodologies have become quite good at separating models and implementations, e.g. by using interfaces/typeclasses/abstract base classes to define constraints on behavior that is concretely realized by the implementation of derived classes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;AI has always been a battle between the &lt;a href=&quot;https://en.wikipedia.org/wiki/Neats_vs._scruffies&quot; rel=&quot;nofollow&quot;&gt;'neats and the scruffies'&lt;/a&gt;. Neats tend to prefer working 'top down' from clean abstractions, 'scruffies' like to work 'bottom up', and 'bang the bits' of the implementation together, to see what happens.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, in practice, interplay between both styles is necessary, but AI &lt;em&gt;as a science&lt;/em&gt; progresses when we abstract mechanisms away from specific implementations into their most general (and hence re-useable) form.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-04T16:24:09.050" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1328" PostTypeId="2" ParentId="1314" CreationDate="2016-08-04T17:15:56.423" Score="6" Body="&lt;p&gt;The human brain contains about 100 billion neurons (&lt;span class=&quot;math-container&quot;&gt;$10^{11}$&lt;/span&gt;) and about a hundred trillion synapses (&lt;span class=&quot;math-container&quot;&gt;$10^{14}$&lt;/span&gt;). Each neuron can fire about 100 times a second. If we model the brain as a simple neural network, then it would be equivalent to a machine that requires 1016 calculations per second and 1013 bits of memory.&lt;/p&gt;&#xA;&lt;p&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Singularity_Is_Near#The_brain&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Kurzweil introduces the idea of &amp;quot;uploading&amp;quot; a specific brain with every mental process intact, to be instantiated on a &amp;quot;suitably powerful computational substrate&amp;quot;. He writes that general modeling requires 1016 calculations per second and 1013 bits of memory, but then explains uploading requires additional detail, perhaps as many as 1019 cps and 1018 bits. Kurzweil says the technology to do this will be available by 2040.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;According to this site &lt;a href=&quot;http://www.extremetech.com/extreme/163051-simulating-1-second-of-human-brain-activity-takes-82944-processors&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Using the NEST software framework, the team led by Markus Diesmann and Abigail Morrison succeeded in creating an artificial neural network of 1.73 billion nerve cells connected by 10.4 trillion synapses. While impressive, this is only a fraction of the neurons every human brain contains. Scientists believe we all carry 80-100 billion nerve cells&lt;/p&gt;&#xA;&lt;p&gt;It took 40 minutes with the combined muscle of 82,944 processors in K computer to get just 1 second of biological brain processing time. While running, the simulation ate up about 1PB of system memory as each synapse was modeled individually.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Computing power will continue to ramp up while transistors scale down, which could make true neural simulations possible in real-time with supercomputers.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SpiNNaker&quot; rel=&quot;nofollow noreferrer&quot;&gt;SpiNNaker&lt;/a&gt; is a manycore computer architecture designed to &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_Brain_Project&quot; rel=&quot;nofollow noreferrer&quot;&gt;simulate the human brain&lt;/a&gt;. It is planned to use 1 million ARM processors (currently .5 million). The completed design will hold 100,000 cores&lt;/p&gt;&#xA;&lt;p&gt;In this &lt;a href=&quot;https://www.youtube.com/watch?v=2e06C-yUwlc&quot; rel=&quot;nofollow noreferrer&quot;&gt;video&lt;/a&gt;, they showed a completed rack with 100,000 cores emulating 25 million neurons (at &lt;span class=&quot;math-container&quot;&gt;$\frac{1}{4}$&lt;/span&gt; the efficiency—it will eventually run 1,000 neurons per core).&lt;/p&gt;&#xA;" OwnerUserId="72" LastEditorUserId="2444" LastEditDate="2021-01-26T01:55:25.793" LastActivityDate="2021-01-26T01:55:25.793" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="1329" PostTypeId="2" ParentId="64" CreationDate="2016-08-04T17:34:17.233" Score="4" Body="&lt;p&gt;What might be classed as AI has of course changed over the years, but landmarks and research breakthroughs include:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Babbagge's &lt;a href=&quot;https://en.wikipedia.org/wiki/Difference_engine&quot; rel=&quot;nofollow&quot;&gt;Difference Engine&lt;/a&gt; (~1823) for tabulating/interpolating polynomials.&lt;/li&gt;&#xA;&lt;li&gt;Frank Rosenblatt's 1957 invention of the &lt;a href=&quot;http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf&quot; rel=&quot;nofollow&quot;&gt;Perceptron&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;John McCarthy's invention of &lt;a href=&quot;https://en.wikipedia.org/wiki/Lisp_(programming_language)&quot; rel=&quot;nofollow&quot;&gt;Lisp&lt;/a&gt; in the late 1950s.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Arthur Samuel's 1959 &lt;a href=&quot;http://link.springer.com/chapter/10.1007/978-3-319-30668-1_9&quot; rel=&quot;nofollow&quot;&gt;checkers player&lt;/a&gt;, which famously improved by playing against itself (it would have been nice if that had destroyed the myth about a program only being as 'intelligent' as its creator).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Newell and Simon's 1959 &lt;a href=&quot;http://bitsavers.informatik.uni-stuttgart.de/pdf/rand/ipl/P-1584_Report_On_A_General_Problem-Solving_Program_Feb59.pdf&quot; rel=&quot;nofollow&quot;&gt;General Problem Solver&lt;/a&gt; applied Means-Ends analysis to solve a range of problems expressed as Horn clauses.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Davis, Putnam et al: 1962 invention of the &lt;a href=&quot;https://en.wikipedia.org/wiki/DPLL_algorithm&quot; rel=&quot;nofollow&quot;&gt;DPLL algorithm&lt;/a&gt; which still forms the core of modern SAT-based theorem provers.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Lawrence Fogel et al: 1966 book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/B0000CNARU&quot; rel=&quot;nofollow&quot;&gt;Artificial Intelligence through Simulated Evolution&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Rechenberg and Schwefel: 1960s development of &lt;a href=&quot;https://en.wikipedia.org/wiki/Evolution_strategy&quot; rel=&quot;nofollow&quot;&gt;Evolutionsstrategie&lt;/a&gt; - an Evolutionary Computation approach using mutation and a form of Darwinian 'survival of the fittest'.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Lotfi Zadeh's 1965 invention of &lt;a href=&quot;https://people.eecs.berkeley.edu/~zadeh/papers/Fuzzy%20Sets-Information%20and%20Control-1965.pdf&quot; rel=&quot;nofollow&quot;&gt;Fuzzy Logic&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;John Holland's 1975 book &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=531075&quot; rel=&quot;nofollow&quot;&gt;&quot;Adaptation in Natural and Artificial Systems&quot;&lt;/a&gt; which introduced Genetic Algorithms.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The 1980 &lt;a href=&quot;http://aitopics.org/sites/default/files/classic/Webber-Nilsson-Readings/Rdgs-NW-Erman-Hayes-Roth-Lesser-Reddy.pdf&quot; rel=&quot;nofollow&quot;&gt;Hearsay II&lt;/a&gt; Blackboard Architecture by Hayes-Roth et al.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The 1980s invention of the &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=104293&quot; rel=&quot;nofollow&quot;&gt;backpropagation algorithm&lt;/a&gt; for Mutlilayer Perceptrons by Rumelhart, Hinton et al.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-04T17:34:17.233" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1330" PostTypeId="5" CreationDate="2016-08-04T18:08:05.010" Score="0" Body="&lt;p&gt;See &lt;a href=&quot;http://www.scholarpedia.org/article/Artificial_General_Intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.scholarpedia.org/article/Artificial_General_Intelligence&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_general_intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://en.wikipedia.org/wiki/Artificial_general_intelligence&lt;/a&gt; for more info.&lt;/p&gt;&#xA;" OwnerUserId="-1" LastEditorUserId="2444" LastEditDate="2020-04-13T16:58:01.280" LastActivityDate="2020-04-13T16:58:01.280" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1331" PostTypeId="4" CreationDate="2016-08-04T18:08:05.010" Score="0" Body="For questions about Artificial General Intelligence (AGI), a hypothetical machine characteristic permitting it to learn any arbitrary intellectual ability up to the limits of the available computing resources." OwnerUserId="8" LastEditorUserId="4302" LastEditDate="2018-10-15T16:56:48.617" LastActivityDate="2018-10-15T16:56:48.617" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1332" PostTypeId="1" CreationDate="2016-08-04T18:26:31.213" Score="6" ViewCount="289" Body="&lt;p&gt;How important is true (non-&lt;a href=&quot;https://en.wikipedia.org/wiki/Pseudorandomness&quot; rel=&quot;noreferrer&quot; title=&quot;pseudo&quot;&gt;pseudo&lt;/a&gt;) randomness in Artificial Intelligence designs? Is there any chance that pseudo-randomness could be a barrier to more successful designs?&lt;/p&gt;&#xA;" OwnerUserId="46" LastActivityDate="2016-08-04T18:56:47.850" Title="How important is true randomness in AI designs?" Tags="&lt;ai-design&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1333" PostTypeId="1" AcceptedAnswerId="1370" CreationDate="2016-08-04T18:31:33.580" Score="0" ViewCount="32" Body="&lt;p&gt;Complex AI that learns lexical-semantic content and its meaning (such as collection of words, their structure and dependencies) such as &lt;em&gt;Watson&lt;/em&gt; takes terabytes of disk space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lets assume &lt;em&gt;DeepQA&lt;/em&gt;-like AI consumed whole Wikipedia of size 10G which took the same amount of structured and unstructured stored content.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Will learning another 10G of different encyclopedia (different topics in the same language) take the same amount of data? Or will the AI reuse the existing structured and take less than half (like 1/10 of it) additional space?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="75" LastEditDate="2016-08-17T13:52:57.853" LastActivityDate="2016-08-18T00:46:03.973" Title="Does learning content from additional encyclopedias consume much less amount of storage?" Tags="&lt;watson&gt;&lt;storage&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1334" PostTypeId="1" AcceptedAnswerId="1342" CreationDate="2016-08-04T18:40:44.093" Score="3" ViewCount="167" Body="&lt;p&gt;Is there any simple explanation how &lt;em&gt;Watson&lt;/em&gt; finds and scores evidence after gathering massive evidence and analyzing the data?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words, how does it know which precise answer it needs to return?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="75" LastEditDate="2016-08-17T13:52:23.383" LastActivityDate="2019-07-03T03:21:53.060" Title="How does Watson find and evaluate its evidence to the answer?" Tags="&lt;watson&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1335" PostTypeId="2" ParentId="1332" CreationDate="2016-08-04T18:56:47.850" Score="5" Body="&lt;p&gt;Randomness is typically the best one can do with ignorance, rather than a source of strength in its own right.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, the primary use of randomness in statistics is random assignment (A/B testing, randomized controlled trials, etc.). The reason to do this is to make the influence of confounders independent from the influence of the factor under investigation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But randomness only works for this &lt;em&gt;in expectation&lt;/em&gt;. If we actually knew what the confounders were, we could do a paired assignment (or a similar scheme) that ensured the various groups were matched &lt;em&gt;as well as possible&lt;/em&gt;, instead of us just not knowing ahead of time which way the bias went.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;There are some cases where pseudorandomness, rather than full randomness, will impair training AI designs. A simple example would be a case where you want to randomly initialize weights in a network where the number of parameters exceeds the periodicity of the RNG; this means that while you have as many possible networks as there are possible unique seeds, you can't actually visit the entire weight space that you wanted to sample over.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't think any of those cases are limiting factors, however. Having truly random stochastic gradient descent instead of pseudorandom stochastic gradient descent doesn't seem like it would make a serious difference in the trajectory of AI designs.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-04T18:56:47.850" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="1337" PostTypeId="2" ParentId="1301" CreationDate="2016-08-04T19:36:06.863" Score="3" Body="&lt;p&gt;AFAIK, normally detection algorithms work in a sub-window of the image and not the whole of it. For example, for a specific size and orientation you slide a sub-window on the image and extract sub-images. Then you apply your algorithm on every sub-image for detection and report the size-and-orientations with positive results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can have a single neural network for face detection in this case or you might want to have different detectors for different orientation or any other feature, that is your decision.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is also the technique of &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensembles_of_classifiers&quot; rel=&quot;nofollow&quot;&gt;Combining Classifiers&lt;/a&gt; by which you can improve the decision of single classifiers by combining them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_learning&quot; rel=&quot;nofollow&quot;&gt;Ensemble Learning&lt;/a&gt; is another way in which your classifiers are not trained independently but rather together. In fact, the well-known &lt;a href=&quot;https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework&quot; rel=&quot;nofollow&quot;&gt;object detector of Viola and Jones&lt;/a&gt; uses such a technique.&lt;/p&gt;&#xA;" OwnerUserId="143" LastEditorUserId="143" LastEditDate="2016-08-04T19:56:38.653" LastActivityDate="2016-08-04T19:56:38.653" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1338" PostTypeId="2" ParentId="36" CreationDate="2016-08-04T20:12:38.983" Score="9" Body="&lt;p&gt;Until we can make a quantum computer with a lot more qubits, the potential to further develop AI will remain just that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;D-Wave (which has just made a 2,000+ qubit system around 2015) is an &lt;em&gt;adiabatic quantum computer&lt;/em&gt;, not a general-purpose quantum computer. It is restricted to certain optimization problems (at which its effectiveness &lt;a href=&quot;https://en.wikipedia.org/wiki/D-Wave_Systems#Reception&quot; rel=&quot;nofollow noreferrer&quot;&gt;has reportedly been doubted&lt;/a&gt; by one of the originators of the theory on which it is based).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose that we could build a 32 qubit general-purpose quantum computer (twice as big as current models, as far as I'm aware). This would still mean that only 2&lt;sup&gt;32&lt;/sup&gt; possibilities exist in superposition. This is a space small enough to be explored exhaustively for many problems. Hence, there are perhaps not so many problems for which any of the known quantum algorithms (e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Shor%27s_algorithm&quot; rel=&quot;nofollow noreferrer&quot;&gt;Shor&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Grover%27s_algorithm&quot; rel=&quot;nofollow noreferrer&quot;&gt;Grover&lt;/a&gt;) would be useful for that number of bits.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="14723" LastEditDate="2018-04-12T02:34:20.583" LastActivityDate="2018-04-12T02:34:20.583" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="1339" PostTypeId="2" ParentId="1274" CreationDate="2016-08-04T20:15:37.607" Score="5" Body="&lt;p&gt;There has been previous research with promising results cited at length in the following recent article, and although they have limited training data, here is some &lt;a href=&quot;http://uaf46365.ddns.uark.edu/SarahStolze_Thesis.pdf&quot; rel=&quot;noreferrer&quot;&gt;impressive research for an undergraduate thesis at the University of Arkansas&lt;/a&gt; which extends that research using an artificial neural network on enhancing a classifying algorithm's capacity to facilitate unspoken, or imagined, speech recognition by collecting and analyzing a large dataset of simultaneous EEG signal and video data streams. &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Imagined speech (unspoken speech, silent speech, or covert speech) is&#xA;  the process by which one thinks about a word, or “hears” the word in&#xA;  one’s head, in the absence of any vocalization or physical movement&#xA;  indicating the word. Though there exists evidence that it is possible&#xA;  for imagined speech information to be captured and interpreted. To&#xA;  facilitate imagined speech, a Brain-to-Computer Interface (BCI) must&#xA;  be implemented to provide silent communication abilities directly&#xA;  between the two entities. One of the most popular methods for&#xA;  interfacing directly between a human brain and a computer is through&#xA;  electroencephalographic signals.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Researchers have created models capable of achieving 70 - 90%&#xA;  predictive accuracy in recognizing patterns in EEG data;&#xA;  however, the accuracy of current methods for unspoken speech&#xA;  recognition is not yet sufficient to enable fluid communication&#xA;  between humans and machines.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;High Level Experiment Design&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;the subjects were asked to imagine a specific word or feeling (label).&#xA;  The subjects responded to a set of uniform verbal cues describing the&#xA;  set of labels as well as the desired individual label to imagine. The&#xA;  data was then processed in order to minimize the effects of irrelevant&#xA;  signal activity, or noise. Additionally the data was processed to&#xA;  minimize its volume while still maintaining the core “information” in&#xA;  the data. The condensed dataset was created by dropping irrelevant&#xA;  information from the EEG device and applying principal component&#xA;  analysis (PCA) to the video stream data. Once the data was processed&#xA;  and assembled into the correct format, cross-validation using a random&#xA;  forest algorithm was performed on the control group of EEG signals&#xA;  alone and on the hypothesis group consisting of both EEG and video&#xA;  data. The predictive accuracy measurements obtained from the&#xA;  cross-validation experiments were used as metrics to evaluate the&#xA;  success of the hypothesis.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The results show a notable improvement classifying thoughts when in conjunction with the video streams.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/LH6sI.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/LH6sI.png&quot; alt=&quot;graph of predictive accuracy&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="62" LastActivityDate="2016-08-04T20:15:37.607" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1340" PostTypeId="2" ParentId="156" CreationDate="2016-08-04T20:48:35.637" Score="5" Body="&lt;p&gt;&lt;a href=&quot;http://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(04)00243-8?_returnURL=http%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1364661304002438%3Fshowall%3Dtrue&quot; rel=&quot;noreferrer&quot;&gt;This article&lt;/a&gt; gives a description of mirror neurons in terms of Hebbian learning, a mechanism that has been widely used in AI. I don't know whether the formulation given in the article has ever actually been implemented computationally.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-04T20:48:35.637" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1341" PostTypeId="2" ParentId="247" CreationDate="2016-08-04T21:05:44.587" Score="3" Body="&lt;p&gt;Multilayer Perceptron (MLP) can theoretically approximate any bounded, continuous function. There's no guarantee for a discontinuous function. There are plenty of important discontinuous functions, like, say, the prime counting function.&lt;/p&gt;&#xA;&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Prime-counting_function&quot; rel=&quot;nofollow noreferrer&quot;&gt;prime counting function&lt;/a&gt; &lt;span class=&quot;math-container&quot;&gt;$\pi(n)$&lt;/span&gt; is simply equal to the number of primes less than or equal to &lt;span class=&quot;math-container&quot;&gt;$n$&lt;/span&gt;. It has a discontinuity about each prime &lt;span class=&quot;math-container&quot;&gt;$p$&lt;/span&gt;, so good luck trying to approximate this with a neural network!&lt;/p&gt;&#xA;&lt;p&gt;However, this function is extensively studied and extremely important in number theory. See the &lt;a href=&quot;https://en.wikipedia.org/wiki/Riemann_hypothesis&quot; rel=&quot;nofollow noreferrer&quot;&gt;Riemann hypothesis&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="127" LastEditorUserId="2444" LastEditDate="2021-01-22T14:34:52.270" LastActivityDate="2021-01-22T14:34:52.270" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1342" PostTypeId="2" ParentId="1334" CreationDate="2016-08-04T21:40:31.970" Score="4" Body="&lt;p&gt;Watson starts off by searching its massive database of sources for stuff that might be pertinent to the question. Next, it searches through all of the search results and turns them into candidate answers. For example, if one of the search results is an article, Watson might pick the title of the article as a possible answer. After finding all of these candidate answers, it proceeds to iteratively score them to determine which one is best.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The scoring process is very complicated, and involves finding supporting evidence for each answer, and then combining many different scoring algorithms to determine which candidate answer is the best. You can read a more detailed (but still very conceptual) overview &lt;a href=&quot;http://www.aaai.org/Magazine/Watson/watson.php&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, by the creators of Watson.&lt;/p&gt;&#xA;" OwnerUserId="127" LastActivityDate="2016-08-04T21:40:31.970" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1343" PostTypeId="5" CreationDate="2016-08-04T21:41:23.037" Score="0" Body="&lt;p&gt;Machine learning revolves around developing self-learning computer algorithms that function by virtue of discovering patterns in data and making intelligent decisions based on such patterns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Machine learning explores the construction and study of algorithms that can learn from and make predictions about data. Such algorithms operate by building a model from the input, in order to make data-driven predictions or decisions, rather than following strictly static program instructions.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Related tags&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;/questions/tagged/deep-learning&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;deep-learning&amp;#39;&quot; rel=&quot;tag&quot;&gt;deep-learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;/questions/tagged/reinforcement-learning&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;reinforcement-learning&amp;#39;&quot; rel=&quot;tag&quot;&gt;reinforcement-learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;/questions/tagged/q-learning&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;q-learning&amp;#39;&quot; rel=&quot;tag&quot;&gt;q-learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;/questions/tagged/unsupervised-learning&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;unsupervised-learning&amp;#39;&quot; rel=&quot;tag&quot;&gt;unsupervised-learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;/questions/tagged/supervised-learning&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;supervised-learning&amp;#39;&quot; rel=&quot;tag&quot;&gt;supervised-learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;/questions/tagged/meta-learning&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;meta-learning&amp;#39;&quot; rel=&quot;tag&quot;&gt;meta-learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;/questions/tagged/incremental-learning&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;incremental-learning&amp;#39;&quot; rel=&quot;tag&quot;&gt;incremental-learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;Journals&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://jmlr.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Journal of Machine Learning Research&lt;/a&gt; (Open Access)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.springer.com/computer/ai/journal/10994&quot; rel=&quot;nofollow noreferrer&quot;&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.springer.com/engineering/computational+intelligence+and+complexity/journal/13042&quot; rel=&quot;nofollow noreferrer&quot;&gt;International Journal of Machine Learning and Cybernetics&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.ijmla.net/index.php/ijmla&quot; rel=&quot;nofollow noreferrer&quot;&gt;International Journal of Machine Learning and Applications&lt;/a&gt; (Open Access)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.ijmlc.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;International Journal of Machine Learning and Computing&lt;/a&gt; (Open Access)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;References&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OGxgnH8y2NM&quot; rel=&quot;nofollow noreferrer&quot;&gt;Machine Learning with Python&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2255" LastEditorUserId="2255" LastEditDate="2020-05-18T22:15:57.890" LastActivityDate="2020-05-18T22:15:57.890" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1344" PostTypeId="4" CreationDate="2016-08-04T21:41:23.037" Score="0" Body="For questions related to machine learning (ML), which is a set of methods that can automatically detect patterns in data, and then use the uncovered patterns to predict future data, or to perform other kinds of decision making under uncertainty (such as planning how to collect more data). ML is usually divided into supervised, unsupervised and reinforcement learning. Deep learning is a subfield of ML that uses deep artificial neural networks." OwnerUserId="3836" LastEditorUserId="2444" LastEditDate="2019-08-13T20:25:51.020" LastActivityDate="2019-08-13T20:25:51.020" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1345" PostTypeId="2" ParentId="198" CreationDate="2016-08-04T21:45:48.653" Score="5" Body="&lt;p&gt;The following survey article by researchers from IIT Bombay summarizes recent advances in sarcasm detection: &lt;a href=&quot;https://arxiv.org/abs/1602.03426&quot; rel=&quot;noreferrer&quot;&gt;Arxiv link&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In reference to your question, I do not think it is considered either extraordinarily difficult or open-ended. While it does introduce ambiguity that computers cannot yet handle, Humans are easily able to understand sarcasm, and are thus able to label datasets for sarcasm detection.&lt;/p&gt;&#xA;" OwnerUserId="130" LastActivityDate="2016-08-04T21:45:48.653" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1346" PostTypeId="5" CreationDate="2016-08-04T21:55:57.107" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-04T21:55:57.107" LastActivityDate="2016-08-04T21:55:57.107" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1347" PostTypeId="4" CreationDate="2016-08-04T21:55:57.107" Score="0" Body="For questions about natural language as it relates to AI but is not a processing question, which should be tagged natural-language-processing instead. This includes questions about the relationship between AI and linguistics, semantics, or language development." OwnerUserId="145" LastEditorUserId="4302" LastEditDate="2018-10-09T18:06:11.547" LastActivityDate="2018-10-09T18:06:11.547" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1348" PostTypeId="1" AcceptedAnswerId="1355" CreationDate="2016-08-04T23:49:01.983" Score="20" ViewCount="681" Body="&lt;p&gt;Isaac Asimov's famous &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Laws_of_Robotics&quot;&gt;Three Laws of Robotics&lt;/a&gt; originated in the context of Asimov's science fiction stories. In those stories, the three laws serve as a safety measure, in order to avoid untimely or manipulated situations from exploding in havoc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More often than not, Asimov's narratives would find a way to break them, leading the writer to make several modifications to the laws themselves. For instance, in some of his stories, he &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#First_Law_modified&quot;&gt;modified the First Law&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Zeroth_Law_added&quot;&gt;added a Fourth (or Zeroth) Law&lt;/a&gt;, or even &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Removal_of_the_Three_Laws&quot;&gt;removed all Laws altogether&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, it is easy to argue that, in popular culture, and even in the field of AI research itself, the Laws of Robotics are taken quite seriously. Ignoring the side problem of the different, subjective, and mutually-exclusive interpretations of the laws, are there any arguments proving the laws themselves intrinsically flawed by their design, or, alternatively, strong enough for use in reality? Likewise, has a better, stricter security heuristics set being designed for the purpose?&lt;/p&gt;&#xA;" OwnerUserId="71" LastEditorUserId="2444" LastEditDate="2020-12-04T23:12:50.163" LastActivityDate="2020-12-04T23:12:50.163" Title="Are Asimov's Laws flawed by design, or are they feasible in practice?" Tags="&lt;ai-safety&gt;&lt;asimovs-laws&gt;" AnswerCount="3" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="1352" PostTypeId="5" CreationDate="2016-08-05T01:17:49.000" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-05T01:17:49.000" LastActivityDate="2016-08-05T01:17:49.000" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1353" PostTypeId="4" CreationDate="2016-08-05T01:17:49.000" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-05T01:17:49.000" LastActivityDate="2016-08-05T01:17:49.000" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1354" PostTypeId="1" AcceptedAnswerId="2143" CreationDate="2016-08-05T01:45:17.633" Score="10" ViewCount="599" Body="&lt;p&gt;Are there any modern techniques of generating &lt;strong&gt;textual&lt;/strong&gt; CAPTCHA (so person needs to type the right text) challenges which can easily &lt;a href=&quot;https://ai.stackexchange.com/q/92/8&quot;&gt;fool AI&lt;/a&gt; with some visual obfuscation methods, but at the same time human can solve them without any struggle?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example I'm talking about plain ability of &lt;strong&gt;recognising text embedded into image&lt;/strong&gt; (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestions or research has been done?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="-1" LastEditDate="2017-04-13T12:53:10.013" LastActivityDate="2016-10-13T09:33:07.423" Title="Are there any textual CAPTCHA challenges which can fool AI, but not human?" Tags="&lt;image-recognition&gt;&lt;research&gt;&lt;optical-character-recognition&gt;" AnswerCount="3" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="1355" PostTypeId="2" ParentId="1348" CreationDate="2016-08-05T01:55:51.737" Score="12" Body="&lt;p&gt;Asimov's laws are not strong enough to be used in practice. Strength isn't even a consideration, when considering that since they're written in English words would first have to be interpreted subjectively to have any meaning at all. You can find a good discussion of this &lt;a href=&quot;https://youtu.be/7PKx3kS7f4A&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To transcribe an excerpt:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;How do you define these things? How do you define &quot;human&quot;, without first having to take a stand on almost every issue. And if &quot;human&quot; wasn't hard enough, you then have to define &quot;harm&quot;, and you've got the same problem again. Almost any really solid unambiguous definitions you give for those words&amp;mdash;that don't rely on human intuition&amp;mdash;result in weird quirks of philosophy, leading to your AI doing something you really don't want it to do.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;One can easily imagine that Asimov was smart enough to know this and was more interested in story-writing than designing real-world AI control protocols.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the novel &lt;a href=&quot;https://en.wikipedia.org/wiki/Neuromancer#Plot_summary&quot;&gt;Neuromancer&lt;/a&gt;, it was suggested that AIs could possibly serve as checks against each other. Ray Kurzweil's impending &lt;a href=&quot;https://en.wikipedia.org/wiki/Technological_singularity&quot;&gt;Singularity&lt;/a&gt;, or the possibility of hyperintelligent AGIs otherwise, might not leave much of a possibility for humans to control AIs at all, leaving peer-regulation as the only feasible possibility.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's worth noting that Eliezer Yudkowsky and others ran an &lt;a href=&quot;http://www.yudkowsky.net/singularity/aibox/&quot;&gt;experiment&lt;/a&gt; wherein Yudkowsky played the role of a superintelligent AI with the ability to speak, but no other connection outside of a locked box. The challengers were tasked simply with keeping the AI in the box at all costs. Yudkowsky escaped both times.&lt;/p&gt;&#xA;" OwnerUserId="46" LastEditorUserId="46" LastEditDate="2016-08-05T02:00:59.683" LastActivityDate="2016-08-05T02:00:59.683" CommentCount="4" ContentLicense="CC BY-SA 3.0" />
  <row Id="1356" PostTypeId="2" ParentId="198" CreationDate="2016-08-05T04:52:42.947" Score="2" Body="&lt;p&gt;There has been a recent work in the same domain where neural networks(CNNs to be accurate) are used for the same purpose. Some info. about the research is:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;To learn that context, the paper describes a method by which the&#xA;  neural network finds the user’s “embeddings” — i.e. contextual cues&#xA;  like the content of previous tweets, related interests and accounts,&#xA;  and so on. It uses these various factors to plot the user with others,&#xA;  and (ideally) finds that they form relatively well-defined groups.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So, the paper uses CNNs, word and user embeddings for detecting sarcasm in text. There is also a &lt;a href=&quot;https://techcrunch.com/2016/08/04/this-neural-network-tries-to-tell-if-youre-being-sarcastic-online/&quot; rel=&quot;nofollow&quot;&gt;Techcrunch article&lt;/a&gt; on that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The paper uses sentiment of the tweet and compares with that of the other similar tweets:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;If the sentiment of the tweet seems to disagree with the bulk of what&#xA;  is expressed by similar users, there’s a good chance sarcasm is being&#xA;  employed.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/1607.00976v2.pdf&quot; rel=&quot;nofollow&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="101" LastActivityDate="2016-08-05T04:52:42.947" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1357" PostTypeId="1" AcceptedAnswerId="1359" CreationDate="2016-08-05T05:40:23.683" Score="10" ViewCount="269" Body="&lt;p&gt;Can an AI program have an EQ (emotional intelligence or emotional quotient)?&lt;/p&gt;&#xA;&lt;p&gt;In other words, can the EQ of an AI program be measured?&lt;/p&gt;&#xA;&lt;p&gt;If EQ is more problematic to measure than IQ (at least with a standard applicable to both humans and AI programs), why is that the case?&lt;/p&gt;&#xA;" OwnerUserId="1278" LastEditorUserId="2444" LastEditDate="2021-09-23T16:14:43.233" LastActivityDate="2021-09-23T16:14:43.233" Title="How can the emotional quotient of an AI program be measured?" Tags="&lt;emotional-intelligence&gt;&lt;intelligence-testing&gt;" AnswerCount="1" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="1358" PostTypeId="1" AcceptedAnswerId="1361" CreationDate="2016-08-05T07:03:50.110" Score="17" ViewCount="2712" Body="&lt;p&gt;I have heard about this concept in a Reddit post about AlphaGo. I have tried to go through the paper and the article, but could not really make sense of the algorithm.&lt;/p&gt;&#xA;&lt;p&gt;So, can someone give an easy-to-understand explanation of how the Monte-Carlo search algorithm work and how is it being used in building game-playing AI bots?&lt;/p&gt;&#xA;" OwnerUserId="101" LastEditorUserId="2444" LastEditDate="2021-12-22T18:13:56.987" LastActivityDate="2021-12-22T18:13:56.987" Title="How does &quot;Monte-Carlo search&quot; work?" Tags="&lt;game-ai&gt;&lt;monte-carlo-tree-search&gt;&lt;monte-carlo-methods&gt;&lt;alphago&gt;" AnswerCount="1" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="1359" PostTypeId="2" ParentId="1357" CreationDate="2016-08-05T08:24:24.657" Score="6" Body="&lt;p&gt;The answer to your question is &quot;In principle, yes&quot; - in it's most general form, EQ testing is just a specific case of the Turing test (&quot;How would you feel about ... ?&quot;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;To see why meaningful EQ tests might be difficult to achieve, consider the following two possible tests:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At one extreme of complexity, the film 'Blade Runner' famously shows a test to distinguish between human and android on the basis of responses to emotionally-charged questions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you tried asking these questions (or even much simpler ones) to a modern chatbot, you'd likely quickly conclude that you were not talking to a person.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem with assessing EQ is that the more emotionally sophisticated the test, the more general the AI system is likely have to be, in order to turn the input into a meaningful representation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At the other extreme from the above, suppose that an EQ test was phrased in an extremely structured way, with the structured input provided by a human. In such a case, success at an 'EQ test' is not really grounded in the real-world.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In an essay entitled &quot;The ineradicable Eliza effect and its dangers&quot;, Douglas Hofstadter gives the following example, in which the ACME program is claimed (not by Hofstadter) to 'understand' analogy.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Here the computer learns about a fellow named Sluggo taking his wife Jane and&#xA;  his good buddy Buck to a bar, where things take their natural course and Jane&#xA;  winds up pregnant by Buck. She has the baby but doesn't want it, and so, aided&#xA;  by her husband, she drowns the baby in a river, thus &quot;neatly solving &quot;the problem&quot;&#xA;  of Bambi.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;This story is presented to ACME in the following form:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ql: (neglectful-husband (Sluggo))&#xA;q2: (lonely-and-sex-starved-wife (Jane-Doe))&#xA;q3: (macho-ladykiller (Buck-Stag))&#xA;q4: (poor-innocent-little-fetus (Bambi))&#xA;q5: (takes-out-to-local-bar (Sluggo Jane-Doe Buck-Stag))&#xA;...&#xA;q11: (neatly-solves-the-problem-of (Jane-Doe Bambi))&#xA;q12: (cause (ql0 q11))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Suppose the program were to be asked if Jane Doe's behavior was moral. Complex compound emotional concepts such as 'neglectful', 'lonely' and 'innocent' are here simply predicates, not available to the AI for deeper introspective examination. They could just as easily be replaced by labels such as as 'bling-blang-blong15657'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So in one sense, the absence of success at an EQ test with any depth is indicative of the general problem currently facing AI: the inability to define (or otherwise learn) meaningful representations of subtle complexities of the human world, which is a lot more complex than being able to recognize videos of cats.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="42" LastEditDate="2016-08-05T12:43:38.537" LastActivityDate="2016-08-05T12:43:38.537" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1360" PostTypeId="1" AcceptedAnswerId="1369" CreationDate="2016-08-05T08:51:50.657" Score="6" ViewCount="170" Body="&lt;p&gt;DNNs are typically used to classify things (of course) but can we let them go wild with sounds and then tell them if we think it sounds good or not? I'd like to think after a training class has been made (perhaps comparing the output to an existing song) we could get an NN that has a basic concept of music.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Timing would be an issue; I'm not sure how feasible this is. A strongly weighted input attached to all hidden layers perhaps? Use it as the bias?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this even slightly feasible? &lt;/p&gt;&#xA;" OwnerUserId="1284" LastEditorUserId="145" LastEditDate="2016-08-18T21:36:41.180" LastActivityDate="2016-08-18T21:36:41.180" Title="Has any research been done on DNN Music?" Tags="&lt;deep-neural-networks&gt;&lt;machine-learning&gt;" AnswerCount="1" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="1361" PostTypeId="2" ParentId="1358" CreationDate="2016-08-05T09:32:09.223" Score="15" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Monte_Carlo_method&quot;&gt;Monte Carlo method&lt;/a&gt; is an approach where you generate a large number of random values or simulations and form some sort of conlusions based on the general patterns, such as the means and variances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an example, you could use it for &lt;a href=&quot;https://en.wikipedia.org/wiki/Numerical_weather_prediction&quot;&gt;weather forecasts&lt;/a&gt;. Predicting long-term weather is quite difficult, because it is a chaotic system where small changes can lead to very different results. Using Monte Carlo methods, you could run a large number of simulations, each with slightly different atmospheric changes. Then you can analyze the results and for example calculate the probability of rain on a given day based on how many simulations ended up with rain. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for the use of Monte Carlo in Alpha Go, they seem to be using the so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Monte_Carlo_tree_search&quot;&gt;Monte Carlo Tree Search&lt;/a&gt;. In this approach, you make a tree of possible moves, a few turns into the future, and try to find the best sequence. However, since the number of possible moves in the game of go is very large, you won't be able to explore very far ahead. This means that some of the moves which look good now might turn out to be bad later. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, in the Monte Carlo Tree Search, you pick a promising sequence of moves and run one or more simulations of how the game might proceed from that point. Then you can use the results of that simulation to get a better idea of how good that specific sequence of moves really is and you update the tree accordingly. Repeat as needed until you find a good move.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want more information or to look at some illustrations, I found an interesting paper on the topic: C. Browne et al., A Survey of Monte Carlo Tree Search Methods (&lt;a href=&quot;http://repository.essex.ac.uk/4117/1/MCTS-Survey.pdf&quot;&gt;open repository&lt;/a&gt; / &lt;a href=&quot;http://dx.doi.org/10.1109/TCIAIG.2012.2186810&quot;&gt;permanent link (paywalled)&lt;/a&gt;)&lt;/p&gt;&#xA;" OwnerUserId="30" LastActivityDate="2016-08-05T09:32:09.223" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1362" PostTypeId="1" AcceptedAnswerId="1365" CreationDate="2016-08-05T10:39:31.520" Score="6" ViewCount="4428" Body="&lt;p&gt;How do I avoid my gradient descent algorithm into falling into the &quot;local minima&quot; trap while backpropogating on my neural network?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any methods which help me avoid it?&lt;/p&gt;&#xA;" OwnerUserId="101" LastEditorUserId="2444" LastEditDate="2019-04-27T00:50:35.780" LastActivityDate="2019-04-30T01:57:53.150" Title="How to avoid falling into the &quot;local minima&quot; trap?" Tags="&lt;neural-networks&gt;&lt;backpropagation&gt;&lt;optimization&gt;&lt;gradient-descent&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1363" PostTypeId="1" CreationDate="2016-08-05T10:49:39.557" Score="9" ViewCount="663" Body="&lt;p&gt;A neural network is a directed weighted graph. These can be represented by a (sparse) matrix. Doing so can expose some elegant properties of the network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this technique beneficial for examining neural networks?&lt;/p&gt;&#xA;" OwnerUserId="1283" LastActivityDate="2017-02-16T19:38:44.487" Title="Is it beneficial to represent a neural net as a matrix?" Tags="&lt;neural-networks&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1364" PostTypeId="2" ParentId="1363" CreationDate="2016-08-05T10:58:53.260" Score="4" Body="&lt;p&gt;It depends on the type of neural networks you are dealing with.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For medium sized neural nets, the matrix approach is a very good way to do quick computations and even backpropogation of errors. One can even exploit sparse matrixes for understanding the sparse architecture of some neural nets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But, for very large neural nets, using matrix computations would be computationally very intensive. So, relevant methods like graph-based stores, etc are used for them depending on the purpose and the architecture.&lt;/p&gt;&#xA;" OwnerUserId="101" LastEditorUserId="101" LastEditDate="2017-02-16T19:38:44.487" LastActivityDate="2017-02-16T19:38:44.487" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1365" PostTypeId="2" ParentId="1362" CreationDate="2016-08-05T11:00:14.427" Score="8" Body="&lt;p&gt;There are several elementary techniques to try and move a search out of the basin of attraction of local optima. They include:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Probabalistically accepting worse solutions in the hope that this&#xA;will jump out of the current basin (like Metropolis-Hastings acceptance in Simulated Annealing). &lt;/li&gt;&#xA;&lt;li&gt;Maintaining a list of recently-encountered states (or attributes thereof) and not returning&#xA;to a recently-encountered one (like Tabu Search). &lt;/li&gt;&#xA;&lt;li&gt;Performing a random walk of a length determined by the current state of the search (an explicit 'Diversification strategy', e.g. as used in 'Reactive Tabu Search').&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;See the excellent (and free online) book &lt;a href=&quot;https://cs.gmu.edu/~sean/book/metaheuristics/&quot;&gt;'Essentials of Metaheuristics'&lt;/a&gt; by Sean Luke for more details on these kind of techniques and some rules of thumb about when and how to use them.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-05T11:00:14.427" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1366" PostTypeId="2" ParentId="1363" CreationDate="2016-08-05T11:15:40.017" Score="4" Body="&lt;p&gt;For large ANNs, something equivalent to a 'sparse matrix format' is used in practice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In contrast to what is said in another answer given, considering an ANN as a graph doesn't actually buy very much, for two reasons:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The backpropagation algorithm can usefully be&#xA;defined in terms of matrix operations. &lt;a href=&quot;http://briandolhansky.com/blog/2014/10/30/artificial-neural-networks-matrix-form-part-5&quot; rel=&quot;nofollow&quot;&gt;This page&lt;/a&gt; gives a&#xA;readable and comprehensive description.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;All real-valued matrices can be represented as graphs, but the converse is clearly not the case. So while it is true that an ANN can be considered as a special case of a graph data structure, making that specialization explicit in matrix form is more efficient.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="42" LastEditorUserId="42" LastEditDate="2016-08-05T19:12:14.883" LastActivityDate="2016-08-05T19:12:14.883" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1369" PostTypeId="2" ParentId="1360" CreationDate="2016-08-05T12:05:13.230" Score="6" Body="&lt;p&gt;The first thing is to define what is a «good» and a «bad» sound. This is an extremely tricky issue, since the networks need &lt;em&gt;numeric&lt;/em&gt; inputs. And music is whole bunch of numbers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know from people doing research in identifying &lt;em&gt;how similar&lt;/em&gt; two sounds are, and imitation, say: you hear a sound and try to make another that sounds like it. Like when you hum a song or similar. That is by no means easy. These guys are using something similar to feature extraction, with Fourier transforms and energy and &lt;a href=&quot;https://en.wikipedia.org/wiki/Music_information_retrieval&quot; rel=&quot;noreferrer&quot;&gt;such things&lt;/a&gt;. They feed the networks with the (selected) features and... Train. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, to return to your original question: *What do you present as &lt;em&gt;target&lt;/em&gt; during training?* You can present different &lt;em&gt;types&lt;/em&gt; of music as categories and classify (I couldn't help but think on &lt;a href=&quot;https://link.springer.com/article/10.3758/BF03192900&quot; rel=&quot;noreferrer&quot;&gt;this research with fish&lt;/a&gt;). Or &lt;strong&gt;you&lt;/strong&gt; define categories of music &lt;strong&gt;you&lt;/strong&gt; like and see if the network can classify them ;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One basic decision here is how long you get a piece of sound. Since it is needed to analyse frequency, this is a key issue. Since you talked about DNN, I was wondering if you wanted to do it &lt;em&gt;online&lt;/em&gt;, as a stream, in which case I don't have the slightest idea where to begin, other than do it &lt;em&gt;after a little while&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Other idea: I remember a little sketch in &lt;a href=&quot;http://www.bbc.co.uk/programmes/b012xppj&quot; rel=&quot;noreferrer&quot;&gt;this series&lt;/a&gt; about a researcher that makes use of the relations between peaks in the Fourier spectrum in order to differentiate noise from music.&lt;/p&gt;&#xA;" OwnerUserId="70" LastActivityDate="2016-08-05T12:05:13.230" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1370" PostTypeId="2" ParentId="1333" CreationDate="2016-08-05T13:55:54.380" Score="3" Body="&lt;p&gt;It seems easy for this to be sublinear growth or superlinear growth, depending on context. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we imagine the space of the complex AI as split into two parts--the context model and the content model (that is, information and structure that is expected to be shared across entries vs. information and structure that is local to particular entries), then expanding the source material means we don't have much additional work to do on the context model, but whether the additional piece of the content model is larger or smaller depends on how connected the new material is to the old material.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That is, one of the reasons why Watson takes many times the space of its source material is because it stores links between objects, which one would expect to grow with roughly order &lt;em&gt;n&lt;/em&gt; squared. If there are many links between the old and new material, then we should expect it to roughly quadruple in size instead of double; if the old material and new material are mostly unconnected and roughly the same in topology, then we expect the model to roughly double; if the new material is mostly unconnected to the old material and also mostly unconnected to itself, then we expect the model to not grow by much.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-05T13:55:54.380" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1371" PostTypeId="2" ParentId="179" CreationDate="2016-08-05T13:59:51.080" Score="2" Body="&lt;p&gt;If anything, multiple intelligences are much more obvious in AI than in other fields, because we haven't yet unlocked how to do transfer between domains.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an example, AlphaGo is very, very good at playing Go, but it's got basically nothing in the way of bodily-kinesthetic intelligence. But other teams have built software to control robots that does have bodily-kinesthetic intelligence, while not being good at the tasks that AlphaGo excels at.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This sort of modular intelligence is typically referred to as 'narrow AI,' whereas we use the term 'general AI' (or AGI, for Artificial General Intelligence) to refer to intelligence that we've built that can do roughly as many different kinds of things as people can do.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-05T13:59:51.080" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1372" PostTypeId="2" ParentId="145" CreationDate="2016-08-05T14:05:19.287" Score="8" Body="&lt;p&gt;&quot;Current artificial intelligence research&quot; is a pretty broad field. From where I sit, in a mostly CS realm, people are focused on narrow intelligence that can do economically relevant work on narrow tasks. (That is, predicting when components will fail, predicting which ads a user will click on, and so on.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For those sorts of tools, the generality of a formalism like AIXI is a weakness instead of a strength. You don't need to take an AI that could in theory compute anything, and then slowly train it to focus on what you want, when you could just directly shape a tool that is the mirror of your task.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not as familiar with AGI research itself, but my impression is that AIXI is, to some extent, the simplest idea that could work--it takes all the hard part and pushes it into computation, so it's 'just an engineering challenge.' (This is the bit about 'finding approximations to AIXI.') The question then becomes, is starting at AIXI and trying to approximate down a more or less fruitful research path than starting at something small and functional, and trying to build up?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My impression is the latter is much more common, but again, I only see a small corner of this space.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-05T14:05:19.287" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1373" PostTypeId="2" ParentId="145" CreationDate="2016-08-05T14:25:48.467" Score="4" Body="&lt;p&gt;AIXI is really a conceptual framework. All the hard work of actually compressing the environment still remains.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To further discuss the question raised in Matthew Graves answer: given our current limited level of ability to represent complex environments, it seems to me that it doesn't make a lot of practical difference whether you start with AIXI as defining the 'top' of the system and working down (e.g. via supposedly generalized compression methods) or start at the 'bottom' and try solve problems in a single domain via domain-specific methods that (you hope) can subsequently be abstracted to provide cross-domain compression.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-05T14:25:48.467" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="1376" PostTypeId="1" CreationDate="2016-08-05T16:51:49.833" Score="4" ViewCount="131" Body="&lt;p&gt;Would it be ethical to implement AI for self-defence for public walking robots which are exposed to dangers such as violence and crime such as robbery (of parts), damage or abduction?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be pros and cons of such AI behavior? Is it realistic, or it won't be taken into account for some obvious reasons?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like pushing back somebody when somebody start pushing it first (AI will say: he pushed me first), or running away on crowded street in case algorithm will detect risk of abduction.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="8" LastEditDate="2016-08-05T17:07:35.410" LastActivityDate="2016-08-05T17:08:27.273" Title="Is it ethical to implement self-defence for street walking AI robots?" Tags="&lt;ethics&gt;&lt;decision-theory&gt;&lt;robots&gt;" AnswerCount="2" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1377" PostTypeId="2" ParentId="1376" CreationDate="2016-08-05T17:06:27.950" Score="2" Body="&lt;p&gt;The question mentions &quot;walking robot&quot;, but it may be illustrative to re-frame the discussion in terms of self-driving cars, because: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It gives a common point of reference, rather than everyone having their own separate vision of how vulnerable/powerful a kung-fu walking robot might be.&lt;/li&gt;&#xA;&lt;li&gt;We already know a lot about societal attitudes to car theft.&lt;/li&gt;&#xA;&lt;li&gt;Given that autonomous vehicles will soon be mainstream, the morality of the question is then more of a pressing issue.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So, should a self-driving car run someone over (likely killing them) if they try to steal it? I'm hoping that few people would argue that it should.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Should it attempt to do a lesser amount of damage (say, calculated to hopefully only break a leg)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again, I'd argue not. The main reason for saying this is that our decision-making algorithms are simply not sufficiently context aware to be able to decide whether theft or harm is the intent. To concretely illustrate this: a recent fatality arose because a self-driving Tesla &lt;a href=&quot;http://www.livescience.com/55273-first-self-driving-car-fatality.html&quot; rel=&quot;nofollow&quot;&gt;was oblivious to context&lt;/a&gt; to the extent that it couldn't distinguish between a high-sided van and empty space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Under those circumstances, it's probably best not to allow commercial autonomous systems to cause physical damage (even to inanimate objects). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;'Running away' (or rather, 'driving away', in the case of the car) is another matter: driving is what it's designed to do.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-05T17:06:27.950" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1378" PostTypeId="2" ParentId="1376" CreationDate="2016-08-05T17:08:27.273" Score="2" Body="&lt;p&gt;It depends on whether the loss of the robot would end up causing harm to humans.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the robot was supposed to be watching for a suspected terrorist attack to start taking place (so it could alert authorities or halt the attack), it would be very bad if somebody dismantled the robot or otherwise stopped it from carrying out its mission. In that case, the device would be certainly justified in stopping humans from injuring it in any meaningful way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A robot carrying classified information should probably be similarly willing to protect itself, since the spread of such data could bring harm to a state or a lot of people.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If an AI-enabled device was just walking the streets in the course of carrying out some mundane task, I think it would be hard to justify allowing the robot to incapacitate a human attacker. After all, it was made - presumably - to serve humans.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;No matter whether the AI was programmed to defend itself, people couldn't just impede or damage it with impunity. &lt;a href=&quot;https://malegislature.gov/laws/generallaws/partiv/titlei/chapter266/section127&quot; rel=&quot;nofollow&quot;&gt;Intentional destruction&lt;/a&gt; of another person's property (including public property) is almost certainly a crime, as is &lt;a href=&quot;http://law.justia.com/codes/georgia/2010/title-16/chapter-10/article-2/16-10-24&quot; rel=&quot;nofollow&quot;&gt;intentional obstruction of law enforcement&lt;/a&gt;. It wouldn't have to be up to each robot to defend itself; it could just send information about the perpetrator to C&amp;amp;C before its demise.&lt;/p&gt;&#xA;" OwnerUserId="75" LastActivityDate="2016-08-05T17:08:27.273" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1379" PostTypeId="1" AcceptedAnswerId="1380" CreationDate="2016-08-05T17:11:34.880" Score="0" ViewCount="70" Body="&lt;p&gt;Is there any risk in the near future of replacing all encyclopedias with Watson-like AI where knowledge is accessible by everybody through &lt;a href=&quot;https://watson-api-explorer.mybluemix.net/&quot; rel=&quot;nofollow&quot;&gt;API&lt;/a&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;Something similar happened in the future in &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Time_Machine_(2002_film)&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;The Time Machine&lt;/strong&gt; movie from 2002&lt;/a&gt;.&lt;/sup&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously maintaining 40 million articles and keeping it up-to-date and consistent could be beyond brain power of few thousands of active editors. Not to mention thousands of other encyclopedias including paperback version or large number of books used by universities which needs to be updated every year by a huge number of people.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the pros and cons of such a change?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="145" LastEditDate="2016-08-11T12:21:13.530" LastActivityDate="2016-08-11T12:21:13.530" Title="How likely is it that Watson-like AI will replace Wikipedia-like encyclopedias?" Tags="&lt;watson&gt;" AnswerCount="1" CommentCount="1" ClosedDate="2016-08-12T15:19:24.397" ContentLicense="CC BY-SA 3.0" />
  <row Id="1380" PostTypeId="2" ParentId="1379" CreationDate="2016-08-05T17:30:07.273" Score="3" Body="&lt;p&gt;I get the impression that (perhaps even more than Bluemix) this is what the &lt;a href=&quot;https://www.wolfram.com/language/elementary-introduction/&quot; rel=&quot;nofollow&quot;&gt;Wolfram Language&lt;/a&gt; is looking to offer in the longer term.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Seems to me that the main pros and cons are two sides of the same coin:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With Wikipedia, there's no 'search filter' between you and the text. Adding an algorithmic level of indirection between the user and the knowledge that they're looking for is subject to hidden biases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If those biases are intended in your best interests, and the search is context-sensitive enough to present you with information in the form that is most useful and digestible to you, then this is a good thing. Otherwise, not. Like many topics in AI, problems arise because we're simply not that good at modelling human context yet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, we're already subject to this &lt;a href=&quot;https://en.wikipedia.org/wiki/Filter_bubble&quot; rel=&quot;nofollow&quot;&gt;filter bubble&lt;/a&gt; effect via search engines and social media. The current consensus seems to be that even more of this would not be a good thing for society.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="42" LastEditDate="2016-08-05T17:37:44.953" LastActivityDate="2016-08-05T17:37:44.953" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1381" PostTypeId="1" AcceptedAnswerId="1382" CreationDate="2016-08-05T17:38:23.270" Score="3" ViewCount="95" Body="&lt;p&gt;I've watched the &lt;a href=&quot;https://www.youtube.com/watch?v=LY7x2Ihqjmc&quot; rel=&quot;nofollow&quot;&gt;Sunspring&lt;/a&gt; video which didn't make any sense to me (a lot of nonsense monologues), mainly because it was created by Jetson AI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What was the mechanism of creating such screenplay?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On what criteria was it trained? What was the goal or motivation in terms of training criteria of defining when text does make sense? And what was missed (that it's so bad) and how possibly this could be improved?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="130" LastEditDate="2016-08-06T01:44:14.583" LastActivityDate="2016-08-06T01:44:14.583" Title="How does the Jetson AI write its screenplays?" Tags="&lt;algorithm&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1382" PostTypeId="2" ParentId="1381" CreationDate="2016-08-05T17:52:10.297" Score="6" Body="&lt;p&gt;It &lt;a href=&quot;http://benjamin.wtf/&quot; rel=&quot;noreferrer&quot;&gt;appears to use&lt;/a&gt; Recurrent NNs (RNNs) that have a 'Long Short-Term Memory' (LTSM) architecture.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://medium.com/artists-and-machine-intelligence/adventures-in-narrated-reality-6516ff395ba3#.5lvtgribl&quot; rel=&quot;noreferrer&quot;&gt;Here's a summary&lt;/a&gt; of the development process that the author, Ross Goodwin, went through to create it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems to me (and is also observed in the above link) that the output is rather poor - simply comparable to what one might expect from Markov chains, a technique that is over 100 years old.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't dug deeply into the technique, so I could be misktaken, but perhaps one of the reasons that it's so bad is that (as far as I can see), the model-building process is essentially &lt;em&gt;lexical&lt;/em&gt; - i.e. it is linking together tokens (words) without any more informed language model to guide it. In particular, the generated output doesn't seem to know anything about the functional roles played by objects (chairs are supporting objects, used by humans for sitting on etc), which is something that might be fairly readily incorporated.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="42" LastEditDate="2016-08-05T18:23:38.027" LastActivityDate="2016-08-05T18:23:38.027" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1384" PostTypeId="1" AcceptedAnswerId="1385" CreationDate="2016-08-05T18:22:41.003" Score="4" ViewCount="266" Body="&lt;p&gt;This &lt;a href=&quot;http://blog.claymcleod.io/2016/06/01/The-truth-about-Deep-Learning/&quot; rel=&quot;nofollow noreferrer&quot;&gt;article&lt;/a&gt; suggests that deep learning is not designed to produce the universal algorithm and cannot be used to create such a complex systems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First of all it requires huge amounts of computing power, time and effort to train the algorithm the right way and adding extra layers doesn't really help to solve complex problems which cannot be easily predicted.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Secondly some tasks are extremely difficult or impossible to solve using DNN, like solving a &lt;a href=&quot;https://ai.stackexchange.com/q/154/8&quot;&gt;math&lt;/a&gt; equations, predicting &lt;a href=&quot;https://ai.stackexchange.com/q/225/8&quot;&gt;pseudo-random lists&lt;/a&gt;, &lt;a href=&quot;https://ai.stackexchange.com/q/168/8&quot;&gt;fluid mechanics&lt;/a&gt;, guessing encryption algorithms, or &lt;a href=&quot;https://ai.stackexchange.com/q/205/8&quot;&gt;decompiling&lt;/a&gt; unknown formats, because there is no simple mapping between input and output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I'm asking, are there any alternative learning algorithms as powerful as deep architectures for general purpose problem solving? Which can solve more variety of problems, than &quot;deep&quot; architectures cannot?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="-1" LastEditDate="2017-04-13T12:53:10.013" LastActivityDate="2017-07-27T08:55:09.363" Title="Are there any learning algorithms as powerful as &quot;deep&quot; architectures?" Tags="&lt;deep-neural-networks&gt;&lt;comparison&gt;&lt;architecture&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1385" PostTypeId="2" ParentId="1384" CreationDate="2016-08-05T18:41:59.757" Score="7" Body="&lt;p&gt;Have you read the book &lt;a href=&quot;http://libgen.io/ads.php?md5=F18395FBEF45575CF68BBD5AD26DF035&quot; rel=&quot;nofollow noreferrer&quot;&gt;The Master Algorithm:&lt;/a&gt; by Pedro Domingos?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;He discusses the present day machine learning algorithms... Their strengths, weaknesses and applications...&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Deep Neural Network&lt;/li&gt;&#xA;&lt;li&gt;Genetic Algorithm&lt;/li&gt;&#xA;&lt;li&gt;Bayesian Network &lt;/li&gt;&#xA;&lt;li&gt;Support Vector Machine&lt;/li&gt;&#xA;&lt;li&gt;Inverse Deduction &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9HpIP.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9HpIP.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="157" LastEditorUserId="157" LastEditDate="2017-07-27T08:55:09.363" LastActivityDate="2017-07-27T08:55:09.363" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1386" PostTypeId="2" ParentId="1384" CreationDate="2016-08-05T18:44:20.053" Score="4" Body="&lt;p&gt;Deep learning is actually pretty useful (relative to other techniques) &lt;em&gt;precisely when there is no simple mapping between input and output&lt;/em&gt;, and features from the raw input need to be aggregated and combined in complex ways by successive layers to form the output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I pointed out in my answer to the &lt;a href=&quot;https://ai.stackexchange.com/questions/205/how-to-write-c-decompiler-using-ai&quot;&gt;AI SE decompilation question&lt;/a&gt;, there is recent DL research which takes a natural language description as input and &lt;a href=&quot;http://arxiv.org/pdf/1510.07211.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;generates program text as output&lt;/a&gt;. Despite working in this general research area, I was personally surprised by this - the problem is significantly harder than the 'AI math' link you provide above.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="-1" LastEditDate="2017-04-13T12:53:10.013" LastActivityDate="2016-08-05T18:44:20.053" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1387" PostTypeId="2" ParentId="6" CreationDate="2016-08-05T19:31:15.263" Score="4" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;the human mind is a battleground of higher level goals and lower level goals &quot;&lt;br&gt;— Marvin Minsky paraphrasing Sigmund Freud&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I argue that in general human agents try to maximise a hierarchy of performance measures.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;performance measures of humans&lt;/h1&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Survival of genetic data &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Energy supply and Water&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Sex&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;myriad subgoals....&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Mysterious mental mechanisms which neuroscientists do not understand yet force the average human agent to maximise various evaluation metrics.&#xA;With the overarching goal of &lt;strong&gt;survival of genetic information&lt;/strong&gt;. Successful genes are immortal. We are still under the yoke of an ancient genetic algorithm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These measures are optimised throughout a humans life time. A 30 year old agent is better at survival than a 10 year old agent. A 30 year old agent makes fewer mistakes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We remember our mistakes. Mistakes are burned into our memory by high levels of neurotransmitters (and reinforcing of synapses) so we don't make them again.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We attempt to optimise a swarm of subgoals that are all connected in one way or another to the main goal &lt;strong&gt;gene survival&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;status&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;money&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;education &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;happiness&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="157" LastEditorUserId="29" LastEditDate="2016-08-05T23:21:31.713" LastActivityDate="2016-08-05T23:21:31.713" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1389" PostTypeId="2" ParentId="60" CreationDate="2016-08-05T20:08:43.130" Score="-2" Body="&lt;p&gt;One obstacle to the development of AI is the fundamental limitations of computer memory. Computers, at a fundamental level, can only work with bits. This limits the type of information that they can describe.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The precise nature and complexity of human memory isn't fully understood, but I would argue that at the very least, human memory is well adapted for the types of tasks that humans perform. Thus, computer memory, even if theoretically capable of representing everything that human memory can, is  probably inefficient and poorly structured for such a task.  &lt;/p&gt;&#xA;" OwnerUserId="127" LastEditorUserId="127" LastEditDate="2016-08-08T17:36:35.903" LastActivityDate="2016-08-08T17:36:35.903" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="1390" PostTypeId="1" AcceptedAnswerId="1412" CreationDate="2016-08-05T20:47:50.290" Score="8" ViewCount="1002" Body="&lt;p&gt;Is there any research on the application of AI for drug design?&lt;/p&gt;&#xA;&lt;p&gt;For example, you could train a deep learning model about current compounds, substances, structures, and their products and chemical reactions from the existing &lt;a href=&quot;https://opendata.stackexchange.com/q/3553/3082&quot;&gt;dataset&lt;/a&gt; (basically what produces what). Then you give the task to find how to create a gold or silver from the group of available substances. Then the algorithm will find the chemical reactions (successfully predicting a new one that wasn't in the dataset) and gives the results. Maybe the gold is not a good example, but the practical scenario would be the creation/design of drugs that are cheaper to create by using much simpler processes or synthesizing some substances for the first time for drug industries.&lt;/p&gt;&#xA;&lt;p&gt;Was there any successful research attempting to achieve that using deep learning algorithms?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-11T20:11:46.910" LastActivityDate="2021-04-19T22:45:21.740" Title="Is there any research on the application of AI for drug design?" Tags="&lt;deep-learning&gt;&lt;reference-request&gt;&lt;applications&gt;&lt;prediction&gt;&lt;drug-design&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1391" PostTypeId="1" AcceptedAnswerId="1399" CreationDate="2016-08-05T21:29:37.880" Score="16" ViewCount="727" Body="&lt;p&gt;Assume that I want to solve an issue with a neural network that either I can't fit to existing architectures (perceptron, Konohen, etc) or I'm simply not aware of the existence of those or I'm unable to understand their mechanics and I rely on my own instead.&lt;/p&gt;&#xA;&lt;p&gt;How can I automate the choice of the architecture/topology (that is, the number of layers, the type of activations, the type and direction of the connections, etc.) of a neural network for an arbitrary problem?&lt;/p&gt;&#xA;&lt;p&gt;I'm a beginner, yet I realized that in some architectures (or, at least, in perceptrons) it is very hard if not impossible to understand the inner mechanics, as the neurons of the hidden layers don't express any mathematically meaningful context.&lt;/p&gt;&#xA;" OwnerUserId="1270" LastEditorUserId="2444" LastEditDate="2021-12-13T14:35:05.673" LastActivityDate="2021-12-25T11:11:47.567" Title="How can I automate the choice of the architecture of a neural network for an arbitrary problem?" Tags="&lt;neural-networks&gt;&lt;reference-request&gt;&lt;hyperparameter-optimization&gt;&lt;architecture&gt;&lt;neuroevolution&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1392" PostTypeId="1" AcceptedAnswerId="1450" CreationDate="2016-08-06T00:24:44.493" Score="1" ViewCount="103" Body="&lt;p&gt;For example there is &lt;a href=&quot;https://en.wikipedia.org/wiki/MNIST_database&quot; rel=&quot;nofollow&quot;&gt;the MNIST database&lt;/a&gt; which is used to test artificial neural network (ANN), however it's not so challenging, because some hierarchical systems of convolutional neural networks manages to get an error rate of 0.23 percent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any similar, especially the most challenging tasks with dataset which are used as benchmark tests to challenge the AI which are fairly reliable and it's possible to pass, but most AAN are struggling to achieve the lower error rate?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="1581" LastEditDate="2018-11-13T17:21:58.497" LastActivityDate="2018-11-13T17:21:58.497" Title="What are the most challenging tasks aiming to achieve the lowest error rate?" Tags="&lt;deep-learning&gt;&lt;image-recognition&gt;&lt;data-science&gt;" AnswerCount="1" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="1393" PostTypeId="1" CreationDate="2016-08-06T00:37:24.067" Score="3" ViewCount="277" Body="&lt;p&gt;The paper &lt;a href=&quot;http://repository.supsi.ch/5145/1/IDSIA-04-12.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;Multi-column Deep Neural Networks for Image Classification&lt;/a&gt; (pages 7-8) shows an attempt at recognizing the traffic signs, with lower error rates, by using multi-column deep neural networks. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are Google cars using similar techniques of predicting signs using DNN, or are they using some other method?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-06-20T14:58:10.773" LastActivityDate="2019-06-20T15:00:05.697" Title="How do Google cars recognize the traffic signs?" Tags="&lt;deep-learning&gt;&lt;image-recognition&gt;&lt;classification&gt;&lt;autonomous-vehicles&gt;&lt;google&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1394" PostTypeId="1" AcceptedAnswerId="1408" CreationDate="2016-08-06T01:27:03.500" Score="4" ViewCount="192" Body="&lt;p&gt;I'd like to know whether there were attempts to simulate the whole brain, I'm not talking only about some &lt;a href=&quot;https://ai.stackexchange.com/q/237/8&quot;&gt;ANN on microchips&lt;/a&gt;, but brain simulations.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-26T01:56:55.193" LastActivityDate="2021-01-26T01:56:55.193" Title="Are there any artificial neuromorphic systems which can mimic the brain?" Tags="&lt;neuromorphic-engineering&gt;&lt;brain&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1395" PostTypeId="2" ParentId="1394" CreationDate="2016-08-06T01:27:03.500" Score="3" Body="&lt;p&gt;Neuromorphic engineering offers various of ways of reproducing the brain’s processing ability.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The recent technology can include IBM's multi-artificial-neuron computer, the world's first artificial nanoscale stochastic phase-change neurons&lt;sup&gt;&lt;a href=&quot;http://arstechnica.com/gadgets/2016/08/ibm-phase-change-neurons/?&quot; rel=&quot;nofollow&quot;&gt;article&lt;/a&gt;&lt;/sup&gt;. Check the: &lt;a href=&quot;http://www.nature.com/nnano/journal/v11/n8/full/nnano.2016.70.html&quot; rel=&quot;nofollow&quot;&gt;Stochastic phase-change neurons&lt;/a&gt; study.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other can include&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://web.stanford.edu/group/brainsinsilicon/neurogrid.html&quot; rel=&quot;nofollow&quot;&gt;Neurogrid&lt;/a&gt;, built by Brains in Silicon at Stanford University is another example for brain simulation. It uses analog computation to emulate ion-channel activity. It emulates neurons using digital circuitry designed to maximize spiking throughput&lt;sup&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Neuromorphic_engineering#Examples&quot; rel=&quot;nofollow&quot;&gt;wiki&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SpiNNaker&quot; rel=&quot;nofollow&quot;&gt;SpiNNaker&lt;/a&gt;, which is a manycore computer to simulate the human brain (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_Brain_Project&quot; rel=&quot;nofollow&quot;&gt;Human Brain Project&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SyNAPSE&quot; rel=&quot;nofollow&quot;&gt;SyNAPSE&lt;/a&gt;, a DARPA neuromorphic machine technology, that scales to biological levels. Each chip can have over a million of electronic “neurons” and 256 million electronic synapses between neurons. In 2014 the 5.4 billion transistor chip had one of the highest transistor counts of any chip ever produced. The program is undertaken by HRL, HP and IBM.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastActivityDate="2016-08-06T01:27:03.500" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1396" PostTypeId="1" AcceptedAnswerId="1406" CreationDate="2016-08-06T01:57:43.263" Score="24" ViewCount="6485" Body="&lt;p&gt;On &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_intelligence&quot; rel=&quot;noreferrer&quot;&gt;the Wikipedia page&lt;/a&gt; about AI, we can read:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Optical character recognition is no longer perceived as an exemplar of &quot;artificial intelligence&quot; having become a routine technology.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;On the other hand, the &lt;a href=&quot;https://en.wikipedia.org/wiki/MNIST_database&quot; rel=&quot;noreferrer&quot;&gt;MNIST&lt;/a&gt; database of handwritten digits is specially designed for training and testing neural networks and their error rates (see: &lt;a href=&quot;https://en.wikipedia.org/wiki/MNIST_database#Classifiers&quot; rel=&quot;noreferrer&quot;&gt;Classifiers&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, why does the above quote state that OCR is no longer exemple of AI?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2020-03-08T05:55:09.787" LastActivityDate="2020-03-08T05:55:09.787" Title="Why can't OCR be perceived as a good example of AI?" Tags="&lt;philosophy&gt;&lt;definitions&gt;&lt;optical-character-recognition&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1397" PostTypeId="1" AcceptedAnswerId="1452" CreationDate="2016-08-06T02:04:19.343" Score="13" ViewCount="488" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Minimum_intelligent_signal_test&quot; rel=&quot;noreferrer&quot;&gt;MIST&lt;/a&gt; is a quantiative test of humanness, consisting of ~80k propositions such as:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is Earth a planet?&lt;/li&gt;&#xA;&lt;li&gt;Is the sun bigger than my foot?&lt;/li&gt;&#xA;&lt;li&gt;Do people sometimes lie?&lt;/li&gt;&#xA;&lt;li&gt;etc.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Have any AI attempted and passed this test to date?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="145" LastEditDate="2016-08-15T14:44:41.513" LastActivityDate="2016-09-06T15:23:41.650" Title="Are there any AI that have passed the MIST test so far?" Tags="&lt;history&gt;&lt;intelligence-testing&gt;&lt;turing-test&gt;&lt;chat-bots&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1398" PostTypeId="2" ParentId="1396" CreationDate="2016-08-06T05:02:50.820" Score="4" Body="&lt;p&gt;I'm not sure if predicting MNIST can be really considered as an AI task. AI problems can be usually framed under the context of an agent acting in an environment. Neural nets and machine learning techniques in general do not have to deal with this framing. Classifiers for example, are learning a mapping between two spaces. Though one could argue that you &lt;em&gt;can&lt;/em&gt; frame OCR/image classification as an AI problem - the classifier is the agent, each prediction it makes is an action, and it receives rewards based on its classification accuracy - this is rather unnatural and different from problems that are commonly considered AI problems.&lt;/p&gt;&#xA;" OwnerUserId="1305" LastActivityDate="2016-08-06T05:02:50.820" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1399" PostTypeId="2" ParentId="1391" CreationDate="2016-08-06T05:40:41.717" Score="11" Body="&lt;p&gt;I think in this case, you'll probably want to use a genetic algorithm to generate a topology rather than working on your own. I personally like &lt;a href=&quot;http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;NEAT (NeuroEvolution of Augmenting Topologies)&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The original NEAT paper involves evolving weights for connections, but if you only want a topology, you can use a weighting algorithm instead. You can also mix activation functions if you aren't sure which to use. &lt;a href=&quot;http://blog.otoro.net/2016/05/07/backprop-neat/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Here&lt;/a&gt; is an example of using backpropagation and multiple neuron types.&lt;/p&gt;&#xA;" OwnerUserId="1306" LastEditorUserId="2444" LastEditDate="2019-07-07T21:18:25.820" LastActivityDate="2019-07-07T21:18:25.820" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1400" PostTypeId="2" ParentId="1391" CreationDate="2016-08-06T06:33:58.880" Score="5" Body="&lt;p&gt;&lt;a href=&quot;https://ai.stackexchange.com/a/1399/2444&quot;&gt;The other answer&lt;/a&gt; mentions &lt;a href=&quot;https://www.cs.ucf.edu/%7Ekstanley/neat.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;NEAT&lt;/a&gt; to generate network weights or topologies. The paper &lt;a href=&quot;http://doc.gold.ac.uk/aisb50/AISB50-S11/AISB50-S11-Turner-paper.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;NeuroEvolution: The Importance of Transfer Function Evolution and Heterogeneous Networks&lt;/a&gt;, which also gives a short summary of neuroevolution techniques, provides an alternative approach to NEAT. It uses &lt;a href=&quot;https://web.archive.org/web/20160730231107/http://www.cartesiangp.co.uk/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cartesian Genetic Programming&lt;/a&gt; to evolve multiple activation functions.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="2444" LastEditDate="2021-12-25T11:11:47.567" LastActivityDate="2021-12-25T11:11:47.567" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1401" PostTypeId="1" AcceptedAnswerId="1403" CreationDate="2016-08-06T07:08:20.203" Score="2" ViewCount="67" Body="&lt;p&gt;It is possible of normal code to prove that it is correct using mathematical techniques, and that is often done to ensure that some parts are bug-free. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can we also prove that a piece of code in AI software will cause it to never turn against us, i.e. that the AI is &lt;a href=&quot;https://en.wikipedia.org/wiki/Friendly_artificial_intelligence&quot; rel=&quot;nofollow&quot;&gt;friendly&lt;/a&gt;? Has there any research been done towards this?&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2016-08-06T07:51:00.567" Title="Can we prove that a piece of code in AI software will cause it to never turn against us?" Tags="&lt;friendly-ai&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1402" PostTypeId="2" ParentId="1396" CreationDate="2016-08-06T07:11:39.830" Score="16" Body="&lt;p&gt;Although OCR is now a mainstream technology, it remains true that none our methods genuinely have the recognition facilities of a 5 year old (claimed success with CAPTCHAs notwithstanding). We don't know how to achieve this using well-understood techniques, so OCR should still rightfully be considered an AI problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To see why this might be so, it is illuminating to read the essay&#xA;&lt;a href=&quot;https://web.stanford.edu/group/SHR/4-2/text/hofstadter.html&quot;&gt;&quot;On seeing A's and seeing AS&quot;&lt;/a&gt; by Douglas Hofstadter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With respect to a point made in another answer, the agent framing is a useful one insofar as it motivates success in increasingly complex environments. However, there are many hard problems (e.g. Bongard) that don't need to be stated in such a fashion. &lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-06T07:11:39.830" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1403" PostTypeId="2" ParentId="1401" CreationDate="2016-08-06T07:24:41.043" Score="5" Body="&lt;p&gt;Unfortunately, this is extremely unlikely.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is nearly impossible to make statements about the behaviour of software in general. This is due to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Halting_problem&quot;&gt;Halting problem&lt;/a&gt;, which shows that it is impossible to prove whether a program will stop for any given input. From this result, many other things have been shown to be unprovable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question whether a piece of code is friendly, can very likely be reduced to a variant of the halting problem.&lt;br&gt;&#xA;An AI that operates in the real world, which is a requirement for &quot;friendliness&quot; to have a meaning, would need to be Turing complete. Input from the real world cannot be reliably interpreted using regular or context-free languages.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Proofs of correctness work for small code snippets, with clearly defined inputs and outputs. They show that an algorithm produces the mathematically right output, given the right input.&lt;br&gt;&#xA;But these are about situations that can be defined with mathematical rigour.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Friendliness&quot; isn't a rigidly defined concept, which already makes it difficult to prove anything about it. On top of that, &quot;friendliness&quot; is about how the AI relates to the real world, which is an environment whose input to the AI is highly unpredictable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The best we can hope for, is that an AI can be programmed to have safeguards, and that the code will raise warning flags if unethical behaviour becomes likely - that AI's are programmed defensively.&lt;/p&gt;&#xA;" OwnerUserId="66" LastEditorUserId="66" LastEditDate="2016-08-06T07:51:00.567" LastActivityDate="2016-08-06T07:51:00.567" CommentCount="4" ContentLicense="CC BY-SA 3.0" />
  <row Id="1404" PostTypeId="1" AcceptedAnswerId="1407" CreationDate="2016-08-06T07:27:52.287" Score="4" ViewCount="979" Body="&lt;p&gt;In the paper &lt;a href=&quot;http://arxiv.org/pdf/1606.00652.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;Death and Suicide in Universal Artificial Intelligence&lt;/a&gt;, a proposal is given for what death could mean for Artificial Intelligence. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What does this mean using English only? I understand that mathematical notation is useful for giving a precise definition, but I'd like to understand what the definition really means. &lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="2444" LastEditDate="2019-04-19T16:44:43.073" LastActivityDate="2019-04-19T16:44:43.073" Title="What does &quot;death&quot; intuitively mean in the paper &quot;Death and Suicide in Universal Artificial Intelligence&quot;?" Tags="&lt;research&gt;&lt;definitions&gt;&lt;agi&gt;&lt;death&gt;&lt;aixi&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1405" PostTypeId="2" ParentId="1401" CreationDate="2016-08-06T07:40:02.787" Score="3" Body="&lt;p&gt;Here are some examples of recent work on verifying certain properties of autonomous systems &lt;a href=&quot;https://www.cs.york.ac.uk/circus/RoboCalc-event/courses/&quot; rel=&quot;nofollow&quot;&gt;[RoboCheck]&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, to achieve the same kind of thing for the notion of 'friendly' using formal verification (i.e. 'proving correctness using mathematical techniques'),&#xA;it would (at the least) seem necessary to be able to express 'friendly' within a logical formalism, (i.e. as a predicate testable within a model-checker, so that we can be sure a system never enters an undesirable state).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, it's not immediately clear that 'friendly' has a more specific definition than 'a desire not to harm humans', so much more low-level detail is needed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some previous work in this general area that might be useful in this respect include:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Deontic_logic&quot; rel=&quot;nofollow&quot;&gt;Deontic Logic&lt;/a&gt; - a logical calculus of obligations.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.jfsowa.com/ikl/McCarthy89&quot; rel=&quot;nofollow&quot;&gt;Elephant 2000&lt;/a&gt; - John McCarthy's description of a promise-based programming language.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-06T07:40:02.787" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1406" PostTypeId="2" ParentId="1396" CreationDate="2016-08-06T07:44:40.520" Score="25" Body="&lt;p&gt;Whenever a problem becomes solvable by a computer, people start arguing that it does not require intelligence. John McCarthy is often quoted: &quot;As soon as it works, no one calls it AI anymore&quot; (&lt;a href=&quot;http://cacm.acm.org/magazines/2012/1/144824-artificial-intelligence-past-and-future/fulltext&quot; rel=&quot;noreferrer&quot;&gt;Referenced in CACM&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of my teachers in college said that in the 1950's, a professor was asked what he thought was intelligent for a machine. The professor reputedly answered that if a vending machine gave him the right change, that would be intelligent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Later, playing chess was considered intelligent. However, computers can now defeat grandmasters at chess, and people are no longer saying that it is a form of intelligence. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now we have OCR. It's already stated in &lt;a href=&quot;https://ai.stackexchange.com/a/1402/66&quot;&gt;another answer&lt;/a&gt; that our methods do not have the recognition facilities of a 5 year old. As soon as this is achieved, people will say &quot;meh, that's not intelligence, a 5 year old can do that!&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A psychological bias, a need to state that we are somehow superior to machines, is at the basis of this.&lt;/p&gt;&#xA;" OwnerUserId="66" LastEditorUserId="-1" LastEditDate="2017-04-13T12:53:10.013" LastActivityDate="2016-08-08T03:21:22.243" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1407" PostTypeId="2" ParentId="1404" CreationDate="2016-08-06T08:07:18.500" Score="7" Body="&lt;p&gt;The authors do actually give an English definition in terms of the well-known agent formulation of AI:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;We intend this usage to be intuitive: death means that one sees&#xA;  no more percepts, and takes no more actions.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It would seem that this becomes possible for a reinforcement learning agent such as AIXI in a formulation that uses &lt;em&gt;semi-measures&lt;/em&gt; of probability (which need not sum up to 1), rather than the more traditional notion.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-06T08:07:18.500" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1408" PostTypeId="2" ParentId="1394" CreationDate="2016-08-06T08:59:35.103" Score="6" Body="&lt;p&gt;Vernor Vinge said that if we can scan a human brain and then simulate it: We can run it at 1000 times the speed. The brain will be able to do 1000 years of thinking in 1 year ect. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this stage in history we have the computer power.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The trouble lies in cutting a brain up and scanning the 100 billion neurons and 12 million kilometres of axons and 100000 billion synapses.&#xA;And piecing together the connectome from all the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sebastian Seung at MIT is working on automating this scanning process with machine learning. By gathering training data from thousands of people playing his &lt;a href=&quot;https://en.wikipedia.org/wiki/Eyewire&quot; rel=&quot;noreferrer&quot;&gt;Eyewire game&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Henry Markram in Europe tried to do something similar with his &lt;a href=&quot;https://en.wikipedia.org/wiki/Blue_Brain_Project&quot; rel=&quot;noreferrer&quot;&gt;Blue Brain Project&lt;/a&gt;.&#xA;He attempted to simulate the neocortical column of a rat. The EU gave him a billion euros to do this. Unfortunately he has been heavily criticised by the Neuroscience community. They claim that we don't know the physiology  well enough to make a valid simulation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Check out his &lt;a href=&quot;https://www.youtube.com/watch?v=LS3wMC2BpxU&quot; rel=&quot;noreferrer&quot;&gt;Ted Talk&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the 1970s Sydney Brenner achieved a &lt;em&gt;full brain scan&lt;/em&gt; of a C Elegans worm. This worm has one of the simplest biological neural networks having only 302 neurons.&#xA;Here is a picture of its connectome:&lt;a href=&quot;https://i.stack.imgur.com/cAZ49.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/cAZ49.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/siHt8.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/siHt8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An accurate computer simulation of this worm would be a major stepping stone to uploading a human brain.&lt;/p&gt;&#xA;" OwnerUserId="157" LastEditorUserId="157" LastEditDate="2016-08-06T09:37:48.850" LastActivityDate="2016-08-06T09:37:48.850" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1409" PostTypeId="2" ParentId="1363" CreationDate="2016-08-06T09:15:09.163" Score="1" Body="&lt;p&gt;Matrix representation is beneficial for implementing neural networks in silicon.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But for examining neural networks empirically it is sometimes good to visualise the synapse weight values as images or videos: &lt;a href=&quot;https://www.youtube.com/watch?v=AgkfIQ4IGaM&quot; rel=&quot;nofollow noreferrer&quot;&gt;Jason Yosinski's &lt;/a&gt; exploration of a convolution neural network. The network seems to have a &quot;filter&quot; that just detects shoulders. A bit like a lock that only opens when it recognises the pattern of shoulders.&lt;a href=&quot;https://i.stack.imgur.com/4g4gF.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/4g4gF.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="157" LastEditorUserId="157" LastEditDate="2016-08-06T10:12:34.840" LastActivityDate="2016-08-06T10:12:34.840" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1410" PostTypeId="1" CreationDate="2016-08-06T10:46:10.100" Score="5" ViewCount="119" Body="&lt;p&gt;We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?&lt;br/&gt;&#xA;I'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.&lt;/p&gt;&#xA;" OwnerUserId="98" LastEditorUserId="10" LastEditDate="2016-08-06T14:14:05.420" LastActivityDate="2016-08-08T18:25:11.147" Title="Do specific units exists for measuring the intelligence of a machine?" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;intelligence-testing&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1411" PostTypeId="2" ParentId="1410" CreationDate="2016-08-06T11:57:19.053" Score="3" Body="&lt;p&gt;One of the challenges of AI is defining Intelligence.&#xA;If we could precisely define general intelligence then we could program it into a computer. After all an algorithm is a process so well defined that it can be run on a computer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Narrow AI can be evaluated on its success at achieving goals in an environment. In domains such as computer vision and speech recognition narrow AI algorithms can be easily evaluated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many universities curate narrow AI tests. Fei-Fei Li a professor at Stanford who directs the Artificial Intelligence lab there organises the annual ImageNet Challenge. In 2012 Geoffrey Hinton famously won the competition by building a Deep Neural Network that could recognize pictures more accurately than humans can.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To my knowledge the testers commonly use &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot; rel=&quot;nofollow&quot;&gt;Precision and recall&lt;/a&gt; evaluation metrics&lt;/p&gt;&#xA;" OwnerUserId="157" LastEditorUserId="157" LastEditDate="2016-08-06T14:07:22.407" LastActivityDate="2016-08-06T14:07:22.407" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1412" PostTypeId="2" ParentId="1390" CreationDate="2016-08-06T14:04:28.247" Score="8" Body="&lt;p&gt;Yes, many people have worked on this sort of thing, due to its obvious industrial applications (most of the ones I'm familiar with are in the pharmaceutical industry). Here's &lt;a href=&quot;https://arxiv.org/abs/1305.7074&quot;&gt;a paper from 2013&lt;/a&gt; that claims good results; following the trail of &lt;a href=&quot;https://scholar.google.com/scholar?cites=10630711614897084406&amp;amp;as_sdt=5,44&amp;amp;sciodt=0,44&amp;amp;hl=en&quot;&gt;papers that cited it&lt;/a&gt; will likely give you more recent work. &lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-06T14:04:28.247" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1413" PostTypeId="2" ParentId="1410" CreationDate="2016-08-06T14:17:28.310" Score="3" Body="&lt;p&gt;Shane Legg and Marcus Hutter &lt;a href=&quot;http://www.vetta.org/documents/42.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;proposed one&lt;/a&gt; in 2006. The main descriptive quotes (see the paper for the actual formula):&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Intelligence measures an agent’s general ability to achieve goals in a wide range of environments&lt;/p&gt;&#xA;&lt;p&gt;...&lt;/p&gt;&#xA;&lt;p&gt;It is clear by construction that universal intelligence measures the general ability of an agent to perform well in a very wide range of environments, as required by our informal definition of intelligence given earlier. The definition places no restrictions on the internal workings of the agent; it only requires that the agent is capable of generating output and receiving input which includes a reward signal.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="10" LastEditorUserId="-1" LastEditDate="2020-06-17T09:57:20.710" LastActivityDate="2016-08-06T14:17:28.310" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1414" PostTypeId="2" ParentId="41" CreationDate="2016-08-06T14:22:56.640" Score="4" Body="&lt;p&gt;The other answers are correct that machine IQ test results are currently &lt;strong&gt;not&lt;/strong&gt; indicative of machine intelligence. One of the surprising facts of human intelligence is that performance on almost all cognitive tasks are correlated with each other; that is, there is such a thing as 'general smartness' and IQ tests attempt to measure that thing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;People &lt;em&gt;have&lt;/em&gt; built programs that take IQ tests, however, and some of them perform quite well. Raven's Progressive Matrices, a visual pattern recognition IQ test, is an easy target for AI (see &lt;a href=&quot;https://www.researchgate.net/publication/288211280_Solving_Raven&amp;#39;s_IQ-tests_An_AI_and_cognitive_modeling_approach&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; as representative) and another group &lt;a href=&quot;http://arxiv.org/abs/1509.03390&quot; rel=&quot;nofollow&quot;&gt;has constructed an AI&lt;/a&gt; that performs about as well as a 4 year old on the verbal intelligence portion of a standard childhood IQ test.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-06T14:22:56.640" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1415" PostTypeId="1" AcceptedAnswerId="1418" CreationDate="2016-08-06T17:24:50.083" Score="9" ViewCount="446" Body="&lt;p&gt;In the mid 1980s, Rodney Brooks famously created the foundations of &quot;the new AI&quot;. The central claim was that the symbolist approach of 'Good Old Fashioned AI' (GOFAI) had failed by attempting to 'cream cognition off the top', and that &lt;em&gt;embodied cognition&lt;/em&gt; was required, i.e. built from the bottom up in a 'hierarchy of competances' (e.g. basic locomotion -&gt; wandering around -&gt; actively foraging) etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I imagine most AI researchers would agree that the 'embodied cognition' perspective has now (at least tacitly) supplanted GOFAI as the mainstream.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question takes the form of a thought experiment and asks: &quot;Which (if any)  aspects of 'embodied' can be relaxed/omitted before we lose something essential for AGI?&quot;&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-06T18:19:44.323" Title="What kind of body (if any) does intelligence require?" Tags="&lt;agi&gt;&lt;symbolic-ai&gt;&lt;embodied-cognition&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1416" PostTypeId="1" CreationDate="2016-08-06T17:35:02.143" Score="6" ViewCount="1239" Body="&lt;p&gt;In other words, which existing reinforcement method learns with fewest episodes? &lt;a href=&quot;http://www.jmlr.org/papers/volume3/brafman02a/brafman02a.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;R-Max&lt;/a&gt; comes to mind, but it's very old and I'd like to know if there is something better now.&lt;/p&gt;&#xA;" OwnerUserId="144" LastEditorUserId="2444" LastEditDate="2022-01-07T22:07:18.303" LastActivityDate="2022-01-07T22:07:18.303" Title="What is the current state-of-the-art in Reinforcement Learning regarding data efficiency?" Tags="&lt;reinforcement-learning&gt;&lt;reference-request&gt;&lt;algorithm-request&gt;&lt;state-of-the-art&gt;&lt;sample-efficiency&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1417" PostTypeId="2" ParentId="92" CreationDate="2016-08-06T17:41:35.647" Score="28" Body="&lt;p&gt;The images that you provided may be unrecognizable for us. They are actually the images that we recognize but evolved using the &lt;a href=&quot;https://github.com/sferes2/sferes2&quot; rel=&quot;noreferrer&quot;&gt;Sferes&lt;/a&gt; evolutionary framework.&lt;/p&gt;&#xA;&lt;p&gt;While these images are almost impossible for humans to label with anything but abstract arts, the Deep Neural Network will label them to be familiar objects with 99.99% confidence.&lt;/p&gt;&#xA;&lt;p&gt;This result highlights differences between how DNNs and humans recognize objects. Images are either directly (or indirectly) encoded&lt;/p&gt;&#xA;&lt;p&gt;According to this &lt;a href=&quot;https://youtu.be/M2IebCN9Ht4&quot; rel=&quot;noreferrer&quot;&gt;video&lt;/a&gt;&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Changing an image originally correctly classified in a way imperceptible to humans can cause the cause DNN to classify it as something else.&lt;/p&gt;&#xA;&lt;p&gt;In the image below the number at the bottom are the images are supposed to look like the digits&#xA;But the network believes the images at the top (the one like white noise) are real digits with 99.99% certainty.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/Jx1wX.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;The main reason why these are easily fooled is that Deep Neural Network does not see the world in the same way as human vision. We use the whole image to identify things while DNN depends on the features. As long as DNN detects certain features, it will classify the image as a familiar object it has been trained on.&#xA;The researchers proposed one way to prevent such fooling by adding the fooling images to the dataset in a new class and training DNN on the enlarged dataset. In the experiment, the confidence score decreases significantly for ImageNet AlexNet. It is not easy to fool the retrained DNN this time. But when the researchers applied such method to MNIST LeNet, evolution still produces many unrecognizable images with confidence scores of 99.99%.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;More details &lt;a href=&quot;http://www.evolvingai.org/fooling&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://www.kdnuggets.com/2015/01/deep-learning-can-be-easily-fooled.html&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="72" LastEditorUserId="-1" LastEditDate="2020-06-17T09:57:20.710" LastActivityDate="2018-04-13T03:22:19.983" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1418" PostTypeId="2" ParentId="1415" CreationDate="2016-08-06T18:19:44.323" Score="4" Body="&lt;p&gt;This is something of an orthogonal answer, but I think Brooks didn't go about his idea the right way. That is, &lt;a href=&quot;https://en.wikipedia.org/wiki/Subsumption_architecture&quot; rel=&quot;noreferrer&quot;&gt;subsumption architecture&lt;/a&gt; is one in which the 'autopilot' is &lt;em&gt;replaced&lt;/em&gt; by a more sophisticated system when necessary. (All pieces receive the raw sensory inputs, and output actions, some of which turn off or on other systems.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But a better approach is the normal hierarchical control approach, in which the target of a lower level system is the output of a higher level system. That is, the targeted joint angle of a robot leg is determined by the system that is trying to optimize the velocity, which is determined by a system that is trying to optimize the trajectory, which is determined by a system that is trying to optimize the target position, and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This allows for increasing level of complexity while maintaining detail and system reusability.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;That said, I don't think you actually need what one would naively call 'embodied cognition' in order to get the bottom-up hierarchy of competencies that Brooks is right to point towards. The core feature is the wide array of inputs and outputs, which are understood in a hierarchical fashion that allows systems to be chained together vertically. I think you could get a functional general intelligence whose only inputs and outputs involve going through an Ethernet cable, and doesn't have anything like a traditional body that it actuates or senses through. (This is a claim that the hierarchical structure is what matters, not the content of what we use that structure for.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(The main place to look for more, I think, is actually a book about &lt;em&gt;human&lt;/em&gt; cognition, called The Control of Perception by William T. Powers.)&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-06T18:19:44.323" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1419" PostTypeId="2" ParentId="104" CreationDate="2016-08-06T18:38:06.953" Score="5" Body="&lt;p&gt;I can offer two (at first sight, conflicting) perspectives on this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Firstly:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;If the letter string 'abc' becomes 'abd' what would &quot;doing the same thing&quot; to 'ijk' look like?&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is just one example of a problem (so-called 'letter-string analogy problems') that is not easily framed as an optimization problem - there is a range of answers that appear compelling to humans, each for its own structurally-specific reason. Some of the subtleties of these kinds of problems are discussed in detail &lt;a href=&quot;http://cognitrn.psych.indiana.edu/rgoldsto/courses/concepts/copycat.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Secondly:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a &lt;em&gt;very&lt;/em&gt; high-level perspective on AGI in which &lt;a href=&quot;http://arxiv.org/abs/cs/0309048&quot; rel=&quot;nofollow noreferrer&quot;&gt;optimization plays a key part&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's not at all clear how these two very different scales of approach might be reconciled. As someone who does optimization research for a living, I'd be inclined to say that, certainly for all &lt;em&gt;current, practical&lt;/em&gt; purposes, AGI can't really be treated as an optimization problem, since most interesting activities don't readily lend themselves to description via a cost function.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="14723" LastEditDate="2018-04-13T18:00:00.527" LastActivityDate="2018-04-13T18:00:00.527" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1420" PostTypeId="1" AcceptedAnswerId="1421" CreationDate="2016-08-06T18:38:50.160" Score="13" ViewCount="1840" Body="&lt;p&gt;Are there any research teams that attempted to create or have already created an AI robot that can be as close to intelligent as these found in &lt;a href=&quot;https://en.wikipedia.org/wiki/Ex_Machina_(film)&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;Ex Machina&lt;/a&gt;&lt;/em&gt; or &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/I,_Robot_(film)&quot; rel=&quot;nofollow noreferrer&quot;&gt;I, Robot&lt;/em&gt;&lt;/a&gt; movies?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not talking about full awareness, but an artificial being that can make its own decisions and physical and intellectual tasks that a human being can do?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-11-22T23:33:06.783" LastActivityDate="2019-11-22T23:33:06.783" Title="How close are we to creating Ex Machina?" Tags="&lt;research&gt;&lt;agi&gt;" AnswerCount="4" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1421" PostTypeId="2" ParentId="1420" CreationDate="2016-08-06T18:54:48.577" Score="23" Body="&lt;p&gt;We are absolutely nowhere near, nor do we have any idea how to bridge the gap between what we can currently do and what is depicted in these films.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The current trend for DL approaches (coupled with the emergence of data science as a mainstream discipline) has led to a lot of popular interest in AI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, researchers and practitioners would do well to learn the lessons of the 'AI Winter' and not engage in hubris or read too much into current successes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Success in transfer learning is very limited. &lt;/li&gt;&#xA;&lt;li&gt;The 'hard problem' (i.e. presenting the 'raw, unwashed environment' to the machine and having it come up with a solution from scratch) is not being&#xA;addressed by DL to the extent that it is popularly portrayed: expert human knowledge is still required to help decide how the input should be framed, tune parameters, interpret output etc.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Someone who has enthusiasm for AGI would hopefully agree that the 'hard problem' is actually the only one that matters. Some years ago, a famous cognitive scientist said &quot;We have yet to successfully represent &lt;em&gt;even a single concept&lt;/em&gt; on a computer&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my opinion, recent research trends have done little to change this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All of this perhaps sounds pessimistic - it's not intended to. None of us want another AI Winter, so we should challenge (and be honest about) the limits of our current techniques rather than mythologizing them.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="42" LastEditDate="2016-08-06T19:04:52.397" LastActivityDate="2016-08-06T19:04:52.397" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="1422" PostTypeId="2" ParentId="1420" CreationDate="2016-08-06T20:30:10.680" Score="0" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;heavier-than-air flying machines are impossible&quot; &lt;em&gt;_ Lord Kelvin 1895&lt;/em&gt; &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;7 years later the Wright brothers built one.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Currently we have many powerful narrow AI (good at special tasks) but we have no idea how to unify them into a single system like in a biological brain. &lt;/p&gt;&#xA;" OwnerUserId="157" LastEditorUserId="157" LastEditDate="2016-08-07T07:02:48.480" LastActivityDate="2016-08-07T07:02:48.480" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1423" PostTypeId="1" AcceptedAnswerId="1440" CreationDate="2016-08-06T22:59:43.413" Score="13" ViewCount="230" Body="&lt;p&gt;Humans can do multiple tasks at the same (e.g. reading while listening to music), but we memorize information from less focused sources with worse efficiency than we do from our main focus or task.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do such things exist in the case of artificial intelligence? I doubt, for example, that neural networks have such characteristics, but I may be wrong.&lt;/p&gt;&#xA;" OwnerUserId="1270" LastEditorUserId="2444" LastEditDate="2019-11-05T16:14:30.497" LastActivityDate="2021-09-15T02:10:00.947" Title="Is there any artificial intelligence that possesses &quot;concentration&quot;?" Tags="&lt;neural-networks&gt;&lt;reference-request&gt;&lt;attention&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1424" PostTypeId="2" ParentId="1390" CreationDate="2016-08-06T23:23:52.457" Score="3" Body="&lt;p&gt;Yes, there were successful attempts at predicting the interaction between molecules and biological proteins which have been used to identify potential treatments by using &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network#Drug_discovery&quot; rel=&quot;nofollow noreferrer&quot;&gt;convolutional neural networks&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;For example in 2015, the first deep learning neural network has been created for structure-based drug design which trains 3-dimensional representation of chemical interactions which works similar to how image recognition works (composing smaller features into larger, complex structures).&lt;sup&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network#Drug_discovery&quot; rel=&quot;nofollow noreferrer&quot;&gt;wiki&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;sup&gt;Study: &lt;a href=&quot;https://arxiv.org/abs/1510.02855&quot; rel=&quot;nofollow noreferrer&quot;&gt;AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Another approach is to use evolutionary artificial neural networks which can achieve great optimization results.&lt;/p&gt;&#xA;&lt;p&gt;Furthermore, the &lt;a href=&quot;https://arxiv.org/pdf/1502.00193.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;paper from 2015&lt;/a&gt; demonstrated heuristic &lt;a href=&quot;http://link.springer.com/article/10.1007/s12293-012-0075-1&quot; rel=&quot;nofollow noreferrer&quot;&gt;chemical reaction optimization&lt;/a&gt; (CRO) which is inspired by the nature of chemical reactions (e.g. transforming the unstable substances into stable ones). Simulation results show that CRO outperforms many evolutionary algorithms by population-based metaheuristics mimicking the transition of molecules and their interactions in a chemical reaction.&lt;/p&gt;&#xA;&lt;p&gt;Sample pseudocode algorithm for predicting synthesis given ω1, ω2 (from the above paper):&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; 1: for all Matrices and vectors m in ω′ do&#xA; 2:     for all Elements e in m do&#xA; 3:         Generate a real r between 0 and 1&#xA; 4:         if r &amp;gt; 0.5 then&#xA; 5:             e =counterpart in m1&#xA; 6:         else&#xA; 7:             e =counterpart in m2&#xA; 8:         end if&#xA; 9:     end for&#xA;10: end for&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;which is used to generate a new solution ω′ based on two given solutions ω1 and ω2.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="36737" LastEditDate="2021-04-19T22:21:33.800" LastActivityDate="2021-04-19T22:21:33.800" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1426" PostTypeId="1" AcceptedAnswerId="1443" CreationDate="2016-08-07T00:06:45.913" Score="5" ViewCount="124" Body="&lt;p&gt;How can a swarm of small robots (like Kilobots) walking close to each other achieve collaboration without bumping into each other? For example, one study shows &lt;a href=&quot;http://science.sciencemag.org/content/345/6198/795.abstract&quot; rel=&quot;noreferrer&quot;&gt;programmable self-assembly in a thousand-robot swarm&lt;/a&gt; (see &lt;a href=&quot;http://robohub.org/thousand-robot-swarm-self-assembles-into-arbitrary-shapes/&quot; rel=&quot;noreferrer&quot;&gt;article&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://vimeo.com/103329200&quot; rel=&quot;noreferrer&quot;&gt;video&lt;/a&gt;) which are moving without GPS-like system and by measuring distances to neighbours. This was achieved, because the robots were very slow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any way that similar robots can achieve much more efficient and quicker assembly by using more complex techniques of coordination? Not by walking around clock-wise (which I guess was the easiest way), but I mean using some more sophisticated way. Because waiting half a day (~11h) to create a simple star shape using a thousand-robot swarm is way too long!&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="145" LastEditDate="2016-08-07T19:10:08.217" LastActivityDate="2016-08-07T19:10:08.217" Title="How can thousand-robot swarm coordinate their moves without bumping into each other?" Tags="&lt;robots&gt;&lt;multi-agent-systems&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1427" PostTypeId="1" AcceptedAnswerId="1428" CreationDate="2016-08-07T00:55:36.657" Score="1" ViewCount="65" Body="&lt;p&gt;On Watson wiki page we can read:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In healthcare, Watson's natural language, hypothesis generation, and evidence-based learning capabilities allow it to function as a clinical decision support system for use by medical professionals.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;How exactly such AI can help doctors to diagnose the diseases?&lt;/p&gt;&#xA;" OwnerUserId="8" LastActivityDate="2021-03-31T22:27:21.613" Title="How Watson can help to make medical diagnoses?" Tags="&lt;watson&gt;&lt;healthcare&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="1428" PostTypeId="2" ParentId="1427" CreationDate="2016-08-07T00:55:36.657" Score="0" Body="&lt;p&gt;Watson can make its diagnosis based on the patient's data and comparing it to the data from millions of other studies.&lt;/p&gt;&#xA;&lt;p&gt;For example, having enough genetic data and the right algorithms, its AI computing capability demonstrated the huge potential of data analysis based on which it can be used for everything from diagnosing rare illnesses to prescribing perfect dosages of medicine based on the patient's personal genetic makeup. Of course, there are still plenty of challenges that need to be overcome before it can be used in mainstream medicine.&lt;/p&gt;&#xA;&lt;p&gt;Recently Watson was able to diagnose a rare form of leukaemia after treatment was proved ineffective. It was fed in with the patient’s genetic data and compared to data from other 20 million oncological studies.&lt;/p&gt;&#xA;&lt;p&gt;Sources:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.ndtv.com/health/artificial-intelligence-used-to-detect-rare-leukemia-type-in-japan-1440789&quot; rel=&quot;nofollow noreferrer&quot;&gt;Artificial Intelligence Used To Detect Rare Leukemia Type In Japan&lt;/a&gt; (2016)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://siliconangle.com/blog/2016/08/05/watson-correctly-diagnoses-woman-after-doctors-were-stumped/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Watson correctly diagnoses woman after doctors were stumped&lt;/a&gt; (2016)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastEditorUserId="36737" LastEditDate="2021-03-31T22:27:21.613" LastActivityDate="2021-03-31T22:27:21.613" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1429" PostTypeId="1" AcceptedAnswerId="1430" CreationDate="2016-08-07T01:32:32.137" Score="3" ViewCount="602" Body="&lt;p&gt;Recently White House published the article: &lt;a href=&quot;https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;Preparing for the Future of Artificial Intelligence&lt;/a&gt; which says that the government is working to leverage AI for the public good and toward a more effective government.&lt;/p&gt;&#xA;&lt;p&gt;I'm especially interested in how AI can help with computational sustainability, environmental management and Earth's ecosystem, such as biological conservation?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-04-01T01:25:39.373" LastActivityDate="2021-04-01T01:25:39.373" Title="How machine learning can help with sustainable development and biological conservation?" Tags="&lt;applications&gt;&lt;biology&gt;&lt;green-ai&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1430" PostTypeId="2" ParentId="1429" CreationDate="2016-08-07T01:32:32.137" Score="2" Body="&lt;p&gt;There are a variety of aspects where AI can help for the public good. Future studies of computational methods can contribute to a sustainable management ecosystem by its data acquisition, interpretation, integration and model fitting.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;http://web.engr.oregonstate.edu/%7Etgd/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Prof. Tom Dietterich&lt;/a&gt; is a leader in combining computer science and ecological sciences to build the new discipline of Ecosystem Informatics which studies methods for collecting, analyzing and visualizing data on the structure and function of ecosystems.&lt;/p&gt;&#xA;&lt;p&gt;His group is involved in many aspects of the ecosystem, such as:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Models that can predict species distribution and their presence/absence elsewhere in order to create species distribution and migration/dispersal maps (such as &lt;a href=&quot;https://dataone.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;DataONE Datanet&lt;/a&gt;, &lt;a href=&quot;http://ebird.org/content/ebird/&quot; rel=&quot;nofollow noreferrer&quot;&gt;eBird project&lt;/a&gt;, &lt;a href=&quot;http://birdcast.info/&quot; rel=&quot;nofollow noreferrer&quot;&gt;BirdCast&lt;/a&gt;) .&lt;/li&gt;&#xA;&lt;li&gt;Bio-economic models require solving large Spatio-temporal optimization problems under uncertainty.&lt;/li&gt;&#xA;&lt;li&gt;Ecosystem prediction problems require integrating heterogeneous data sources.&lt;/li&gt;&#xA;&lt;li&gt;Algorithms for deployment (sensor placement), cleaning and analysis of sensor network data of resulting data to increase agricultural productivity (Project &lt;a href=&quot;http://tahmo.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;TAHMO&lt;/a&gt;), like deployment of 20,000 hydro-meteorological stations in Africa (e.g. computational problem where to place it).&lt;/li&gt;&#xA;&lt;li&gt;Systems for capturing, imaging, and sorting bugs combined with general image processing/machine learning/pattern recognition tools for counting and classifying them (&lt;a href=&quot;http://web.engr.oregonstate.edu/%7Etgd/bugid/&quot; rel=&quot;nofollow noreferrer&quot;&gt;BugID project&lt;/a&gt;). The goal is to develop algorithms for automating biodiversity based on visual pattern recognition by using the computer vision method.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For further information about this work, check the following resources, see:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;(video) &lt;a href=&quot;https://www.youtube.com/watch?v=FjHBFWwOdIk&quot; rel=&quot;nofollow noreferrer&quot;&gt;&amp;quot;Challenges for Machine Learning in Computational Sustainability&amp;quot; (CRCS)&lt;/a&gt; (2013)&lt;/li&gt;&#xA;&lt;li&gt;(slides) &lt;a href=&quot;http://cra.org/ccc/wp-content/uploads/sites/2/2016/06/Thomas-Dietterich-AI-slides.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;Tom Dietterich, Understanding and Managing Ecosystems through AI&lt;/a&gt; (2013)&lt;/li&gt;&#xA;&lt;li&gt;(study) &lt;a href=&quot;http://rspb.royalsocietypublishing.org/content/282/1808/20142984&quot; rel=&quot;nofollow noreferrer&quot;&gt;Adapting environmental management to uncertain but inevitable change&lt;/a&gt; (2015)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastEditorUserId="36737" LastEditDate="2021-03-31T22:22:33.033" LastActivityDate="2021-03-31T22:22:33.033" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1431" PostTypeId="1" AcceptedAnswerId="1438" CreationDate="2016-08-07T01:51:28.157" Score="-2" ViewCount="184" Body="&lt;p&gt;When AI has some narrow domain, such as chess, where it can outperform the world's human masters of chess, does it make it a &lt;a href=&quot;https://en.wikipedia.org/wiki/Superintelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;superintelligence&lt;/a&gt; or not?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2020-08-11T19:45:36.987" LastActivityDate="2020-08-11T19:45:36.987" Title="Is Deep Blue superintelligent or not?" Tags="&lt;definitions&gt;&lt;superintelligence&gt;&lt;deep-blue&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1432" PostTypeId="1" CreationDate="2016-08-07T02:08:34.803" Score="9" ViewCount="2127" Body="&lt;p&gt;Suppose my goal is to collaborate and create an advanced AI, for instance, one that resembles a human being and the project would be on the frontier of AI research. What kind of skills would I need?&lt;/p&gt;&#xA;&lt;p&gt;I am talking about specific things, like what university program should I complete to enter and be competent in the field.&lt;/p&gt;&#xA;&lt;p&gt;Here are some of the things that I thought about, just to exemplify what I mean:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Computer sciences: obviously, the AI is built on computers, it wouldn't hurt to know how computers work, but some low-level stuff and machine-specific things do not seem essential, I may be wrong of course.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Psychology: if AI resembles human beings, knowledge of human cognition would probably be useful, although I do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the Oedipus complex would be relevant, but again, I may be wrong.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="1321" LastEditorUserId="2444" LastEditDate="2021-12-21T15:26:36.793" LastActivityDate="2021-12-21T15:26:36.793" Title="What kind of education is required for researchers in AI?" Tags="&lt;agi&gt;&lt;research&gt;&lt;academia&gt;&lt;education&gt;" AnswerCount="2" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="1433" PostTypeId="1" AcceptedAnswerId="1434" CreationDate="2016-08-07T02:33:51.350" Score="3" ViewCount="786" Body="&lt;p&gt;&lt;a href=&quot;https://web.archive.org/web/20161221222448/https://www.whitehouse.gov/webform/rfi-preparing-future-artificial-intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;White House published the information&lt;/a&gt; about AI which requests mentions about &lt;em&gt;the most important research gaps in AI that must be addressed to advance this field and benefit the public&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are these exactly?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2020-03-07T20:34:56.477" LastActivityDate="2021-03-31T22:27:56.533" Title="What are the most pressing fundamental questions and gaps in AI research?" Tags="&lt;research&gt;&lt;social&gt;&lt;ethics&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1434" PostTypeId="2" ParentId="1433" CreationDate="2016-08-07T02:33:51.350" Score="3" Body="&lt;p&gt;According to IBM Research organization in the response to the White House as part of &lt;a href=&quot;http://web.archive.org/web/20160808135040/https://www.whitehouse.gov/webform/rfi-preparing-future-artificial-intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;preparing for the future of Artificial Intelligence&lt;/a&gt;, AI depends upon many long-term advances, not only from AI researchers but from many interdisciplinary teams of experts from many disciplines, including the following challenges:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Machine learning and reasoning.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Currently, AI systems use supervised learning using a huge amount of dataset of labelled data for training. This is very different to how humans learn by creating concepts, relationship, common sense reasoning which gives the ability to learn much without too much data. Therefore machine learning with common-sense reasoning capabilities should be researched further more.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Decision techniques.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Current AI-based systems have very limited ability for making decisions, therefore new techniques must be developed (e.g. modeling systemic risks, analyzing tradeoffs, detecting anomalies in context, analyzing data while preserving privacy).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Domain-specific AI systems.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;The current AI-based system is lack of abilities to understand the variety of domains of human expertise (such as medicine, engineering, law and many more). The systems should be able to perform professional-level tasks such as designing problems, experiments, managing contradictions, negotiating, etc.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data assurance and trust.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;The current AI-based systems require huge amounts of data and their behaviour directly depends on the quality of this data which can be biased, incomplete or compromised. This can be expensive and time-consuming especially where it is used for safety-critical systems which potentially can be very dangerous.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Radically efficient computing infrastructure.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;The current AI-based systems require unprecedented workloads and computing power which require the development of new computing architectures (such as neuromorphic).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Interpretability and explanations.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;For people to follow AI suggestions, they need to trust systems, and this is only when they are capable of knowing users' intents, priorities, reasoning and they can learn from their mistakes. These capabilities are required in many business domains and professionals&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Value alignment and ethics.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Humans can share the common knowledge of how the world function, the machine cannot. They can fail by having unintended and unexpected behaviour only because humans did not specify the right goals for them or they omitted essential training details. The systems should be able to correct the specification of the goals and avoid unintended and undesired consequences in the behaviour.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Social AI.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;The AI-based systems should be able to work closely with humans in their professional and personal life, therefore they should have significant social capabilities because they can impact our emotions and our decision-making capabilities. Also, sophisticated natural language capabilities will need to be developed to allow a natural interaction and dialogue between humans and machines.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Source: &lt;em&gt;Fundamental questions in AI research, and the most important research gaps (RFI questions 5 and 6)&lt;/em&gt;. (link no longer available)&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="36737" LastEditDate="2021-03-31T22:27:56.533" LastActivityDate="2021-03-31T22:27:56.533" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="1436" PostTypeId="1" AcceptedAnswerId="1439" CreationDate="2016-08-07T03:49:20.143" Score="5" ViewCount="616" Body="&lt;p&gt;Is there any methods by which artificial intelligence use recursion(s) to solve a certain issue or to keep up working and calculating?&lt;/p&gt;&#xA;" OwnerUserId="1270" LastActivityDate="2018-12-28T21:43:24.817" Title="Is recursion used in practice to improve performance of AI systems?" Tags="&lt;math&gt;" AnswerCount="1" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
</posts>